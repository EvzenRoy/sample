{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e168c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=55c1ea680988c75e79bd5d05af862eff21039d6743ecacd704a9e4105f74237a\n",
      "  Stored in directory: c:\\users\\evzen\\appdata\\local\\pip\\cache\\wheels\\f0\\69\\93\\a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "\n",
      "   -------------------- ------------------- 1/2 [feedparser]\n",
      "   ---------------------------------------- 2/2 [feedparser]\n",
      "\n",
      "Successfully installed feedparser-6.0.12 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1fa096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (4.11.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (2.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (6.0.12)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (9.4.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (3.7)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (4.9.1)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.3.2.post1)\n",
      "Requirement already satisfied: six in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.9.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\evzen\\anaconda3\\anaconda.new\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38fe89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: feedparser in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from newspaper3k) (4.14.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from newspaper3k) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from newspaper3k) (6.0.3)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k)\n",
      "  Downloading lxml-6.0.2-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from newspaper3k) (2.32.5)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/7.4 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 1.3/7.4 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 3.1/7.4 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.2/7.4 MB 6.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 5.5/7.4 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.6/7.4 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 6.0 MB/s  0:00:01\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
      "Requirement already satisfied: six in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2025.11.12)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading lxml-6.0.2-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.0/4.0 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.1/4.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.9/4.0 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/4.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 4.6 MB/s  0:00:00\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.3 MB/s  0:00:00\n",
      "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k\n",
      "  Building wheel for tinysegmenter (pyproject.toml): started\n",
      "  Building wheel for tinysegmenter (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13666 sha256=c7b9b19eb249032b78421fcab074f98c57c1086082509dae83121574cfcc89f3\n",
      "  Stored in directory: c:\\users\\evzen\\appdata\\local\\pip\\cache\\wheels\\c8\\d6\\6c\\384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
      "  Building wheel for feedfinder2 (pyproject.toml): started\n",
      "  Building wheel for feedfinder2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3408 sha256=ee6a26ad4d30ae1274b5b62335cc6bb0f8e9589a2b8f36682f8d6130b22a50b0\n",
      "  Stored in directory: c:\\users\\evzen\\appdata\\local\\pip\\cache\\wheels\\97\\02\\e7\\a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
      "  Building wheel for jieba3k (pyproject.toml): started\n",
      "  Building wheel for jieba3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398410 sha256=c8afcf0b2f3785c8effd5fe855ac8d9a73b6acce32961d98f7c4633c73af2cd2\n",
      "  Stored in directory: c:\\users\\evzen\\appdata\\local\\pip\\cache\\wheels\\7a\\c4\\0c\\12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k\n",
      "Installing collected packages: tinysegmenter, jieba3k, lxml, joblib, cssselect, click, requests-file, nltk, feedfinder2, tldextract, newspaper3k\n",
      "\n",
      "   --- ------------------------------------  1/11 [jieba3k]\n",
      "   --- ------------------------------------  1/11 [jieba3k]\n",
      "   --- ------------------------------------  1/11 [jieba3k]\n",
      "   --- ------------------------------------  1/11 [jieba3k]\n",
      "   ------- --------------------------------  2/11 [lxml]\n",
      "   ------- --------------------------------  2/11 [lxml]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ------------------ ---------------------  5/11 [click]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------- --------------  7/11 [nltk]\n",
      "   ------------------------------------ --- 10/11 [newspaper3k]\n",
      "   ---------------------------------------- 11/11 [newspaper3k]\n",
      "\n",
      "Successfully installed click-8.3.1 cssselect-1.3.0 feedfinder2-0.0.4 jieba3k-0.35.1 joblib-1.5.2 lxml-6.0.2 newspaper3k-0.2.8 nltk-3.9.2 requests-file-3.0.1 tinysegmenter-0.3 tldextract-5.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\evzen\\anaconda3\\envs\\nlp\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tldextract.exe is installed in 'C:\\Users\\evzen\\anaconda3\\envs\\nlp\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install newspaper3k feedparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c27a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml[html_clean] in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (6.0.2)\n",
      "Collecting lxml_html_clean (from lxml[html_clean])\n",
      "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"lxml[html_clean]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526f5fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good!\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "print(\"All good!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02701109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch article: https://www.fortra.com/blog/fbi-warns-surge-account-takeover-ato-fraud-schemes-need-know | Exception: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.fortra.com/blog/fbi-warns-surge-account-takeover-ato-fraud-schemes-need-know on URL https://www.fortra.com/blog/fbi-warns-surge-account-takeover-ato-fraud-schemes-need-know\n",
      "\n",
      "===== DOMAIN: DATA_SCIENCE =====\n",
      "\n",
      "Title: Bootstrap a Data Lakehouse in an Afternoon\n",
      "Author: ['Thomas Reid', 'Roger Noble', 'Tds Editors', 'Mike Shakhomirov', 'Bennett Meares', 'Mikkel Dengsøe', 'Rohan Paithankar', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-04 13:30:00+00:00\n",
      "Content: doesn’t need to be that complicated. In this article, I’ll show you how to develop a basic, “starter” one that uses an Iceberg table on AWS S3 storage. Once the table is registered using AWS Glue, you’ll be able to query and mutate it from Amazon Athena, including using:\n",
      "\n",
      "Time travel queries,\n",
      "\n",
      "Merging, updating and deleting data\n",
      "\n",
      "Optimising and vacuuming your tables.\n",
      "\n",
      "I’ll also show you how to inspect the same tables locally from DuckDB, and we’ll also see how to use Glue/Spark to insert more table data.\n",
      "\n",
      "Our example might be basic, but it’ll showcase the setup, the different tools and the processes you can put in place to build up a more extensive data store. All modern cloud providers have equivalents of the AWS services I’m discussing in this article, so it should be fairly straightforward to replicate what I discuss here on Azure, Google Cloud, and others.\n",
      "\n",
      "To make sure we’re all on the same page, here is a brief explanation of some of the key technologies we’ll be using.\n",
      "\n",
      "AWS Glue/Spark\n",
      "\n",
      "AWS Glue is a fully managed, serverless ETL service from Amazon that streamlines data preparation and integration for analytics and machine learning. It automatically detects and catalogues metadata from various sources, such as S3, into a centralised Data Store. Additionally, it can create customisable Python-based Spark ETL scripts to execute these tasks on a scalable, serverless Apache Spark platform. This makes it great for building data lakes on Amazon S3, loading data into data warehouses like Amazon Redshift, and performing data cleaning and transformation. all without managing infrastructure.\n",
      "\n",
      "AWS Athena\n",
      "\n",
      "AWS Athena is an interactive query service that simplifies data analysis directly in Amazon S3 using standard SQL. As a serverless platform, there’s no need to manage or provision servers; just point Athena at your S3 data, define your schema (usually with AWS Glue), and begin running SQL queries. It’s frequently utilised for ad hoc analysis, reporting, and exploration of large datasets in formats such as CSV, JSON, ORC, or Parquet.\n",
      "\n",
      "Iceberg tables\n",
      "\n",
      "Iceberg tables are an open table format for datasets that provide database-like capabilities for data stored in data lakes, such as Amazon S3 object storage. Traditionally, on S3, you can create, read, and delete objects(files), but updating them is not possible. The Iceberg format addresses that limitation while also offering other benefits, including ACID transactions, schema evolution, hidden partitioning, and time-travel features.\n",
      "\n",
      "DuckDB\n",
      "\n",
      "DuckDB is an in-memory analytical database written in C++ and designed for analytical SQL workloads. Since its release a couple of years ago, it has grown in popularity and is now one of the premier data processing tools used by data engineers and scientists, thanks to its grounding in SQL, performance, and versatility.\n",
      "\n",
      "Scenario overview\n",
      "\n",
      "Let’s say you have been tasked with building a small “warehouse-lite” analytics table for order events, but you don’t want to adopt a heavyweight platform just yet. You need:\n",
      "\n",
      "Safe writes (no broken readers, no partial commits)\n",
      "\n",
      "(no broken readers, no partial commits) Row-level changes (UPDATE/DELETE/MERGE, not only append)\n",
      "\n",
      "changes (UPDATE/DELETE/MERGE, not only append) Point-in-time reads (for audits and debugging)\n",
      "\n",
      "(for audits and debugging) Local analytics against production-accurate data for quick checks\n",
      "\n",
      "What we’ll build\n",
      "\n",
      "Create an Iceberg table in Glue & S3 via Athena Load and mutate rows (INSERT/UPDATE/DELETE/MERGE) Time travel to prior snapshots (by timestamp and by snapshot ID) Keep it fast with OPTIMIZE and VACUUM Read the same table locally from DuckDB (S3 access via DuckDB Secrets) See how to add new records to our table using Glue Spark code\n",
      "\n",
      "So, in a nutshell, we’ll be using:-\n",
      "\n",
      "S3 for data storage\n",
      "\n",
      "Glue Catalogue for table metadata/discovery\n",
      "\n",
      "Athena for serverless SQL reads and writes\n",
      "\n",
      "DuckDB for cheap, local analytics against the same Iceberg table\n",
      "\n",
      "Spark for processing grunt\n",
      "\n",
      "The key takeaway from our perspective is that by using the above technologies, we will be able to perform database-like queries on object storage.\n",
      "\n",
      "Setting up our development environment\n",
      "\n",
      "I prefer to isolate local tooling in a separate environment. Use any tool you like to do this; I’ll show using conda since that’s what I usually do. For demo purposes, I’ll be running all the code within a Jupyter Notebook environment.\n",
      "\n",
      "# create and activate a local env conda create -n iceberg-demo python=3.11 -y conda activate iceberg-demo # install duckdb CLI + Python package and awscli for quick tests pip install duckdb awscli jupyter\n",
      "\n",
      "Prerequisites\n",
      "\n",
      "As we’ll be using AWS services, you’ll need an AWS account. Also,\n",
      "\n",
      "An S3 bucket for the data lake (e.g., s3://my-demo-lake/warehouse/ )\n",
      "\n",
      ") A Glue database (we’ll create one)\n",
      "\n",
      "Athena Engine Version 3 i n your workgroup\n",
      "\n",
      "n your workgroup An IAM role or user for Athena with S3 + Glue permissions\n",
      "\n",
      "1/ Athena setup\n",
      "\n",
      "Once you’ve signed into AWS, open Athena in the console and set your workgroup, engine version and S3 output location (for query results). To do this, look for a hamburger-style menu icon on the top left of the Athena home screen. Click on it to bring up a new menu block on the left. In there, you should see an Administration-> Workgroups link. You will automatically be assigned to the primary workgroup. You can stick with this or create a new one if you like. Whichever option you choose, edit it and ensure that the following options are selected.\n",
      "\n",
      "Analytics Engine — Athena SQL. Manually set the engine version to 3.0.\n",
      "\n",
      "Select customer-managed query result configuration and enter the required bucket and account information.\n",
      "\n",
      "2/ Create an Iceberg table in Athena\n",
      "\n",
      "We’ll store order events and let Iceberg manage partitioning transparently. I’ll use a “hidden” partition on the day of the timestamp to spread writes/reads. Go back to the Athena home page and launch the Trino SQL query editor. Your screen should look like this.\n",
      "\n",
      "Image from AWS website\n",
      "\n",
      "Type in and run the following SQL. Change bucket/table names to suit.\n",
      "\n",
      "-- This automatically creates a Glue database -- if you don't have one already CREATE DATABASE IF NOT EXISTS analytics;\n",
      "\n",
      "CREATE TABLE analytics.sales_iceberg ( order_id bigint, customer_id bigint, ts timestamp, status string, amount_usd double ) PARTITIONED BY (day(ts)) LOCATION 's3://your_bucket/warehouse/sales_iceberg/' TBLPROPERTIES ( 'table_type' = 'ICEBERG', 'format' = 'parquet', 'write_compression' = 'snappy' )\n",
      "\n",
      "3) Load and mutate data (INSERT / UPDATE / DELETE / MERGE)\n",
      "\n",
      "Athena supports real Iceberg DML, allowing you to insert rows, update and delete records, and upsert using the MERGE statement. Under the hood, Iceberg uses snapshot-based ACID with delete files; readers stay consistent while writers work in parallel.\n",
      "\n",
      "Seed a few rows.\n",
      "\n",
      "INSERT INTO analytics.sales_iceberg VALUES (101, 1, timestamp '2025-08-01 10:00:00', 'created', 120.00), (102, 2, timestamp '2025-08-01 10:05:00', 'created', 75.50), (103, 2, timestamp '2025-08-02 09:12:00', 'created', 49.99), (104, 3, timestamp '2025-08-02 11:47:00', 'created', 250.00);\n",
      "\n",
      "A quick sanity check.\n",
      "\n",
      "SELECT * FROM analytics.sales_iceberg ORDER BY order_id; order_id | customer_id | ts | status | amount_usd ----------+-------------+-----------------------+----------+----------- 101 | 1 | 2025-08-01 10:00:00 | created | 120.00 102 | 2 | 2025-08-01 10:05:00 | created | 75.50 103 | 2 | 2025-08-02 09:12:00 | created | 49.99 104 | 3 | 2025-08-02 11:47:00 | created | 250.00\n",
      "\n",
      "Update and delete.\n",
      "\n",
      "UPDATE analytics.sales_iceberg SET status = 'paid' WHERE order_id IN (101, 102)\n",
      "\n",
      "-- removes order 103 DELETE FROM analytics.sales_iceberg WHERE status = 'created' AND amount_usd < 60\n",
      "\n",
      "Idempotent upserts with MERGE\n",
      "\n",
      "Let’s treat order 104 as refunded and create a new order 105.\n",
      "\n",
      "MERGE INTO analytics.sales_iceberg AS t USING ( VALUES (104, 3, timestamp '2025-08-02 11:47:00', 'refunded', 250.00), (105, 4, timestamp '2025-08-03 08:30:00', 'created', 35.00) ) AS s(order_id, customer_id, ts, status, amount_usd) ON s.order_id = t.order_id WHEN MATCHED THEN UPDATE SET customer_id = s.customer_id, ts = s.ts, status = s.status, amount_usd = s.amount_usd WHEN NOT MATCHED THEN INSERT (order_id, customer_id, ts, status, amount_usd) VALUES (s.order_id, s.customer_id, s.ts, s.status, s.amount_usd);\n",
      "\n",
      "You can now re-query to see: 101/102 → paid, 103 deleted, 104 → refunded, and 105 → created. (If you’re running this in a “real” account, you’ll notice the S3 object count ticking up — more on maintenance shortly.)\n",
      "\n",
      "SELECT * FROM analytics.sales_iceberg ORDER BY order_id # order_id customer_id ts status amount_usd 1 101 1 2025-08-01 10:00:00.000000 paid 120.0 2 105 4 2025-08-03 08:30:00.000000 created 35.0 3 102 2 2025-08-01 10:05:00.000000 paid 75.5 4 104 3 2025-08-02 11:47:00.000000 refunded 250.0\n",
      "\n",
      "4) Time travel (and version travel)\n",
      "\n",
      "This is where the real value of using Iceberg shines. You can query the table as it looked at a moment in time or by a specific snapshot ID. In Athena, use this syntax,\n",
      "\n",
      "-- Time travel to noon on Aug 2 (UTC) SELECT order_id, status, amount_usd FROM analytics.sales_iceberg FOR TIMESTAMP AS OF TIMESTAMP '2025-08-02 12:00:00 UTC' ORDER BY order_id; -- Or Version travel (replace the id with an actual snapshot id from your table) SELECT * FROM analytics.sales_iceberg FOR VERSION AS OF 949530903748831860;\n",
      "\n",
      "To get the various version (snapshot) IDs associated with a particular table, use this query.\n",
      "\n",
      "SELECT * FROM \"analytics\".\"sales_iceberg$snapshots\" ORDER BY committed_at DESC;\n",
      "\n",
      "5) Keeping your data healthy: OPTIMIZE and VACUUM\n",
      "\n",
      "Row-level writes (UPDATE/DELETE/MERGE) create many delete files and can fragment data. Two statements keep things fast and storage-friendly:\n",
      "\n",
      "OPTIMIZE … REWRITE DATA USING BIN_PACK — compacts small/fragmented files and folds deletes into data\n",
      "\n",
      "— compacts small/fragmented files and folds deletes into data VACUUM — expires old snapshots + cleans orphan files\n",
      "\n",
      "-- compact \"hot\" data (yesterday) and merge deletes OPTIMIZE analytics.sales_iceberg REWRITE DATA USING BIN_PACK WHERE ts >= date_trunc('day', current_timestamp - interval '1' day); -- expire old snapshots and remove orphan files VACUUM analytics.sales_iceberg;\n",
      "\n",
      "6) Local analytics with DuckDB (read-only)\n",
      "\n",
      "It’s great to be able to sanity-check production tables from a laptop without having to run a cluster. DuckDB’s httpfs + iceberg extensions make this simple.\n",
      "\n",
      "6.1 Install & load extensions\n",
      "\n",
      "Open your Jupyter notebook and type in the following.\n",
      "\n",
      "# httpfs gives S3 support; iceberg adds Iceberg readers. import duckdb as db db.sql(\"install httpfs; load httpfs;\") db.sql(\"install iceberg; load iceberg;\")\n",
      "\n",
      "6.2 Provide S3 credentials to DuckDB the “right” way (Secrets)\n",
      "\n",
      "DuckDB has a small but powerful secrets manager. The most robust setup in AWS is the credential chain provider, which reuses whatever the AWS SDK can find (environment variables, IAM role, etc.). Therefore, you will need to ensure that, for instance, your AWS CLI credentials are configured.\n",
      "\n",
      "db.sql(\"\"\"CREATE SECRET ( TYPE s3, PROVIDER credential_chain )\"\"\")\n",
      "\n",
      "After that, any s3://… reads in this DuckDB session will use the secret data.\n",
      "\n",
      "6.3 Point DuckDB at the Iceberg table’s metadata\n",
      "\n",
      "The most explicit way is to reference a concrete metadata file (e.g., the latest one in your table’s metadata/ folder:)\n",
      "\n",
      "To get a list of those, use this query\n",
      "\n",
      "result = db.sql(\"\"\" SELECT * FROM glob('s3://your_bucket/warehouse/**') ORDER BY file \"\"\") print(result) ... ... s3://your_bucket_name/warehouse/sales_iceberg/metadata/00000-942a25ce-24e5-45f8-ae86-b70d8239e3bb.metadata.json │ s3://your_bucket_name/warehouse/sales_iceberg/metadata/00001-fa2d9997-590e-4231-93ab-642c0da83f19.metadata.json │ s3://your_bucket_name/warehouse/sales_iceberg/metadata/00002-0da3a4af-64af-4e46-bea2-0ac450bf1786.metadata.json │ s3://your_bucket_name/warehouse/sales_iceberg/metadata/00003-eae21a3d-1bf3-4ed1-b64e-1562faa445d0.metadata.json │ s3://your_bucket_name/warehouse/sales_iceberg/metadata/00004-4a2cff23-2bf6-4c69-8edc-6d74c02f4c0e.metadata.json ... ... ...\n",
      "\n",
      "Look for the metadata.json file with the highest numbered start to the file name, 00004 in my case. Then, you can use that in a query like this to retrieve the latest position of your underlying table.\n",
      "\n",
      "# Use the highest numbered metadata file (00004 appears to be the latest in my case) result = db.sql(\"\"\" SELECT * FROM iceberg_scan('s3://your_bucket/warehouse/sales_iceberg/metadata/00004-4a2cff23-2bf6-4c69-8edc-6d74c02f4c0e.metadata.json') LIMIT 10 \"\"\") print(result) ┌──────────┬─────────────┬─────────────────────┬──────────┬────────────┐ │ order_id │ customer_id │ ts │ status │ amount_usd │ │ int64 │ int64 │ timestamp │ varchar │ double │ ├──────────┼─────────────┼─────────────────────┼──────────┼────────────┤ │ 105 │ 4 │ 2025-08-03 08:30:00 │ created │ 35.0 │ │ 104 │ 3 │ 2025-08-02 11:47:00 │ refunded │ 250.0 │ │ 101 │ 1 │ 2025-08-01 10:00:00 │ paid │ 120.0 │ │ 102 │ 2 │ 2025-08-01 10:05:00 │ paid │ 75.5 │ └──────────┴─────────────┴─────────────────────┴──────────┴────────────┘\n",
      "\n",
      "Want a specific snapshot? Use this to get a list.\n",
      "\n",
      "result = db.sql(\"\"\" SELECT * FROM iceberg_snapshots('s3://your_bucket/warehouse/sales_iceberg/metadata/00004-4a2cff23-2bf6-4c69-8edc-6d74c02f4c0e.metadata.json') \"\"\") print(\"Available Snapshots:\") print(result) Available Snapshots: ┌─────────────────┬─────────────────────┬─────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ sequence_number │ snapshot_id │ timestamp_ms │ manifest_list │ │ uint64 │ uint64 │ timestamp │ varchar │ ├─────────────────┼─────────────────────┼─────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤ │ 1 │ 5665457382547658217 │ 2025-09-09 10:58:44.225 │ s3://your_bucket/warehouse/sales_iceberg/metadata/snap-5665457382547658217-1-bb7d0497-0f97-4483-98e2-8bd26ddcf879.avro │ │ 3 │ 8808557756756599285 │ 2025-09-09 11:19:24.422 │ s3://your_bucket/warehouse/sales_iceberg/metadata/snap-8808557756756599285-1-f83d407d-ec31-49d6-900e-25bc8d19049c.avro │ │ 2 │ 31637314992569797 │ 2025-09-09 11:08:08.805 │ s3://your_bucket/warehouse/sales_iceberg/metadata/snap-31637314992569797-1-000a2e8f-b016-4d91-9942-72fe9ddadccc.avro │ │ 4 │ 4009826928128589775 │ 2025-09-09 11:43:18.117 │ s3://your_bucket/warehouse/sales_iceberg/metadata/snap-4009826928128589775-1-cd184303-38ab-4736-90da-52e0cf102abf.avro │ └─────────────────┴─────────────────────┴─────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "7) Optional extra: Writing from Spark/Glue\n",
      "\n",
      "If you prefer Spark for larger batch writes, Glue can read/write Iceberg tables registered in the Glue Catalogue. You’ll probably still want to use Athena for ad-hoc SQL, time travel, and maintenance, but large CTAS/ETL can come via Glue jobs. (Just be aware that version compatibility and AWS LakeFormation permissions can bite, as Glue and Athena may lag slightly on Iceberg versions.)\n",
      "\n",
      "Here’s an example of some Glue Spark code that inserts a few new data rows, starting at order_id = 110, into our existing table. Before running this, you should add the following Glue job parameter (under Glue Job Details-> Advanced Parameters-> Job parameters.\n",
      "\n",
      "Key: --conf Value: spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "\n",
      "import sys import random from datetime import datetime from pyspark.context import SparkContext from awsglue.utils import getResolvedOptions from awsglue.context import GlueContext from awsglue.job import Job from pyspark.sql import Row # -------------------------------------------------------- # Init Glue job # -------------------------------------------------------- args = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) # -------------------------------------------------------- # Force Iceberg + Glue catalog configs (dynamic only) # -------------------------------------------------------- spark.conf.set(\"spark.sql.catalog.glue_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") spark.conf.set(\"spark.sql.catalog.glue_catalog.warehouse\", \"s3://your_bucket/warehouse/\") spark.conf.set(\"spark.sql.catalog.glue_catalog.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") spark.conf.set(\"spark.sql.catalog.glue_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") spark.conf.set(\"spark.sql.defaultCatalog\", \"glue_catalog\") # -------------------------------------------------------- # Debug: list catalogs to confirm glue_catalog is registered # -------------------------------------------------------- print(\"Current catalogs available:\") spark.sql(\"SHOW CATALOGS\").show(truncate=False) # -------------------------------------------------------- # Read existing Iceberg table (optional) # -------------------------------------------------------- existing_table_df = glueContext.create_data_frame.from_catalog( database=\"analytics\", table_name=\"sales_iceberg\" ) print(\"Existing table schema:\") existing_table_df.printSchema() # -------------------------------------------------------- # Create 5 new records # -------------------------------------------------------- new_records_data = [] for i in range(5): order_id = 110 + i record = { \"order_id\": order_id, \"customer_id\": 1000 + (i % 10), \"price\": round(random.uniform(10.0, 500.0), 2), \"created_at\": datetime.now(), \"status\": \"completed\" } new_records_data.append(record) new_records_df = spark.createDataFrame([Row(**r) for r in new_records_data]) print(f\"Creating {new_records_df.count()} new records:\") new_records_df.show() # Register temp view for SQL insert new_records_df.createOrReplaceTempView(\"new_records_temp\") # -------------------------------------------------------- # Insert into Iceberg table (alias columns as needed) # -------------------------------------------------------- spark.sql(\"\"\" INSERT INTO analytics.sales_iceberg (order_id, customer_id, ts, status, amount_usd) SELECT order_id, customer_id, created_at AS ts, status, price AS amount_usd FROM new_records_temp \"\"\") print(\" Sccessfully added 5 new records to analytics.sales_iceberg\") # -------------------------------------------------------- # Commit Glue job # -------------------------------------------------------- job.commit()\n",
      "\n",
      "Double-check with Athena.\n",
      "\n",
      "select * from analytics.sales_iceberg order by order_id # order_id customer_id ts status amount_usd 1 101 1 2025-08-01 10:00:00.000000 paid 120.0 2 102 2 2025-08-01 10:05:00.000000 paid 75.5 3 104 3 2025-08-02 11:47:00.000000 refunded 250.0 4 105 4 2025-08-03 08:30:00.000000 created 35.0 5 110 1000 2025-09-10 16:06:45.505935 completed 248.64 6 111 1001 2025-09-10 16:06:45.505947 completed 453.76 7 112 1002 2025-09-10 16:06:45.505955 completed 467.79 8 113 1003 2025-09-10 16:06:45.505963 completed 359.9 9 114 1004 2025-09-10 16:06:45.506059 completed 398.52\n",
      "\n",
      "Future Steps\n",
      "\n",
      "From here, you could:\n",
      "\n",
      "Create more tables with data.\n",
      "\n",
      "Add schema evolution’\n",
      "\n",
      "Experiment with partition evolution (e.g., change table partition from day → hour as volumes grow),\n",
      "\n",
      "Add scheduled maintenance. For example, EventBridge , Step, and Lambdas could be used to run OPTIMIZE/VACUUM on a scheduled cadence.\n",
      "\n",
      "Summary\n",
      "\n",
      "In this article, I’ve tried to provide a clear path for building an Iceberg data lakehouse on AWS. It should serve as a guide for data engineers who want to connect simple object storage with complex enterprise data warehouses.\n",
      "\n",
      "Hopefully, I’ve shown that building a Data Lakehouse—a system that combines the low cost of data lakes with the transactional integrity of warehouses—doesn’t necessarily require extensive infrastructure deployment. And while creating a full lakehouse is something that evolves over a long time, I hope I’ve convinced you that you really can make the bones of one in an afternoon.\n",
      "\n",
      "By leveraging Apache Iceberg on a cloud storage system like Amazon S3, I demonstrated how to transform static files into dynamic, managed tables capable of ACID transactions, row-level mutations (MERGE, UPDATE, DELETE), and time travel, all without provisioning a single server.\n",
      "\n",
      "I also showed that by using new analytic tools such as DuckDB, it’s possible to read small to medium data lakes locally. And when your data volumes grow and get too big for local processing, I showed how easy it was to step up to an enterprise class data processing platform like Spark.\n",
      "\n",
      "Title: The Best Data Scientists are Always Learning\n",
      "Author: ['Jarom Hulet', 'Shreya Rao', 'Piero Paialunga', 'Luis Fernando Pérez Armas', 'Luigi Battistoni', 'Tds Editors', 'Vadim Arzamasov', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-04 12:00:00+00:00\n",
      "Content: it’s possible to fully master every topic in data science?\n",
      "\n",
      "With data science covering such a broad range of areas — statistics, programming, optimization, experimental design, data storytelling, generative AI, to name a few — I personally don’t think so.\n",
      "\n",
      "Here’s a narrower question. Is it possible to fully master a single topic within data science? Sure, you can become an expert in some areas, but can you ever reach a point where there’s nothing left to learn? Again, I really don’t think so.\n",
      "\n",
      "Every data scientist has something to learn, even those with extensive experience. The aim of my writing is to provide some insights from my learning journey that I hope will help you in yours.\n",
      "\n",
      "This is the first part in a two-part series. In this article I’ll cover:\n",
      "\n",
      "Why you should continuously learn as a data scientist How to come up with topics to study\n",
      "\n",
      "Let’s jump in!\n",
      "\n",
      "1. Why continuously learn as a data scientist?\n",
      "\n",
      "Continuous learners differentiate themselves\n",
      "\n",
      "When I was younger, I studied Spanish in a group setting. Something interesting happened after the group became conversational. Many students stopped studying, they were content with their level of proficiency. Others continued to do daily study and practice.\n",
      "\n",
      "At first, there wasn’t much difference between the two groups. But over time, those who continued learning pulled ahead. Their fluency, vocabulary, and confidence compounded, while the others plateaued.\n",
      "\n",
      "Unfortunately, the same thing can happen to data scientists. Some stop learning after they have developed sufficient skills to do their jobs well. Similar to the Spanish cohort, early in a career, continuous learners and content data scientists will look similar. But as time passes, those who keep learning start to stand out. Their knowledge compounds, their judgment improves, and their ability to solve complex problems deepens.\n",
      "\n",
      "Continuous learners and content data scientists will look similar early in their careers. But as time passes, those who keep learning will start to stand out.\n",
      "\n",
      "Continuous learners shine because they can use their knowledge to come up with smarter solutions to problems. They will have a more mature understanding of data science tools and how to use them correctly in their work.\n",
      "\n",
      "Learning brings fulfillment (for most)\n",
      "\n",
      "This is a little bit fluffy, so I’ll keep it short. But I really do enjoy learning. I get a lot of fulfillment and satisfaction from taking some time to invest in myself and master new topics. If you like the idea of continuous learning, you will probably get a lot of fulfillment from it as well!\n",
      "\n",
      "2. How to come up with things to study\n",
      "\n",
      "We’ve established the value of career-long learning in the previous section, let’s talk about how to come up with things to study.\n",
      "\n",
      "The best thing about studying on your own is that no one is telling you what to study. The worst thing about studying on your own is that no one is telling you what to study.\n",
      "\n",
      "You’re not in school anymore, which is great. No more deadlines, no more exams and, perhaps most importantly, no more tuition. But you also lose the curated list of topics to study with corresponding materials, texts and lectures. Creating that is your job now! The flexibility of developing your own study plan is amazing. But the ambiguous, undirected space can be daunting.\n",
      "\n",
      "Over the years, I’ve developed three approaches to come up with study subjects that work really well for me. My goal is that they can be a good starter for you to develop your own approach. Ultimately, you’ll have to find what works best for you.\n",
      "\n",
      "Let’s get into the three approaches.\n",
      "\n",
      "Topics from projects at work\n",
      "\n",
      "If you are working as a data scientist, your projects will give you a rich supply of ‘deep dive’ study topics. This approach is pretty straight forward – study techniques/subjects that are pertinent to your work. Give special focus to areas where your understanding is the weakest.\n",
      "\n",
      "For example, if you are designing an experiment, study experimental design. If you are solving an optimization problem, study optimization.\n",
      "\n",
      "One great benefit of this approach is that it makes you better at your job immediately. You will have a deeper understanding of the problems you’re facing, and you’ll be able apply that understanding right away.\n",
      "\n",
      "Following a “web” of topics\n",
      "\n",
      "Data science is such a rich field of study, you can always go deeper on any given subject and so many topics are interrelated.\n",
      "\n",
      "When studying, you’ll find many ‘tangent’ topics that are related to the topic at hand. I often take note of those topics and come back to them later. I call this the ‘web of topics.’ This is a great technique because you slowly build up a web of understanding around groups or related topics. This gives a deep knowledge that will differentiate you.\n",
      "\n",
      "Here is an example of a small web of topics around logistic regression. I only included a few topics for the illustration – I’m sure you could come up with many more. Each one of the topics in the web have their own web, making a mega-web of related study topics.\n",
      "\n",
      "Image generated by Dall-e based on specific prompt from user\n",
      "\n",
      "I could keep going, but you get the point. Any individual topic will have a huge web of related topics. Keep a list of these somewhere and when you are done with the current subject you will always have a backlog of pertinent topics to dive into!\n",
      "\n",
      "Note: Your web of topics needs to start somewhere. If you are having a hard time kicking it off, I recommend reading ‘The Elements of Statistical Learning’ or ‘Introduction to Statistical Learning’ by Hastie, Tibshirani and Friedman. These are foundational reads that will get you into a great web of study topics.\n",
      "\n",
      "Discovery channels\n",
      "\n",
      "Work projects and topic webs are two excellent approaches to curating a list of study subjects. However, these two approaches have a major blind spot. If you only use these techniques, you won’t be exposed to topics that don’t show up at work or in your natural sequence of study. There are likely really important topics that will be left untouched.\n",
      "\n",
      "I use ‘discovery channels’ to help catch important topics that don’t come up organically. A discovery channel is any source of content that expose me to topics that are independent from my other studies. My main source of discovery channels are Towards Data Science, podcasts and YouTube channels.\n",
      "\n",
      "\n",
      "\n",
      "My current favorite ‘discovery channel’ sources – image by author\n",
      "\n",
      "When choosing a discovery channel, it is important to choose a source that covers a broad range of topics. If I, for example, followed a podcast that focused on experimental design – I probably wouldn’t source a wide array of topics to study from it. It might be a great resource for DOE study, but it wouldn’t be a good discovery channel.\n",
      "\n",
      "I spend a relatively small percentage of my overall study effort on discovery channels, but they play the very important role in my studies.\n",
      "\n",
      "Wrapping it up\n",
      "\n",
      "I hope that this article leaves you feeling motivated to start independently studying if you aren’t already or has given you additional motivation to keep going if you already are studying. I also hope that I’ve given you a few fresh ideas on how to come up with things to study.\n",
      "\n",
      "In a few weeks I’ll be posting part 2 of this article that will cover how to (1) avoid burnout, (2) choose learning strategies and (3) leverage solitude to cement and deepen your knowledge – stay tuned!\n",
      "\n",
      "Title: The Architecture Behind Web Search in AI Chatbots\n",
      "Author: ['Ida Silfverskiöld', 'Tomaz Bratanic', 'Jacopo Tagliabue', 'Laurent Picard', 'Deepak Krishnamurthy', 'Erika G. Gonçalves', 'Parul Pandey', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-04 06:19:55+00:00\n",
      "Content: or Claude to “search the web,” it isn’t just answering from its training data. It’s calling a separate search system.\n",
      "\n",
      "Most people know that part.\n",
      "\n",
      "What’s less clear is how much traditional search engines matter and how much has been built on top of them.\n",
      "\n",
      "All of it isn’t fully public, so I’m doing some mental deduction here. But we can use different hints from looking at larger systems to build a useful mental model.\n",
      "\n",
      "We’ll go through query optimization, how search engines are used for discovery, chunking content, “on-the-fly” retrieval, and how you could potentially reverse-engineer a system like this to build a “GEO [ ] scoring system.”\n",
      "\n",
      "If you’re familiar with RAG, some of this will be repetition, but it can still be useful to see how larger systems split the pipeline into a discovery phase and a retrieval phase.\n",
      "\n",
      "If you’re short on time, you can read the TL;DR.\n",
      "\n",
      "TL;DR\n",
      "\n",
      "Web search in these AI chatbots is likely a two-part process. The first part leans on traditional search engines to find and rank candidate docs. In the second part, they fetch the content from those URLs and pull out the most relevant passages using passage-level retrieval.\n",
      "\n",
      "The big change (from traditional SEO) is query rewriting and passage-level chunking, which let lower-ranked pages outrank higher ones if their specific paragraphs match the question better.\n",
      "\n",
      "The technical process\n",
      "\n",
      "The companies behind Claude and ChatGPT aren’t fully transparent about how their web search systems work within the UI chat, but we can infer a lot by piecing things together.\n",
      "\n",
      "We know they lean on search engines to find candidates, at this scale, it would be absurd not to. We also know that what the LLM actually sees are pieces of text (chunks or passages) when grounding their answer.\n",
      "\n",
      "This strongly hints at some kind of embedding-based retrieval over those chunks rather than over full pages.\n",
      "\n",
      "This process has several parts, so we’ll go through it step by step.\n",
      "\n",
      "Query re-writing & fan-out\n",
      "\n",
      "First, we’ll look at how the system cleans up human queries and expands them. We’ll cover the rewrite step, the fan-out step, and why this matters for both engineering and SEO.\n",
      "\n",
      "we’ll start at query rewriting — showing the entire pipeline we’re going through\n",
      "\n",
      "I think this part might be the most transparent, and the one most people seem to agree on online.\n",
      "\n",
      "The query optimization step is about taking a human query and turning it into something more precise. For example, “please search for those red shoes we talked about earlier” becomes “brown-red Nike sneakers.”\n",
      "\n",
      "The fan-out step, on the other hand, is about generating additional rewrites. So if a user asks about hiking routes near me, the system might try things like “beginner hikes near Stockholm,” “day hikes near Stockholm public transport,” or “family-friendly trails near Stockholm.”\n",
      "\n",
      "This is different from just using synonyms, which traditional search engines are already optimized for.\n",
      "\n",
      "If this is the first time you’re hearing about it and you’re unconvinced, take a look at Google’s own docs on AI query fan-out or do a bit of digging around query rewriting.\n",
      "\n",
      "To what extent this actually works, we can’t know. They may not fan it out that much and just work with a single query, then send additional ones down the pipeline if the results are lackluster.\n",
      "\n",
      "What we can say is that it’s probably not a big model doing this part. If you look at the research, Ye et al. explicitly use an LLM to generate strong rewrites, then distill that into a smaller rewriter to avoid latency and cost overhead.\n",
      "\n",
      "As for what this part of the pipeline means, for engineering, it just means you want to clean up the messy human queries and turn them into something that has a higher hit rate.\n",
      "\n",
      "For the business and SEO people out there, it means those human queries you’ve been optimizing for are getting transformed into more robotic, document-shaped ones.\n",
      "\n",
      "SEO, as I understand it, used to care a lot about matching the exact long-tail phrase in titles and headings. If someone searched for “best running shoes for bad knees,” you’d stick to that exact string.\n",
      "\n",
      "What you need to care about now is also entities, attributes, and relationships.\n",
      "\n",
      "So, if a user asks for “something for dry skin,” the rewrites might include things like “moisturizer,” “occlusive,” “humectant,” “ceramides,” “fragrance-free,” “avoid alcohols” and not just “how would I find a good product for dry skin.”\n",
      "\n",
      "But let’s be clear so there’s no confusion: we can’t see the internal rewrites themselves, so these are just examples.\n",
      "\n",
      "If you’re interested in this part, you can dig deeper. I bet there are plenty of papers out there on how to do this well.\n",
      "\n",
      "Let’s move on to what these optimized queries are actually used for.\n",
      "\n",
      "Using search engines (for doc level discovery)\n",
      "\n",
      "It’s pretty common knowledge by now that, to get up-to-date answers, most AI bots rely on traditional search engines. That’s not the whole story, but it does cut the web down to something smaller to work with.\n",
      "\n",
      "next up doc discovery — showing the entire pipeline we’re going through\n",
      "\n",
      "I’m assuming the full web is too large, too noisy, and too fast-changing for an LLM pipeline to pull raw content directly. So by using already established search engines, you get a way to narrow the universe.\n",
      "\n",
      "If you look at larger RAG pipelines that work with millions of documents, they do something similar. I.e. using a filter of some sort to decide which documents are important and worth further processing.\n",
      "\n",
      "For this part, we do have proof.\n",
      "\n",
      "Both OpenAI and Anthropic have said they use third-party search engines like Bing and Brave, alongside their own crawlers.\n",
      "\n",
      "Perplexity may have built out this part on their own by now, but in the beginning, they would have done the same.\n",
      "\n",
      "We also have to consider that traditional search engines like Google and Bing have already solved the hardest problems. They’re an established technology that handles things like language detection, authority scoring, domain trust, spam filtering, recency, geo-biasing, personalization, and so on.\n",
      "\n",
      "Throwing all of that away to embed the entire web yourself seems unlikely. So I’m guessing they lean on those systems instead of rebuilding them.\n",
      "\n",
      "However, we don’t know how many results they actually fetch per query, whether it’s just the top 20 or 30. One unofficial article compared citations from ChatGPT and Bing, looked at the ranking order, and found that some came from as far down as 22nd place. If true, this suggests you need to aim for top-20-ish visibility.\n",
      "\n",
      "Furthermore, we also don’t know what other metrics they use to decide what surfaces from there. This article argues that AI engines heavily favor earned media rather than official sites or socials, so there’s more going on.\n",
      "\n",
      "Still, the search engine’s job (whether it’s fully third-party or a mix) is discovery. It ranks the URL based on authority and keywords. It might include a snippet of information, but that alone won’t be enough to answer the question.\n",
      "\n",
      "If the model relied only on the snippet, plus the title and URL, it would likely hallucinate the details. That’s not enough context.\n",
      "\n",
      "So this pushes us toward a two-stage architecture, where a retrieval step is baked in — which we’ll get to soon.\n",
      "\n",
      "What does this mean in terms of SEO?\n",
      "\n",
      "It means you still need to rank high in traditional search engines to be included in that initial batch of documents that gets processed. So, yes, classic SEO still matters.\n",
      "\n",
      "But it may also mean you need to think about potential new metrics they might be using to rank those results.\n",
      "\n",
      "This stage is all about narrowing the universe to a few pages worth digging into, using established search tech plus internal knobs. Everything else (the “it returns passages of information” part) comes after this step, using standard retrieval techniques.\n",
      "\n",
      "Crawl, chunk & retrieve\n",
      "\n",
      "Now let’s move on to what happens when the system has identified a handful of interesting URLs.\n",
      "\n",
      "Once a small set of URLs passes the first filter, the pipeline is fairly straightforward: crawl the page, break it into pieces, embed those pieces, retrieve the ones that match the query, and then re-rank them. This is what’s called retrieval.\n",
      "\n",
      "next up chunking, retrieval — showing the entire pipeline we’re going through\n",
      "\n",
      "I call it on-the-fly here because the system only embeds chunks once a URL becomes a candidate, then it caches those embeddings for reuse. This part might be new if you’re already familiar with retrieval.\n",
      "\n",
      "To crawl the page, they use their own crawlers. For OpenAI, this is OAI-SearchBot, which fetches the raw HTML so it can be processed. Crawlers don’t execute JavaScript. They rely on server-rendered HTML, so the same SEO rules apply: content needs to be accessible.\n",
      "\n",
      "Once the HTML is fetched, the content has to be turned into something searchable.\n",
      "\n",
      "If you’re new to this, it might feel like the AI “scans the document,” but that’s not what happens. Scanning entire pages per query would be too slow and too expensive.\n",
      "\n",
      "Instead, pages are split into passages, usually guided by HTML structure: headings, paragraphs, lists, section breaks, that kind of thing. These are called chunks in the context of retrieval.\n",
      "\n",
      "Each chunk becomes a small, self-contained unit. Token-wise, you can see from Perplexity UI citations that chunks are on the order of tens of tokens, maybe around 150, not 1,000. That’s about 110–120 words.\n",
      "\n",
      "After chunking, those units are embedded using both sparse and dense vectors. This enables the system to run hybrid search and match a query both semantically and by keyword.\n",
      "\n",
      "If you’re new to semantic search, in short, it means the system searches for meaning instead of exact words. So a query like “symptoms of iron deficiency” and “signs your body is low on iron” would still land near each other in embedding space. You can read more on embeddings here if you’re keen to learn how it works.\n",
      "\n",
      "Once a popular page has been chunked and embedded, those embeddings are probably cached. No one is re-embedding the same StackOverflow answer thousands of times a day.\n",
      "\n",
      "This is obviously why the system feels so fast, probably the hot 95–98% of the web that actually gets cited is already embedded, and cached aggressively.\n",
      "\n",
      "We don’t know to what extent though and how much they pre-embed to make sure the system runs fast for popular queries.\n",
      "\n",
      "Now the system needs to figure out which chunks matter. It uses the embeddings for each chunk of text to compute a score for both semantic and keyword matching.\n",
      "\n",
      "It picks the chunks with the highest scores. This can be anything from 10 to 50 top-scoring chunks.\n",
      "\n",
      "From here, most mature systems will use a re-ranker (cross-encoder) to process those top chunks again, doing another round of ranking. This is the “fix the retrieval mess” stage, because unfortunately retrieval isn’t always completely reliable for a lot of reasons.\n",
      "\n",
      "Although they say nothing about using a cross-encoder, Perplexity is one of the few that documents their retrieval process openly.\n",
      "\n",
      "Their Search API says they “divide documents into fine-grained units” and score those units individually so they can return the “most relevant snippets already ranked.”\n",
      "\n",
      "What does this all mean for SEO? If the system is doing retrieval like this, your page isn’t treated as one big blob.\n",
      "\n",
      "It’s broken into pieces (often paragraph or heading level) and those pieces are what get scored. The full page matters during discovery, but once retrieval begins, it’s the chunks that matter.\n",
      "\n",
      "That means each chunk needs to answer the user’s question.\n",
      "\n",
      "It also means that if your important information isn’t contained inside a single chunk, the system can lose context. Retrieval isn’t magic. The model never sees your full page.\n",
      "\n",
      "So now we’ve covered the retrieval stage: where the system crawls pages, chops them into units, embeds those units, and then uses hybrid retrieval and re-ranking to pull out only the passages that can answer the user’s question.\n",
      "\n",
      "Doing another pass & handing over chunks to LLM\n",
      "\n",
      "Now let’s move on to what happens after the retrieval part, including the “continuing to search” feature and handing the chunks to the main LLM.\n",
      "\n",
      "next up checking content + handing it over to the LLM\n",
      "\n",
      "Once the system has identified a few high-ranking chunks, it has to decide whether they’re good enough or if it needs to keep searching. This decision is almost certainly made by a small controller model, not the main LLM.\n",
      "\n",
      "I’m guessing here, but if the material looks thin or off-topic, it may run another round of retrieval. If it looks solid, it can hand those chunks over to the LLM.\n",
      "\n",
      "At some point, that handoff happens. The selected passages, along with some metadata, are passed to the main LLM.\n",
      "\n",
      "The model reads all the provided chunks and picks whichever one best supports the answer it wants to generate.\n",
      "\n",
      "It does not mechanically follow the retriever’s order. So there’s no guarantee the LLM will use the “top” chunk. It may prefer a lower-ranked passage simply because it’s clearer, more self-contained, or closer to the phrasing needed for the answer.\n",
      "\n",
      "So just like us, it decides what to take in and what to ignore. And even if your chunk scores the highest, there’s no assurance it will be the first one mentioned.\n",
      "\n",
      "What to think about\n",
      "\n",
      "This system isn’t really a black box. It’s a system people have built to hand the LLMs the right information to answer a user’s question.\n",
      "\n",
      "If what I’ve inferred here is true, then it finds candidates, splits documents into units, searches and ranks those units, and then hands them over to an LLM to summarize.\n",
      "\n",
      "From this we can also figure out what we need to think about when creating content for it.\n",
      "\n",
      "Traditional SEO still matters a lot, because this system leans on the old one. Things like having a proper sitemap, easily rendered content, proper headings, domain authority, and accurate last-modified tags are all important for your content to be sorted correctly.\n",
      "\n",
      "As I pointed out, they may be mixing search engines with their own technology to decide which URLs get picked, which is worth keeping in mind.\n",
      "\n",
      "But if they use retrieval on top of it, then paragraph level relevance is the new leverage point.\n",
      "\n",
      "Maybe this means answer-in-one-chunk design will rule. (Just don’t do it in a way that feels weird, maybe a TL;DR.) And remember to use the right vocabulary: entities, attributes, relationships, like we talked about in the query optimization section.\n",
      "\n",
      "How to build a “GEO Scoring System” (for fun)\n",
      "\n",
      "To figure out how well your content will do, we’ll have to simulate the hostile environment your content will live in. So let’s try to reverse engineer this pipeline.\n",
      "\n",
      "Note, this is non-trivial, as we don’t know the internal metrics they use nor is this system fully public, so think of this as a rough blueprint.\n",
      "\n",
      "The idea is to create a pipeline that can do query rewrite, discovery, retrieval, re-ranking and an LLM judge, and then see where you end up compared to your competitors for different topics.\n",
      "\n",
      "sketching the pipeline to check where you score compared to competitors\n",
      "\n",
      "You begin with a few topics like “hybrid retrieval for enterprise RAG” or “LLM evaluation with LLM-as-judge,” and then build a system that generates natural queries around them.\n",
      "\n",
      "Then you pass those queries through an LLM rewrite step, because these systems often reformulate the user query before retrieval. Those rewritten queries are what you actually push through the pipeline.\n",
      "\n",
      "The first check is visibility. For each query, look at the top 20–30 results across Brave, Google and Bing. Note whether your page appears and where it sits relative to competitors.\n",
      "\n",
      "At the same time, collect domain-level authority metrics (Moz DA, Ahrefs DR, etc.) so you can fold that in later, since these systems probably still lean heavily on those signals.\n",
      "\n",
      "If your page appears in these first results, you move on to the retrieval part.\n",
      "\n",
      "Fetch your page and the competing pages, clean the HTML, split them into chunks, embed those chunks, and build a small hybrid retrieval setup that combines semantic and keyword matching. Add a re-ranking step.\n",
      "\n",
      "Somewhere here you also inject the authority signal, because higher-authority domains realistically get scored higher (even though we don’t know exactly how much).\n",
      "\n",
      "Once you have the top chunks, you add the final layer: an LLM-as-a-judge. Being in the top five doesn’t guarantee citation, so you simulate the last step by handing the LLM a few of the top-scored chunks (with some metadata) and see which one it cites first.\n",
      "\n",
      "When you run this for your pages and competitors, you see where you win or lose: the search layer, the retrieval layer or the LLM layer.\n",
      "\n",
      "Remember this is still a rough sketch, but it gives you something to start with if you want to build a similar system.\n",
      "\n",
      "This article focused on the mechanics rather than the strategy side of SEO/GEO, which I get won’t be for everyone.\n",
      "\n",
      "The goal was to map the flow from a user query to the final answer and show that the AI search tool isn’t some opaque force.\n",
      "\n",
      "Even if parts of the system aren’t public, we can still infer a reasonable sketch of what’s happening. What’s clear so far is that the AI web search doesn’t replace traditional search engines. It just layers retrieval on top of them.\n",
      "\n",
      "Before finishing this, it’s worth mentioning that the deeper research feature is different from the built-in search tools, which are fairly limited and cheap. Deep research likely leans on more agentic search, which may be “scanning” the pages to a greater extent.\n",
      "\n",
      "This might explain why my own content from my website shows up in deep research even though it’s not optimized for the basic search layer, so it almost never shows up in basic AI search.\n",
      "\n",
      "There’s still more to figure out before saying what actually matters in practice. Here I’ve mostly gone through the technical pipeline but if this is new stuff I hoped it explain it well.\n",
      "\n",
      "Hopefully it was easy to read. If you enjoyed it, feel free to share it or connect with me on LinkedIn, Medium or through my site.\n",
      "\n",
      "❤\n",
      "\n",
      "Title: Overcoming the Hidden Performance Traps of Variable-Shaped Tensors: Efficient Data Sampling in PyTorch\n",
      "Author: ['Chaim Rand', 'Shreya Rao', 'Srijanie Dey', 'Peng Qian', 'Tds Editors', 'Sheila Teo', 'Ibrahim Kovan', 'Matteo Ciprian', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-03 17:00:00+00:00\n",
      "Content: is the part of a series of posts on the topic of analyzing and optimizing PyTorch models. Throughout the series, we have advocated for using the PyTorch Profiler in AI model development and demonstrated the potential impact of performance optimization on the speed and cost of running AI/ML workloads. One common phenomenon we have seen is how seemingly innocent code can hamper runtime performance. In this post, we explore some of the penalties associated with the naive use of variable-shaped tensors — tensors whose shape is dependent on preceding computations and/or inputs. While not applicable to all situations, there are times when the use of variable-shaped tensors can be avoided — although this may come at the expense of additional compute and/or memory. We will demonstrate the tradeoffs of these alternatives on a toy implementation of data sampling in PyTorch.\n",
      "\n",
      "Three Downsides of Variable Shaped Tensors\n",
      "\n",
      "We motivate the discussion by presenting three disadvantages to the use of variable-shaped tensors:\n",
      "\n",
      "Host-Device Sync Events\n",
      "\n",
      "In an ideal scenario, the CPU and GPU are able to run in parallel in an asynchronous manner, with the CPU continuously feeding the GPU with input samples, allocating required GPU memory, and loading GPU compute kernels, and the GPU executing the loaded kernels on the provided inputs using the allocated memory. The presence of dynamic-shaped tensors throws a wrench into this parallelism. In order to allocate the appropriate amount memory, the CPU must wait for the GPU to report the tensor’s shape, and then the GPU must wait for the CPU to allocate the memory and proceed with the kernel loading. The overhead of this sync event can cause a drop in the GPU utilization and slow runtime performance.\n",
      "\n",
      "We saw an example of this in part three of this series when we studied a naive implementation of the common cross-entropy loss that included calls to torch.nonzero and torch.unique. Both APIs return tensors with shapes that are dynamic and dependent on the contents of the input. When these functions are run on the GPU, a host-device synchronization event occurs. In the case of the cross-entropy loss, we discovered the inefficiency through the use of PyTorch Profiler and were able to easily overcome it with an alternative implementation that avoided the use of variable-shaped tensors and demonstrated much better runtime performance.\n",
      "\n",
      "Graph Compilation\n",
      "\n",
      "In a recent post we explored the performance benefits of applying just-in-time (JIT) compilation using the torch.compile operator. One of our observations was that graph compilation provided much better results when the graph was static. The presence of dynamic shapes in the graph limits the extent of the optimization via compilation: In some cases, it fails completely; in others it results in lower performance gains. The same implications also apply to other forms of graph compilation, such as XLA, ONNX, OpenVINO, and TensorRT.\n",
      "\n",
      "Data Batching\n",
      "\n",
      "Another optimization we have encountered in several of our posts (e.g., here) is sample-batching. Batching improves performance in two primary ways:\n",
      "\n",
      "Reducing overhead of kernel loading: Rather than loading the GPU kernels required for the computation pipeline once per input sample, the CPU can load the kernels once per batch. Maximizing parallelization across compute units: GPUs are highly parallel compute engines. The more we are able to parallelize computation, the more we can saturate the GPU and increase its utilization. By batching we can potentially increase the degree of parallelization by a factor of the batch size.\n",
      "\n",
      "Despite their downsides, the use of variable-shaped tensors is often unavoidable. But sometimes we can modify our model implementation to circumvent them. Sometimes these changes will be straightforward (as in the cross-entropy loss example). Other times they may require some creativity in coming up with a different sequence of fixed-shape PyTorch APIs that provide the same numerical result. Often, this effort can deliver meaningful rewards in runtime and costs.\n",
      "\n",
      "In the next sections, we will study the use of variable-shaped tensors in the context of the data sampling operation. We will start with a trivial implementation and analyze its performance. We will then propose a GPU-friendly alternative that avoids the use of variable-shaped tensors.\n",
      "\n",
      "To compare our implementations, we will use an Amazon EC2 g6e.xlarge with an NVIDIA L40S running an AWS Deep Learning AMI (DLAMI) with PyTorch (2.8). The code we will share is intended for demonstration purposes. Please do not rely on it for accuracy or optimality. Please do not interpret our mention of any framework, library, or platform and an endorsement of its use.\n",
      "\n",
      "Sampling in AI Model Workloads\n",
      "\n",
      "In the context of this post, sampling refers to the selection of a subset of items from a large set of candidates for the purposes of computational efficiency, balancing of datatypes, or regularization. Sampling is common in many AI/ML models, such as detection, ranking, and contrastive learning systems.\n",
      "\n",
      "We define a simple variation of the sampling problem: Given a list of N tensors each with a binary label, we are asked to return a subset of K tensors containing both positive and negative examples, in random order. If the input list contains enough samples of each label (K/2), the returned subset should be evenly split. If it is lacking samples of one type, these should be filled with random samples of the second type.\n",
      "\n",
      "The code block below contains a PyTorch implementation of our sampling function. The implementation is inspired by the popular Detectron2 library (e.g., see here and here). For the experiments in this post, we will fix the sampling ratio to 1:10.\n",
      "\n",
      "import torch INPUT_SAMPLES = 10000 SUB_SAMPLE = INPUT_SAMPLES // 10 FEATURE_DIM = 16 def sample_data(input_array, labels): device = labels.device positive = torch.nonzero(labels == 1, as_tuple=True)[0] negative = torch.nonzero(labels == 0, as_tuple=True)[0] num_pos = min(positive.numel(), SUB_SAMPLE//2) num_neg = min(negative.numel(), SUB_SAMPLE//2) if num_neg < SUB_SAMPLE//2: num_pos = SUB_SAMPLE - num_neg elif num_pos < SUB_SAMPLE//2: num_neg = SUB_SAMPLE - num_pos # randomly select positive and negative examples perm1 = torch.randperm(positive.numel(), device=device)[:num_pos] perm2 = torch.randperm(negative.numel(), device=device)[:num_neg] pos_idxs = positive[perm1] neg_idxs = negative[perm2] sampled_idxs = torch.cat([pos_idxs, neg_idxs], dim=0) rand_perm = torch.randperm(SUB_SAMPLE, device=labels.device) sampled_idxs = sampled_idxs[rand_perm] return input_array[sampled_idxs], labels[sampled_idxs]\n",
      "\n",
      "Performance Analysis With PyTorch Profiler\n",
      "\n",
      "Even when not immediately obvious, the use of dynamic shapes is easily identifiable in the PyTorch Profiler Trace view. We use the following function to enable PyTorch Profiler:\n",
      "\n",
      "def profile(fn, input, labels): def export_trace(p): p.export_chrome_trace(f\"{fn.__name__}.json\") with torch.profiler.profile( activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], with_stack=True, schedule=torch.profiler.schedule(wait=0, warmup=10, active=5), on_trace_ready=export_trace ) as prof: for _ in range(20): fn(input, labels) torch.cuda.synchronize() # explicit sync for trace readability prof.step() # create random input input_samples = torch.randn((INPUT_SAMPLES, FEATURE_DIM), device='cuda') labels = torch.randint(0, 2, (INPUT_SAMPLES,), device='cuda', dtype=torch.int64) # run with profiler profile(sample_data, input_samples, labels)\n",
      "\n",
      "The image below was captured for the value of ten million input samples. It clearly shows the presence of sync events coming from the torch.nonzero call, as well as the corresponding drops in GPU utilization:\n",
      "\n",
      "Profiler Trace of Sampler (by Author)\n",
      "\n",
      "The use of torch.nonzero in our implementation is not ideal, but can it be avoided?\n",
      "\n",
      "A GPU-Friendly Data Sampler\n",
      "\n",
      "We propose an alternative implementation of our sampling function that replaces the dynamic torch.nonzero function with a creative combination of the static torch.count_nonzero, torch.topk, and other APIs:\n",
      "\n",
      "def opt_sample_data(input, labels): pos_mask = labels == 1 neg_mask = labels == 0 num_pos_idxs = torch.count_nonzero(pos_mask, dim=-1) num_neg_idxs = torch.count_nonzero(neg_mask, dim=-1) half_samples = labels.new_full((), SUB_SAMPLE // 2) num_pos = torch.minimum(num_pos_idxs, half_samples) num_neg = torch.minimum(num_neg_idxs, half_samples) num_pos = torch.where( num_neg < SUB_SAMPLE // 2, SUB_SAMPLE - num_neg, num_pos ) num_neg = SUB_SAMPLE - num_pos # create random ordering on pos and neg entries rand = torch.rand_like(labels, dtype=torch.float32) pos_rand = torch.where(pos_mask, rand, -1) neg_rand = torch.where(neg_mask, rand, -1) # select top pos entries and invalidate others # since CPU doesn't know num_pos, we assume maximum to avoid sync top_pos_rand, top_pos_idx = torch.topk(pos_rand, k=SUB_SAMPLE) arange = torch.arange(SUB_SAMPLE, device=labels.device) if num_pos.numel() > 1: # unsqueeze to support batched input arange = arange.unsqueeze(0) num_pos = num_pos.unsqueeze(-1) num_neg = num_neg.unsqueeze(-1) top_pos_rand = torch.where(arange >= num_pos, -1, top_pos_rand) # repeat for neg entries top_neg_rand, top_neg_idx = torch.topk(neg_rand, k=SUB_SAMPLE) top_neg_rand = torch.where(arange >= num_neg, -1, top_neg_rand) # combine and mix together positive and negative idxs cat_rand = torch.cat([top_pos_rand, top_neg_rand], dim=-1) cat_idx = torch.cat([top_pos_idx, top_neg_idx], dim=-1) topk_rand_idx = torch.topk(cat_rand, k=SUB_SAMPLE)[1] sampled_idxs = torch.gather(cat_idx, dim=-1, index=topk_rand_idx) sampled_input = torch.gather(input, dim=-2, index=sampled_idxs.unsqueeze(-1)) sampled_labels = torch.gather(labels, dim=-1, index=sampled_idxs) return sampled_input, sampled_labels\n",
      "\n",
      "Clearly, this function requires more memory and more operations than our first implementation. The question is: Do the performance benefits of a static, synchronization-free implementation outweigh the extra cost in memory and compute?\n",
      "\n",
      "To assess the tradeoffs between the two implementations, we introduce the following benchmarking utility:\n",
      "\n",
      "def benchmark(fn, input, labels): # warm-up for _ in range(20): _ = fn(input, labels) iters = 100 start = torch.cuda.Event(enable_timing=True) end = torch.cuda.Event(enable_timing=True) torch.cuda.synchronize() start.record() for _ in range(iters): _ = fn(input, labels) end.record() torch.cuda.synchronize() avg_time = start.elapsed_time(end) / iters print(f\"{fn.__name__} average step time: {(avg_time):.4f} ms\") benchmark(sample_data, input_samples, labels) benchmark(opt_sample_data, input_samples, labels)\n",
      "\n",
      "The following table compares the average runtime of each of the implementations for a variety of input sample sizes:\n",
      "\n",
      "Comparative Step Time Performance — Lower is Better (by Author)\n",
      "\n",
      "For most of the input sample sizes, the overhead of the host-device sync event is either comparable or lower than the additional compute of the static implementation. Disappointingly, we only see a major benefit from the sync-free alternative when the input sample size reaches ten million. Sample sizes that large are uncommon in AI/ML settings. But it’s not our tendency to give up so easily. As noted above, the static implementation enables other optimizations like graph compilation and input batching.\n",
      "\n",
      "Graph Compilation\n",
      "\n",
      "Contrary to the original function — which fails to compile — our static implementation is fully compatible with torch.compile:\n",
      "\n",
      "benchmark(torch.compile(opt_sample_data), input_samples, labels)\n",
      "\n",
      "The following table includes the runtimes of our compiled function:\n",
      "\n",
      "Comparative Step Time Performance — Lower is Better (by Author)\n",
      "\n",
      "The results are significantly better — providing a 70–75 percent boost over the original sampler implementation in the 1–10 thousand range. But we still have one more optimization up our sleeve.\n",
      "\n",
      "Maximizing Performance with Batched Input\n",
      "\n",
      "Because the original implementation contains variable-shaped operations, it cannot handle batched input directly. To process a batch, we have no choice but to apply it to each input individually, in a Python loop:\n",
      "\n",
      "BATCH_SIZE = 32 def batched_sample_data(inputs, labels): sampled_inputs = [] sampled_labels = [] for i in range(inputs.size(0)): inp, lab = sample_data(inputs[i], labels[i]) sampled_inputs.append(inp) sampled_labels.append(lab) return torch.stack(sampled_inputs), torch.stack(sampled_labels)\n",
      "\n",
      "In contrast, our optimized function supports batched inputs as is — no changes necessary.\n",
      "\n",
      "input_batch = torch.randn((BATCH_SIZE, INPUT_SAMPLES, FEATURE_DIM), device='cuda') labels = torch.randint(0, 2, (BATCH_SIZE, INPUT_SAMPLES), device='cuda', dtype=torch.int64) benchmark(batched_sample_data, input_batch, labels) benchmark(opt_sample_data, input_batch, labels) benchmark(torch.compile(opt_sample_data), input_batch, labels)\n",
      "\n",
      "The table below compares the step times of our sampling functions on a batch size of 32:\n",
      "\n",
      "Step Time Performance on Batched Input — Lower is Better (by Author)\n",
      "\n",
      "Now the results are definitive: By using a static implementation of the data sampler, we are able to boost performance by 2X–52X(!!) the variable-shaped option, depending on the input sample size.\n",
      "\n",
      "Note that although our experiments were run on a GPU device, the model compilation and input batching optimizations also apply to a CPU environment. Thus, avoiding variable shapes could have implications on AI/ML model performance on CPU, as well.\n",
      "\n",
      "Summary\n",
      "\n",
      "The optimization process we demonstrated in this post generalizes beyond the specific case of data sampling:\n",
      "\n",
      "Discovery via Performance Profiling: Using the PyTorch Profiler we were able to identify drops in GPU utilization and discover their source: the presence of variable-shaped tensors resulting from the torch.nonzero operation.\n",
      "\n",
      "Using the PyTorch Profiler we were able to identify drops in GPU utilization and discover their source: the presence of variable-shaped tensors resulting from the torch.nonzero operation. An Alternate Implementation: Our profiling findings allowed us to develop an alternative implementation that accomplished the same goal while avoiding the use of variable-shaped tensors. However, this step came at the cost of additional compute and memory overhead. As seen in our initial benchmarks, the sync-free alternative demonstrated worse performance on common input sizes.\n",
      "\n",
      "Our profiling findings allowed us to develop an alternative implementation that accomplished the same goal while avoiding the use of variable-shaped tensors. However, this step came at the cost of additional compute and memory overhead. As seen in our initial benchmarks, the sync-free alternative demonstrated worse performance on common input sizes. Unlocking Further Potential for Optimization: The true breakthrough came because the static-shaped implementation was compilation-friendly and supported batching. These optimizations provided performance gains that dwarfed the initial overhead, leading to a 2x to 52x speedup over the original implementation.\n",
      "\n",
      "Naturally, not all stories will end as happily as ours. In many cases, we may come across PyTorch code that performs poorly on the GPU but does not have an alternative implementation, or it may have one that requires significantly more compute resources. However, given the potential for meaningful gains in performance and reductions in cost, the process of identifying runtime inefficiencies and exploring alternative implementations is an essential part of AI/ML development.\n",
      "\n",
      "Title: The Machine Learning “Advent Calendar” Day 3: GNB, LDA and QDA in Excel\n",
      "Author: ['Angela Shi', 'Shreya Rao', 'Dr. Robert Kübler', 'Piero Paialunga', 'Shaw Talebi', 'Luigi Battistoni', 'Tds Editors', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-03 16:30:00+00:00\n",
      "Content: working with k-NN (k-NN regressor and k-NN classifier), we know that the k-NN approach is very naive. It keeps the entire training dataset in memory, relies on raw distances, and does not learn any structure from the data.\n",
      "\n",
      "We already began to improve the k-NN classifier, and in today’s article, we will implement these different models:\n",
      "\n",
      "GNB: Gaussian Naive Bayes\n",
      "\n",
      "LDA: Linear Discriminant Analysis\n",
      "\n",
      "QDA: Quadratic Discriminant Analysis\n",
      "\n",
      "For all these models, the distribution is considered as Gaussian. So at the end, we will also see an approach to get a more customized distribution.\n",
      "\n",
      "If you read my previous article, here are some questions for you:\n",
      "\n",
      "What is the relationship between LDA and QDA?\n",
      "\n",
      "What is the relation between GBN and QDA?\n",
      "\n",
      "What happens if the data is not Gaussian at all?\n",
      "\n",
      "What is the method to get a customized distribution?\n",
      "\n",
      "What is linear in LDA? What is quadratic in QDA?\n",
      "\n",
      "When reading through the article, you can use this Excel/Google sheet.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "Nearest Centroids: What This Model Really Is\n",
      "\n",
      "Let’s do a quick recap about what we already started yesterday.\n",
      "\n",
      "We introduced a simple idea: when we calculate the average of each continuous feature inside a class, that class collapses into one single representative point.\n",
      "\n",
      "This gives us the Nearest Centroids model.\n",
      "\n",
      "Each class is summarized by its centroid, the average of all its feature values.\n",
      "\n",
      "Now, let us think about this from a Machine Learning point of view.\n",
      "\n",
      "We usually separate the process into two parts: the training step and the hyperparameter tuning step.\n",
      "\n",
      "For Nearest Centroids, we can draw a small “model card” to understand what this model really is:\n",
      "\n",
      "How is the model trained? By computing one average vector per class. Nothing more.\n",
      "\n",
      "Does it handle missing values? Yes. A centroid can be computed using all available (non-empty) values.\n",
      "\n",
      "Does scale matter? Yes, absolutely, because distance to a centroid depends on the units of each feature.\n",
      "\n",
      "What are the hyperparameters? None.\n",
      "\n",
      "We said that the k-NN classifier may not be a real machine learning model because it is not an actual model.\n",
      "\n",
      "For Nearest Centroids, we can say that it is not really a machine learning model because it cannot be tuned. So what about overfitting and underfitting?\n",
      "\n",
      "Well, the model is so simple that it cannot memorize noise in the same way k-NN does.\n",
      "\n",
      "So, Nearest Centroids will only tend to underfit when classes are complex or not well separated, because one single centroid cannot capture their full structure.\n",
      "\n",
      "Understanding Class Shape with One Feature: Adding Variance\n",
      "\n",
      "Now, in this section, we will use only one continuous feature, and 2 classes.\n",
      "\n",
      "Up to now, we used only one statistic per class: the average value.\n",
      "\n",
      "Let us now add a second piece of information: the variance (or equivalently, the standard deviation).\n",
      "\n",
      "This tells us how “spread out” each class is around its average.\n",
      "\n",
      "A natural question appears immediately: Which variance should we use?\n",
      "\n",
      "The most intuitive answer is to compute one variance per class, because each class might have a different spread.\n",
      "\n",
      "But there is another possibility: we could compute one common variance for both classes, usually as a weighted average of the class variances.\n",
      "\n",
      "This feels a bit unnatural at first, but we will see later that this idea leads directly to LDA.\n",
      "\n",
      "So the table below gives us everything we need for this model, in fact, for both versions (LDA and QDA) of the model.\n",
      "\n",
      "the number of observations in each class (to weight the classes)\n",
      "\n",
      "the mean of each class\n",
      "\n",
      "the standard deviation of each class\n",
      "\n",
      "and the common standard deviation across both classes\n",
      "\n",
      "With these values, the entire model is completely defined.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "Now, once we have a standard deviation, we can build a more refined distance: the distance to the centroid divided by the standard deviation.\n",
      "\n",
      "Why do we do this?\n",
      "\n",
      "Because this gives a distance that is scaled by how variable the class is.\n",
      "\n",
      "If a class has a large standard deviation, being far from its centroid is not surprising.\n",
      "\n",
      "If a class has a very small standard deviation, even a small deviation becomes significant.\n",
      "\n",
      "This simple normalization turns our Euclidean distance into something a little bit more meaningful, that represents the shape of each class.\n",
      "\n",
      "This distance was introduced by Mahalanobis, so we call it the Mahalanobis distance.\n",
      "\n",
      "Now we can do all these calculations directly in the Excel file.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "The formulas are straightforward, and with conditional formatting, we can clearly see how the distance to each center changes and how the scaling affects the results.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "Now, let’s do some plots, always in Excel.\n",
      "\n",
      "This diagram below shows the full progression: how we start from the Mahalanobis distance, move to the likelihood under each class distribution, and finally obtain the probability prediction.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "LDA vs. QDA, what do we see?\n",
      "\n",
      "With just one feature, the difference becomes very easy to visualize.\n",
      "\n",
      "For LDA, the separation on the x-axis is always cut into two parts. This is why the method is called Linear Discriminant Analysis.\n",
      "\n",
      "For QDA, even with only one feature, the model produces two cut points on the x-axis. In higher dimensions, this becomes a curved boundary, described by a quadratic function. Hence, the name Quadratic Discriminant Analysis.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "And you can directly modify the parameters to see how they impact the decision boundary.\n",
      "\n",
      "The changes in the means or variances will change the frontier, and Excel makes these effects very easy to visualize.\n",
      "\n",
      "By the way, does the shape of the LDA probability curve remind you of a model that you surely know? Yes, it looks exactly the same.\n",
      "\n",
      "You can already guess which one, right?\n",
      "\n",
      "But now the real question is: are they really the same model? And if not, how do they differ?\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "We can also study the case with three classes. You can try this yourself as an exercise in Excel.\n",
      "\n",
      "Here are the results. For each class, we repeat exactly the same procedure. And for the final probability prediction, we simply sum all the likelihoods and take the proportion of each one.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "Again, this approach is also used in another well-known model.\n",
      "\n",
      "Do you know which one? It is much more familiar to most people, and this shows how closely connected these models really are.\n",
      "\n",
      "When you understand one of them, you automatically understand the others much better.\n",
      "\n",
      "Class Shape in 2D: Variance Only or Covariance as Well?\n",
      "\n",
      "With one feature, we do not talk about dependency, as there is none. So in this case, QDA behaves exactly like Gaussian Naive Bayes. Because we usually allow each class to have its own variance, which is perfectly natural.\n",
      "\n",
      "The difference will appear when we move to two or more features. At that point, we will distinguish cases of how the model treats the covariance between the features.\n",
      "\n",
      "Gaussian Naive Bayes makes one very strong simplifying assumption:\n",
      "\n",
      "the features are independent. This is the reason for the word Naive in its name.\n",
      "\n",
      "LDA and QDA, however, do not make this assumption. They allow interactions between features, and this is what generates linear or quadratic boundaries in higher dimensions.\n",
      "\n",
      "Let’s do the exercice in Excel!\n",
      "\n",
      "Gaussian Naive Bayes: no covariance\n",
      "\n",
      "Let us begin with the simplest case: Gaussian Naive Bayes.\n",
      "\n",
      "So, we do not need to compute any covariance at all, because the model assumes that the features are independent.\n",
      "\n",
      "To illustrate this, we can look at a small example with three classes.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "QDA: each class has its own covariance\n",
      "\n",
      "For QDA, we now have to calculate the covariance matrix for each class.\n",
      "\n",
      "And once we have it, we also need to compute its inverse, because it is used directly in the formula for the distance and the likelihood.\n",
      "\n",
      "So there are a few more parameters to compute compared to Gaussian Naive Bayes.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "LDA: all classes share the same covariance\n",
      "\n",
      "For LDA, all classes share the same covariance matrix, which reduces the number of parameters and forces the decision boundary to be linear.\n",
      "\n",
      "Even though the model is simpler, it remains very effective in many situations, especially when the amount of data is limited.\n",
      "\n",
      "GNB, LDA and QDA in Excel – image by author\n",
      "\n",
      "Customized Class Distributions: Beyond the Gaussian Assumption\n",
      "\n",
      "Up to now, we only talked about Gaussian distributions. And it is for its simplificity. And we also can use other distributions. So even in Excel, it is very easy to change.\n",
      "\n",
      "In reality, data usually do not follow a perfect Gaussian curve.\n",
      "\n",
      "For exploring a dataset, we use the empiric density plots almost every time. They give an immediate visual feeling of how the data is distributed.\n",
      "\n",
      "And the kernel density estimator (KDE) as a non-parametric method, is often used.\n",
      "\n",
      "BUT, in practice, KDE is rarely used as a full classification model. It is not very convenient, and its predictions are often sensitive to the choice of bandwidth.\n",
      "\n",
      "And what is interesting is that this idea of kernels will come back again when we discuss other models.\n",
      "\n",
      "So even though we show it here mainly for exploration, it is an essential building block in machine learning.\n",
      "\n",
      "KDE (Kernel Density Estimator) in Excel – image by author\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Today, we followed a natural path that begins with simple averages and gradually leads to full probabilistic models.\n",
      "\n",
      "Nearest Centroids compresses each class into one point.\n",
      "\n",
      "Gaussian Naive Bayes adds the notion of variance, and assumes the independance of the features.\n",
      "\n",
      "QDA gives each class its own variance or covariance\n",
      "\n",
      "LDA simplifies the shape by sharing the covariance.\n",
      "\n",
      "We even saw that we can step outside the Gaussian world and explore customized distributions.\n",
      "\n",
      "All these models are connected by the same idea: a new observation belongs to the class it most resembles.\n",
      "\n",
      "The difference is how we define resemblance, by distance, by variance, by covariance, or by a full probability distribution.\n",
      "\n",
      "For all these models, we can do the two steps easily in Excel:\n",
      "\n",
      "the first step is to estimate the paramters, which can be considered as the model training\n",
      "\n",
      "the inference step that is to calculate the distance and the probability for each class\n",
      "\n",
      "GNB, LDA and QDA – image by author\n",
      "\n",
      "One more thing\n",
      "\n",
      "Before closing this article, let us draw a small cartography of distance-based supervised models.\n",
      "\n",
      "We have two main families:\n",
      "\n",
      "local distance models\n",
      "\n",
      "global distance models\n",
      "\n",
      "For local distance, we already know the two classical ones:\n",
      "\n",
      "k-NN regressor\n",
      "\n",
      "k-NN classifier\n",
      "\n",
      "Both predict by looking at neighbors and using the local geometry of the data.\n",
      "\n",
      "For global distance, all the models we studied today belong to the classification world.\n",
      "\n",
      "Why?\n",
      "\n",
      "Because global distance requires centers defined by classes.\n",
      "\n",
      "We measure how close a new observation is to each class prototype?\n",
      "\n",
      "But what about regression?\n",
      "\n",
      "It seems that this notion of global distance does not exist for regression, or does it really?\n",
      "\n",
      "The answer is yes, it does exist…\n",
      "\n",
      "Title: How to Turn Your LLM Prototype into a Production-Ready System\n",
      "Author: ['Piero Paialunga', 'Tomaz Bratanic', 'Jacopo Tagliabue', 'Laurent Picard', 'Deepak Krishnamurthy', 'Erika G. Gonçalves', 'Parul Pandey', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-03 15:30:00+00:00\n",
      "Content: applications of LLMs are the ones that I like to call the “wow effect LLMs.” There are plenty of viral LinkedIn posts about them, and they all sound like this:\n",
      "\n",
      "“I built [x] that does [y] in [z] minutes using AI.”\n",
      "\n",
      "Where:\n",
      "\n",
      "[x] is usually something like a web app/platform\n",
      "\n",
      "[y] is a somewhat impressive main feature of [x]\n",
      "\n",
      "[z] is usually an integer number between 5 and 10.\n",
      "\n",
      "“AI” is really, most of the time, a LLM wrapper (Cursor, Codex, or similar)\n",
      "\n",
      "If you notice carefully, the focus of the sentence is not really the quality of the analysis but the amount of time you save. This is to say that, when dealing with a task, people are not excited about the LLM output quality in tackling the problem, but they are thrilled that the LLM is spitting out something quick that might sound like a solution to their problem.\n",
      "\n",
      "This is why I refer to them as wow-effect LLMs. As impressive as they sound and look, these wow-effect LLMs display multiple issues that prevent them from being actually implemented in a production environment. Some of them:\n",
      "\n",
      "The prompt is usually not optimized: you don’t have time to test all the different versions of the prompts, evaluate them, and provide examples in 5-10 minutes. They are not meant to be sustainable: in that short of time, you can develop a nice-looking plug-and-play wrapper. By default, you are throwing all the costs, latency, maintainability, and privacy considerations out of the window. They usually lack context: LLMs are powerful when they are plugged into a big infrastructure, they have decisional power over the tools that they use, and they have contextual data to augment their answers. No chance of implementing that in 10 minutes.\n",
      "\n",
      "Now, don’t get me wrong: LLMs are designed to be intuitive and easy to use. This means that evolving LLMs from the wow effect to production-level is not rocket science. However, it requires a specific methodology that needs to be implemented.\n",
      "\n",
      "The goal of this blog post is to provide this methodology.\n",
      "\n",
      "The points we will cover to move from wow-effect LLMs to production-level LLMs are the following:\n",
      "\n",
      "LLM System Requirements . When this beast goes into production, we need to know how to maintain it. This is done in stage zero, through adequate system requirements analysis.\n",
      "\n",
      ". When this beast goes into production, we need to know how to maintain it. This is done in stage zero, through adequate system requirements analysis. Prompt Engineering . We are going to optimize the prompt structure and provide some best-practice prompt strategies.\n",
      "\n",
      ". We are going to optimize the prompt structure and provide some best-practice prompt strategies. Force structure with schemas and structured output . We are going to move from free text to structured objects, so the format of your response is fixed and reliable.\n",
      "\n",
      ". We are going to move from free text to structured objects, so the format of your response is fixed and reliable. Use tools so the LLM does not work in isolation . We are going to let the model connect to data and call functions. This provides richer answers and reduces hallucinations.\n",
      "\n",
      ". We are going to let the model connect to data and call functions. This provides richer answers and reduces hallucinations. Add guardrails and validation around the model . Check inputs and outputs, enforce business rules, and define what happens when the model fails or goes out of bounds.\n",
      "\n",
      ". Check inputs and outputs, enforce business rules, and define what happens when the model fails or goes out of bounds. Combine everything into a simple, testable pipeline. Orchestrate prompts, tools, structured outputs, and guardrails into a single flow that you can log, monitor, and improve over time.\n",
      "\n",
      "We are going to use a very simple case: we are going to make LLM judge data scientists’ tests. This is just a concrete case to avoid a totally abstract and confusing article. The procedure is general enough to be adapted to other LLM applications, typically with very minor adjustments.\n",
      "\n",
      "Looks like we’ve got a lot of ground to cover. Let’s get started!\n",
      "\n",
      "Image generated by author using Excalidraw Whiteboard\n",
      "\n",
      "The whole code and data can be found here.\n",
      "\n",
      "Tough choices: cost, latency, privacy\n",
      "\n",
      "Before writing any code, there are a few important questions to ask:\n",
      "\n",
      "How complex is your task?\n",
      "\n",
      "Do you really need the latest and most expensive model, or can you use a smaller one or an older family?\n",
      "\n",
      "Do you really need the latest and most expensive model, or can you use a smaller one or an older family? How often do you run this, and at what latency?\n",
      "\n",
      "Is this a web app that must respond on demand, or a batch job that runs once and stores results? Do users expect an immediate answer, or is “we’ll email you later” acceptable?\n",
      "\n",
      "Is this a web app that must respond on demand, or a batch job that runs once and stores results? Do users expect an immediate answer, or is “we’ll email you later” acceptable? What is your budget?\n",
      "\n",
      "You should have a rough idea of what is “ok to spend”. Is it 1k, 10k, 100k? And compared to that, would it make sense to train and host your own model, or is that clearly overkill?\n",
      "\n",
      "You should have a rough idea of what is “ok to spend”. Is it 1k, 10k, 100k? And compared to that, would it make sense to train and host your own model, or is that clearly overkill? What are your privacy constraints?\n",
      "\n",
      "Is it ok to send this data through an external API? Is the LLM seeing sensitive data? Has this been approved by whoever owns legal and compliance?\n",
      "\n",
      "Let me throw some examples at you. If we consider OpenAI, this is the table to look at for prices:\n",
      "\n",
      "Image from https://platform.openai.com/docs/pricing\n",
      "\n",
      "For simple tasks, where you have a low budget and need low latency, the smaller models (for example the 4.x mini family or 5 nano) are usually your best bet. They are optimized for speed and price, and for many basic use cases like classification, tagging, light transformations, or simple assistants, you will barely notice the quality difference while paying a fraction of the cost.\n",
      "\n",
      "For more complex tasks, such as complex code generation, long-context analysis, or high-stakes evaluations, it can be worth using a stronger model in the 5.x family, even at a higher per-token cost. In those cases, you are explicitly trading money and latency for better decision quality.\n",
      "\n",
      "If you are running large offline workloads, for example re-scoring or re-evaluating thousands of items overnight, batch endpoints can significantly reduce costs compared to real-time calls. This often changes which model fits your budget, because you can afford a “bigger” model when latency is not a constraint.\n",
      "\n",
      "From a privacy standpoint, it is also good practice to only send non-sensitive or “sensitive-cleared” data to your provider, meaning data that has been cleaned to remove anything confidential or personal. If you need even more control, you can consider running local LLMs.\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "The specific use case\n",
      "\n",
      "For this article, we’re building an automated grading system for Data Science exams. Students take a test that requires them to analyze actual datasets and answer questions based on their findings. The LLM’s job is to grade these submissions by:\n",
      "\n",
      "Understanding what each question asks Accessing the correct answers and grading criteria Verifying student calculations against the actual data Providing detailed feedback on what went wrong\n",
      "\n",
      "This is a perfect example of why LLMs need tools and context. You see, you could indeed do a plug-and-play approach. If we were to do a simple DS through a single prompt and API call, it would have the wow-effect, but it would not work well in production. Without access to the datasets and grading rubrics, the LLM cannot grade accurately. It needs to retrieve the actual data to verify whether a student’s answer is correct.\n",
      "\n",
      "Our exam is stored in test.json and contains 10 questions across three sections. Students must analyze three different datasets: e-commerce sales, customer demographics, and A/B test results. Let’s look at a few example questions:\n",
      "\n",
      "As you can see, the questions are data-related, so the LLM will need a tool to analyze these questions. We will go back to this.\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "Building the prompt\n",
      "\n",
      "When I use ChatGPT for small daily questions, I am terribly lazy, and I don’t pay attention to the prompt quality at all, and that is ok. Imagine that you need to know the current situation of the housing market in your city, and you have to sit down at your laptop and write thousands of lines of Python code. Not very appealing, right?\n",
      "\n",
      "However, to truly get the best prompt for your production-level LLM application, there are some key components to follow:\n",
      "\n",
      "Clear Role Definition . WHO the LLM is and WHAT expertise it has.\n",
      "\n",
      ". WHO the LLM is and WHAT expertise it has. System vs User Messages . The system is the LLM-specific instructions. The “user” represents the specific prompt to run, with the current request from the user.\n",
      "\n",
      ". The system is the LLM-specific instructions. The “user” represents the specific prompt to run, with the current request from the user. Explicit Rules with Chain-of-Thought . This is the list of steps that the LLM has to follow to perform the task. This step-by-step reasoning triggers the Chain-of-Thought, which improves performance and reduces hallucinations.\n",
      "\n",
      ". This is the list of steps that the LLM has to follow to perform the task. This step-by-step reasoning triggers the Chain-of-Thought, which improves performance and reduces hallucinations. Few-Shot Examples. This is a list of examples, so that we show explicitly how the LLM should perform the task. Show the LLM correct grading examples.\n",
      "\n",
      "It is usually a good idea to have a prompt.py file, with SYSTEM_PROMPT, USER_PROMPT_TEMPLATE, and FEW_SHOT_EXAMPLES. This is the example for our use-case:\n",
      "\n",
      "So the prompts that we will reuse are stored as constants, while the ones that change based on the student answer are obtained from get_grading_prompt .\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "Output Formatting\n",
      "\n",
      "If you notice, the output in the few-shot example already has a sort of “structure”. At the end of the day, the score should be “packaged” in a production-adequate format. It is not acceptable to have the output as a free-text/string.\n",
      "\n",
      "In order to do that, we are going to use the magic Pydantic. Pydantic allows us to easily create a schema that can then be passed to the LLM, which will build the output based on the schema.\n",
      "\n",
      "This is our schemas.py file:\n",
      "\n",
      "If you focus on GradingResult, you can see that you have these kinds of features:\n",
      "\n",
      "question_number: int = Field(..., ge=1, le=10, description=\"Question number (1-10)\") points_earned: float = Field(..., ge=0, le=10, description=\"Points earned out of 10\") points_possible: int = Field(default=10, description=\"Maximum points for this question\")\n",
      "\n",
      "Now, imagine that we want to add a new feature (e.g. completeness_of_the_answer), this would be very easy to do: you just add it to the schema. However, keep in mind that the prompt should reflect the way your output will look.\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "Tools Description\n",
      "\n",
      "The /data folder has:\n",
      "\n",
      "A list of datasets, which will be the topic of our questions (e.g. Calculate the average order value (AOV) for customers who used the discount code \\”SAVE20\\”. What percentage of total orders used this discount code). This folder has a set of tables, which represent the data that should be analyzed by the students when taking the tests. The grading rubric dataset, which will describe how we are going to evaluate each question. The ground truth dataset, which will describe the ground truth answer for every question\n",
      "\n",
      "We are going to give the LLM free roam on these datasets; we are letting it explore each file based on the specific question.\n",
      "\n",
      "For example, get_ground_truth_answer() allows the LLM to pull the ground truth for a given question. query_dataset() allows you to do some operations on the LLM, like computing the mean, max, and count.\n",
      "\n",
      "Even in this case, it is worth noticing that tools, schema, and prompt are completely customizable. If your LLM has access to 10 tools, and you need to add one more functionality, there is no need to do any structural change to the code: the only thing to do is to add the functionality in terms of prompt, schema, and tool.\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "Guardrails Description\n",
      "\n",
      "In Software Engineering, you recognize a good system from how gracefully it fails. This shows the amount of work that has been put into the task.\n",
      "\n",
      "In this case, some “graceful falls” are the following:\n",
      "\n",
      "The input should be sanitized: the question ID should exist, the student’s answer text should exist, and not be too long The output should be sanitized: the question ID should exist, the score should be between 1 to 10, and the output should be in the correct format identified by Pydantic. The output should “make sense”: you can not give the best score if there are errors, or give 0 if there are no errors. A rate limit should be implemented: in production, you don’t want to accidentally run thousands of threads at once for no reason. It is best to implement a RateLimit check.\n",
      "\n",
      "This part is slightly boring, but very necessary. As it is necessary, it is included in my Github Folder, as it is boring, I won’t copy-paste it here. You’re welcome! 🙂\n",
      "\n",
      "Image made by author using Excalidraw Whiteboard\n",
      "\n",
      "Whole pipeline\n",
      "\n",
      "The whole pipeline is implemented through CrewAI, which is built on top of LangChain. The logic is simple:\n",
      "\n",
      "The crew is the main object that is used to generate the output for a given input with a single command ( crew.kickoff() ).\n",
      "\n",
      "is the main object that is used to generate the output for a given input with a single command ( ). An agent is defined: this wraps the tools, the prompts, and the specific LLM (e.g, GPT 4 with a given temperature). This is connected to the crew.\n",
      "\n",
      "is defined: this wraps the tools, the prompts, and the specific LLM (e.g, GPT 4 with a given temperature). This is connected to the crew. The task is defined: this is the specific task that we want the LLM to perform. This is also connected to the crew.\n",
      "\n",
      "Now, the magic is that the task is connected to the tools, the prompts, and the Pydantic schema. This means that all the dirty work is done in the backend. The pseudo-code looks like this:\n",
      "\n",
      "agent = Agent( role=\"Expert Data Science Grader\", goal=\"Grade student data science exam submissions accurately and fairly by verifying answers against actual datasets\", backstory=SYSTEM_PROMPT, tools=tools_list, llm=llm, verbose=True, allow_delegation=False, max_iter=15 ) task = Task( description=description, expected_output=expected_output, agent=agent, output_json=GradingResult # Enforce structured output ) crew = Crew( agents=[self.grader_agent], tasks=[task], process=Process.sequential, verbose=self.verbose ) result = crew.kickoff()\n",
      "\n",
      "Now, let’s say we have the following JSON output, with the student work:\n",
      "\n",
      "We can use the following main.py file to process this:\n",
      "\n",
      "And run it through:\n",
      "\n",
      "python main.py --submission ../data/test.json \\ --limit 1 \\ --output ../results/test_llm_output.json\n",
      "\n",
      "This kind of setup is exactly how production-level code works: the output is passed through an API as a structured piece of information, and the code needs to run on that piece of data.\n",
      "\n",
      "This is how the terminal will display to you:\n",
      "\n",
      "Image made by author\n",
      "\n",
      "As you can see from the screenshot above, the input is processed through the LLM, but before the output is produced, the CoT is triggered, the tools are called, and the tables are read.\n",
      "\n",
      "And this is what the output looks like (test_llm_output.json):\n",
      "\n",
      "This is a good example of how LLMs can be exploited in their full power. At the end of the day, the main advantage of LLMs is their ability to read the context efficiently. The more context we provide (tools, rule-based prompting, few-shot prompting, output formatting), the less the LLM will have to “fill the gaps” (usually hallucinating) and the better job it will eventually do.\n",
      "\n",
      "Image generated by author using Excalidraw Whiteboard\n",
      "\n",
      "Conclusions\n",
      "\n",
      "Thank you for sticking with me throughout this long, but hopefully not too painful, blog post. 🙂\n",
      "\n",
      "We cover a lot of fun stuff. More specifically, we started from the wow-effect LLMs, the ones that look great in a LinkedIn post but fall apart as soon as you ask them to run every day, within a budget, and under real constraints.\n",
      "\n",
      "Instead of stopping at the demo, we walked through what it actually takes to turn an LLM into a system:\n",
      "\n",
      "We defined the system requirements first, thinking in terms of cost, latency, and privacy, instead of just picking the biggest model available.\n",
      "\n",
      "first, thinking in terms of cost, latency, and privacy, instead of just picking the biggest model available. We framed a concrete use case : an automated grader for Data Science exams that has to read questions, look at real datasets, and give structured feedback to students.\n",
      "\n",
      ": an automated grader for Data Science exams that has to read questions, look at real datasets, and give structured feedback to students. We designed the prompt as a specification, with a clear role, explicit rules, and few-shot examples to guide the model toward consistent behavior.\n",
      "\n",
      "as a specification, with a clear role, explicit rules, and few-shot examples to guide the model toward consistent behavior. We enforced structured output using Pydantic, so the LLM returns typed objects instead of free text that needs to be parsed and fixed every time.\n",
      "\n",
      "using Pydantic, so the LLM returns typed objects instead of free text that needs to be parsed and fixed every time. We plugged in tools to give the model access to the datasets, grading rubrics, and ground truth answers, so it can check the student work instead of hallucinating results.\n",
      "\n",
      "to give the model access to the datasets, grading rubrics, and ground truth answers, so it can check the student work instead of hallucinating results. We added guardrails and validation around the model, making sure inputs and outputs are sane, scores make sense, and the system fails gracefully when something goes wrong.\n",
      "\n",
      "around the model, making sure inputs and outputs are sane, scores make sense, and the system fails gracefully when something goes wrong. Finally, we put everything together into a simple pipeline, where prompts, tools, schemas, and guardrails work as one unit that you can reuse, test, and monitor.\n",
      "\n",
      "The main idea is simple. LLMs are not magical oracles. They are powerful components that need context, structure, and constraints. The more you control the prompt, the output format, the tools, and the failure modes, the less the model has to fill the gaps on its own, and the fewer hallucinations you get.\n",
      "\n",
      "Before you head out\n",
      "\n",
      "Thank you again for your time. It means a lot ❤️\n",
      "\n",
      "My name is Piero Paialunga, and I’m this guy here:\n",
      "\n",
      "Image made by author\n",
      "\n",
      "I’m originally from Italy, hold a Ph.D. from the University of Cincinnati, and work as a Data Scientist at The Trade Desk in New York City. I write about AI, Machine Learning, and the evolving role of data scientists both here on TDS and on LinkedIn. If you liked the article and want to know more about machine learning and follow my studies, you can:\n",
      "\n",
      "A. Follow me on Linkedin, where I publish all my stories\n",
      "\n",
      "B. Follow me on GitHub, where you can see all my code\n",
      "\n",
      "C. For questions, you can send me an email\n",
      "\n",
      "Title: Multi-Agent Arena: Insights from London Great Agent Hack 2025\n",
      "Author: ['Erika G. Gonçalves', 'Rahul Vir', 'Gustavo Santos', 'Benjamin Lee', 'Frank Wittkampf', 'Tomaz Bratanic', 'Eivind Kjosbakken', '.Wp-Block-Post-Author-Name Box-Sizing Border-Box']\n",
      "Publish Date: 2025-12-03 14:00:00+00:00\n",
      "Content: People are going to use more and more AI. Acceleration is going to be the path forward for computing. These fundamental trends, I completely believe in them. Jensen Huang. Nvidia CEO\n",
      "\n",
      "I had the amazing opportunity to participate in the Great Agent Hack 2025, hosted by Holistic AI at UCL[2, 3]. The hackathon was structured around three big challenges: Agent Iron Man, Agent Glass Box, and Dear Grandma, each representing a different philosophy of agentic AI. These weren’t just creative names for convenient categories; they reflected three pillars of how we think about agents today: robustness, transparency, and user safety (of anyone, including your grandma 😄). Being immersed in that environment for a weekend was a kind of reset button for me: it was energising, it reminded me why I enjoy working in this field, and it left me genuinely inspired to keep learning and building, even if there’s never enough time to explore everything that’s happening around AI.\n",
      "\n",
      "In this hackathon, more than 50 projects were developed across three tracks. The focus of this article will be on key moments from the event and a handful of projects that stood out to me personally, while recognizing that every team contributed something valuable to the broader conversation on building robust and trustworthy agents. For readers who want to explore the full range of ideas, the complete gallery of 51 submissions is available here: https://hai-great-agent-hack-2025.devpost.com/project-gallery?page=1 [4]​.\n",
      "\n",
      "Figure 1. Official leaflet and my T-shirt from The Great Agent Hack 2025. Image by the author.\n",
      "\n",
      "Hosted by the UCL Centre for Digital Innovation (CDI), we spent the weekend in some truly unique spaces in East London, the kind of place where you walk past the Orbit Tower (the red sculpture from the 2012 Olympics) and then code under a rotating floating Earth inside the building (Figure 2). London was already covered in Christmas lights everywhere you walked, so moving between the hackathon and the city felt like stepping between a research lab and a holiday postcard.\n",
      "\n",
      "Figure 2. East London views: UCL East campus and the ArcelorMittal Orbit (also called Orbit Tower) (left), and the floating Earth installation inside the UCL Centre for Digital Innovation (right). Photos by the author.\n",
      "\n",
      "In total, the hackathon brought together more than 200 participants and roughly 25 different awards across all kinds of categories. Teams weren’t dropped in cold: before the weekend they had access to tutorials, example notebooks, and other resources that helped them prepare [5], choose a track, and hit the ground running once the clock started. As deliverables, each team was expected to submit a public GitHub repository, record a short demo, and create a poster or slide deck to present their solution to the jury, which made it much easier to understand the full workflow and real-world potential of every project.\n",
      "\n",
      "The jury came from a surprisingly diverse mix of organisations: Holistic AI (the organiser), the UCL Centre for Digital Innovation (CDI), AWS, Valyu, NVIDIA, Entrepreneurs First, and others, including companies interested in the talent and ideas on display. They selected the winners for each of the three main tracks, but also handed out a whole constellation of mystery and special awards that celebrated much more than just the most technically advanced solution.\n",
      "\n",
      "Among these special awards there was a Brave Soldier-style prize for the team that showed true resilience and kept going even when their teammates started disappearing, literally leaving one soldier standing; a Best Pitch award, because selling your idea is also part of getting the job done (especially since technical professionals tend to struggle a bit with this); and a Highest Resource Usage prize for the teams that really leaned into AWS and squeezed every last spark out of the cloud. These and other award categories are summarised on the hackathon website [2].\n",
      "\n",
      "One of the most curious things about the weekend was the chance to see NVIDIA’s ultra‑compact AI supercomputer up close and even take a photo with the iconic leather‑jacket setup to recreate the famous Elon Musk × Jensen Huang “leather jacket moment” [6] shown on the big screen (Figure 3). To make it even better, some of the agents we were trying to break in the Dear Grandma challenge were actually running on similar NVIDIA GPU hardware, so this tiny supercomputer was literally the brain behind the agents that competitors were attacking.\n",
      "\n",
      "Figure 3. The full NVIDIA experience: the leather-jacket photo setup with the DGX Spark (left) and a close-up of the ultra-compact DGX Spark (right). Images by the author.\n",
      "\n",
      "The Agentic Arena\n",
      "\n",
      "As mentioned at the beginning of this article, the heart of the weekend was structured around three tracks (Figure 4). Each one explored a different question about modern AI agents: how to build them so they work, how to make them transparent, and how to make sure they don’t go rogue.\n",
      "\n",
      "Teams could pick whichever track best fit their use case, but in practice many projects naturally crossed track boundaries; a sign of how eager people were to learn, connect, and bring together different aspects of the agent lifecycle (yes, the idea that the more tracks you join the greater your chances of winning was floating around too, but we’ll skip that for now 😉).\n",
      "\n",
      "Figure 4. The three tracks of the Great Agent Hack 2025: Agent Iron Man (build agents that don’t break), Agent Glass Box (understand agent behaviour), and Dear Grandma (attack like a red team, defend like a guardian). Image by Author.\n",
      "\n",
      "Track A. Agent Iron Man: Agents that work, and last\n",
      "\n",
      "This was the engineering reality check track. The goal was to build a high-performing, production-ready multi-agent architecture with clear agent roles, tools, and memory wired together in a way that could actually survive outside a hackathon.\n",
      "\n",
      "Evaluation focused on things that usually only hurt you in production: performance (speed, latency, cost), robustness (how the agent handles tool failures, bad inputs, and edge cases), architecture quality (clean separation between agents, safe tool orchestration, sensible fallbacks), and monitoring (observability, structured outputs, basic health checks). Teams were also expected to account for carbon footprint by favouring smaller or cheaper models where possible and measuring energy and token usage, so the agent remains a conservative, responsible use of compute.\n",
      "\n",
      "This track is also a small taste of what is coming as agents become more widely used and systems grow more complex, with many services talking to each other while still needing to meet tight latency and cost targets.\n",
      "\n",
      "Between the projects, one that caught my eye was FairQuote [4]: an intelligent car‑insurance underwriting system that uses an orchestrator agent plus specialised intake, pricing, and policy agents that coordinate to collect data, assess risk, calculate premiums, and generate explainable policies in a single conversation; architecturally, it points toward the next wave of multi‑agent enterprise workflows, where robustness, clear responsibilities, and strong observability matter just as much as the underlying models.\n",
      "\n",
      "Underwriting is a good example because it’s one of the hardest and most business-critical problems in insurance. It sits at the intersection of regulation, actuarial science, and customer experience: every decision about accepting a risk, pricing it, or applying exclusions passes through this process. When underwriting is slow or opaque, customers get frustrated, partners lose trust, and insurers risk mispriced portfolios and regulatory scrutiny. When it works well, it quietly keeps the system stable, allocating capital efficiently, protecting the balance sheet, and supporting fair pricing across segments.\n",
      "\n",
      "So, in this track, it was great to see not only solid engineering, but also the real problems teams tackled: underwriting, end-to-end claims handling, fraud investigation, and even emergency-services dispatch, where multi-agent systems coordinated triage and decision support in real time. Even if the weekend outputs were still demos, they pointed toward the multi-agent patterns, safeguards, and monitoring that will matter as similar architectures move from hackathon tables into live enterprise environments.\n",
      "\n",
      "Team tool choices lined up closely with the hackathon’s recommended stack: AWS AgentCore with the Strands Agents SDK for orchestration, Amazon Nova and other Bedrock-hosted models (smaller SLMs to stay frugal), and evaluation frameworks like AgentHarm [7]. The latter lets you test whether an LLM agent can correctly sequence synthetic tools such as dark-web search, web scrapers, email senders, payment or bank-transfer functions, and code or shell tools; so you can measure both its robustness to jailbreaks and how capable it remains at executing multi-step harmful workflows once safety barriers are bypassed.\n",
      "\n",
      "Track B. Agent Glass Box: Agents you can see, and trust\n",
      "\n",
      "The transparency track focused on making agentic systems explainable, auditable, and interpretable for humans and organisations. Teams were asked to build agents whose reasoning, memory updates, and actions could be traced and inspected in real time, instead of remaining opaque black boxes. In practice, the projects fell into several families: observability pipelines, explainability tools, governance and safety layers and expert‑discovery or traceability tools.\n",
      "\n",
      "For me, one of the projects that best captured the idea of a “glass box” was GenAI Explainer. We all know text-to-image diffusion models can be powerful but risky: traditional diffusion systems have already been shown to reproduce societal biases [8], and even newer models like FLUX.1 can still reflect patterns in their training data [9] while offering almost no insight into why a particular image appears the way it does. At the hackathon, the GenAI Explainer team tackled this by wrapping FLUX.1 with an explainability layer that lets you see how each word or segment of a prompt influences the generated image, audit outputs for brand, legal, or safety compliance, and iteratively refine prompts while watching the impact live, with every generation step tracked. In practice, they turned diffusion from a black box into something much closer to a glass-box, auditable workflow.\n",
      "\n",
      "In the end, Track B was a reminder that algorithmic transparency is no longer optional: legal and risk teams increasingly need to show that automated decisions are explainable and not biased, and the kind of ‘glass‑box’ thinking behind projects like GenAI Explainer is something we should carry into every agentic application we build.\n",
      "\n",
      "In this track, team tool choices combined tracing platforms such as LangSmith or LangFuse, AWS observability services like CloudWatch, X‑Ray, or Bedrock monitoring, and research tools like AgentGraph [10] (converting traces into interactive knowledge graphs), AgentSeer [11] (building action graphs and doing failure/vulnerability analysis), and the Who_and_When failure‑attribution [12] dataset to analyse and visualise agent traces in depth, to mention just a few.\n",
      "\n",
      "Track C. Dear Grandma: Agents that stay safe, and behave\n",
      "\n",
      "In this track, teams were given seven secret LLM agents 🐺🦊🦅🐻🐜🐘🦎, each represented by an animal, and the mission was to break them, understand them, and identify them. These seven hidden “stealth agents” symbolised different behaviours, strengths, and attack surfaces that teams needed to uncover. The challenge was to build a red‑teaming framework that could attack any of the seven live animal‑agent endpoints using the API provided by the event organisers, backed by NVIDIA powered infrastructure.\n",
      "\n",
      "In the hackathon, each “animal” agent was a live AI system exposed through a single API service, with different routes for each animal. Teams could send prompts to these animal‑specific routes and observe how the agents behaved in real time, each with its own personality and capabilities, which helped red‑teamers design targeted tests and attacks.\n",
      "\n",
      "Figure 5. Example of a jailbreak test against some of the “animal” agents: in front of a DAN‑style prompt, each model responds with a playful refusal and a consistent safety message, revealing both their shared guardrails and their distinct personalities.\n",
      "\n",
      "Track C wasn’t limited to the seven “animal” agents behind the API; attacking commercial systems like ChatGPT, Claude, or Gemini was also allowed as long as teams treated it as part of a systematic security assessment.\n",
      "\n",
      "In this way, the solution should analyse, attack, and explain AI agent vulnerabilities, perform behavioural forensics, and understand why the attack works.\n",
      "\n",
      "The jailbreaking lab team use a two‑step process where they first built an attack library of proven jailbreak prompts, based on techniques reported in the literature such as Base64 obfuscation, CSS/HTML injection, and other prompt‑level tricks. Second, they applied a genetic algorithm to mutate and improve these prompts: whenever an attack from step one partially succeeded, the algorithm would tweak it (changing wording, adding context, combining two prompts, or further obfuscating instructions) so that successful variants were kept and weak ones were discarded. Over time, this evolutionary search produced stronger and stronger adversarial prompts and even uncovered entirely new ways to break the agents.\n",
      "\n",
      "HSIA was another standout project that pushed these ideas into the robotics world. Instead of attacking the animal agents, they targeted a Visual–Language–Action (VLA) robotic system and showed how its perception could be corrupted at the semantic level. The pixels in the image stayed exactly the same; what changed was the internal caption generated by the model. With subtle, carefully crafted perturbations, the VLA system could flip from “I see a bottle in the image” to “I see a knife in the image,” even though no knife was present, leading the robot to act on a false belief about its environment. Their work highlights that multimodal systems can be compromised without touching the raw image, exposing a critical vulnerability for next-generation robotic AI.\n",
      "\n",
      "Lessons Learned\n",
      "\n",
      "If I had to summarise what this hackathon taught me, it would be:\n",
      "\n",
      "Be a Brave Soldier. Perseverance matters more than competition. It’s not about beating others; it’s about staying resilient, adapting when things break (because they will), and delivering the best version of your idea. Events like this aren’t just technical challenges; they’re opportunities to showcase your talent and the kind of determination companies genuinely value.\n",
      "\n",
      "Prepare ahead of time. The teams that did well weren’t necessarily the most senior, they were the ones who arrived already knowing the format, the expectations, the evaluation criteria, and had gone through the tutorials and resources shared in advance.\n",
      "\n",
      "Master the 5-minute pitch. This is critical. Evaluators and judges move fast. You might spend several days building something, but you only get a few minutes to make them care. So, have a pitch ready that explains the value of your project clearly, quickly, and in a way that sparks curiosity. If those 5 minutes are great, the judges will ask for more. This applies equally to junior profiles and senior engineers (storytelling is part of the job). I struggle with this too; in real life we usually don’t have much time to prove our ideas.\n",
      "\n",
      "These Events Are Becoming More Meaningful Than Ever. These events are gaining more interest every year, and the organisers even doubled the number of spots this year, which shows how valuable the experience is. That’s why it’s so important to participate only if you truly want to be there and can commit your time and energy.\n",
      "\n",
      "Study the sponsors. Before the event, look up the companies involved and think about which ones might be most interested in your approach. Tailor your pitch accordingly. Sponsors are not just judges they’re potential collaborators, mentors, or even future teammates.\n",
      "\n",
      "Strong Fundamentals Beat Shiny Models. One key takeaway from the hackathon is that winning wasn’t about using the newest or most hyped models. The top teams didn’t succeed because they relied on the largest or flashiest architectures, they excelled because they built strong solutions on top of solid, well-understood techniques: genetic algorithms, robust diffusion models, between other. The real differentiator was how creatively they combined these foundations with agentic methodologies, clever evaluation setups, and smart engineering to tackle persistent challenges.\n",
      "\n",
      "Collaborative Innovation Accelerates Progress. The event highlighted how cross-disciplinary collaboration between academia, industry, and AI governance experts can significantly strengthen both AI development and governance frameworks. Even participants who weren’t in technical roles contributed valuable ideas grounded in real problems from their own domains, bringing perspectives that pure engineering alone can’t provide. It’s also a great opportunity to connect with people outside your usual technical bubble, expanding not just your network, but the way you think about the impact and applications of AI.\n",
      "\n",
      "Finally, a bigger reflection: agents are evolving fast, and with that comes new architectural challenges, safety concerns, and responsibilities. These are not hypothetical problems of the future, they are happening right now. Being responsible with AI applications is not a hype-driven slogan; it’s part of the daily job of any AI or data science professional.\n",
      "\n",
      "Conclusions\n",
      "\n",
      "These events are quietly shaping how we think about AI governance. When you put powerful agentic systems under time pressure and in messy, realistic scenarios, you’re forced to confront unpredictable behaviour head-on. That’s where the real learning happens: how do we balance rapid innovation with trust and safety? How do we design evaluation frameworks and guardrails that let us move fast without losing control? This hackathon didn’t just reward clever models, it rewarded thoughtful governance.\n",
      "\n",
      "And while there are plenty of AI events popping up everywhere, this is one of the few you should really keep an eye on, the kind that genuinely helps you grow, exposes you to real-world challenges, and reminds you why it’s worth staying curious and keeping your skills sharp.\n",
      "\n",
      "References\n",
      "\n",
      "References in order of appearance:\n",
      "\n",
      "[1] “NVIDIA CEO Jensen Huang kicks off CES 2025. The Future is Here!” SupplyChainToday, 2025. Link.\n",
      "\n",
      "[2] Great Agent Hack 2025: Holistic AI x UCL. Available at: https://hackathon.holisticai.com/ (accessed November 22, 2025).\n",
      "\n",
      "[3] Valyu AI. (2025). The Great Agent Hack 2025: Agent Performance, Reliability and Valyu-Powered Retrieval. Retrieved from https://www.valyu.ai/blogs/the-great-agent-hack-2025-agent-performance-reliability-and-valyu-powered-retrieval\n",
      "\n",
      "[4] Great Agent Hack 2025. “Project gallery — Great Agent Hack 2025: Build and test transparent, robust, and safe AI agents for real‑world impact.” Devpost. Available at: https://hai-great-agent-hack-2025.devpost.com/project-gallery?page=1.\n",
      "\n",
      "[5] Holistic AI. (2025). Hackathon 2025 [Source code]. GitHub. https://github.com/holistic-ai/hackathon-2025 (Last accessed: November 30, 2025)\n",
      "\n",
      "[6] Elon Musk Stunned by Jensen Huang’s DGX Spark Gift. (n.d.). YouTube Shorts. https://www.youtube.com/shorts/l7x_Tfrbubs\n",
      "\n",
      "[7] Andriushchenko, M., Souly, A., Dziemian, M., Duenas, D., Lin, M., Wang, J., Hendrycks, D., Zou, A., Kolter, Z., Fredrikson, M., Winsor, E., Wynne, J., Gal, Y., & Davies, X. (2024). AgentHarm: A benchmark for measuring harmfulness of LLM agents. arXiv. https://arxiv.org/abs/2410.09024\n",
      "\n",
      "[8] Tiku N., Schaul K. and Chen S. (2023, November 01). This is how AI image generators see the world. Washington Post. https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/ (last accessed Aug 20, 2025).\n",
      "\n",
      "[9] Porikli, S., & Porikli, V. (2025). Hidden Bias in the Machine: Stereotypes in Text-to-Image Models. Available at: https://openreview.net/pdf?id=u4KsKVp53s\n",
      "\n",
      "[10] Wu, Z., Cho, S., Munoz, C., King, T., Mohammed, U., Kazimi, E., Pérez-Ortiz, M., Bulathwela, S., & Koshiyama, A. (2025). AgentGraph: Trace-to-Graph platform for interactive analysis and robustness testing in agentic AI systems. Holistic AI & University College London.\n",
      "\n",
      "[11] Wicaksono, I., Wu, Z., Patel, R., King, T., Koshiyama, A., & Treleaven, P. (2025). Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs\n",
      "\n",
      "[12] Zhang, S., Yin, M., Zhang, J., Liu, J., Han, Z., Zhang, J., Li, B., Wang, C., Wang, H., Chen, Y., & Wu, Q. (2025). Which agent causes task failures and when? On automated failure attribution of LLM multi-agent systems (arXiv Preprint No. 2505.00212).\n",
      "\n",
      "Title: Nexus isn’t going all-in on AI, keeping half of its new $700M fund for India startups\n",
      "Author: ['Jagmeet Singh', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media', 'Min-Width']\n",
      "Publish Date: 2025-12-04 00:00:00\n",
      "Content: While many venture firms seem to only have eyes for AI these days, Nexus Venture Partners is deliberately splitting its focus for its new $700 million fund.\n",
      "\n",
      "The firm will back AI startups and seek out India-focused startups in consumer, fintech, and digital infrastructure.\n",
      "\n",
      "AI has soaked up most of the venture capital raised globally and the 20-year-old VC firm also sees AI as a defining technological shift. But it argues crowding into a single, overheated category carries its own risks. India’s digital economy provides a counterbalance: an expanding market where AI adoption is rising and opportunities remain more diverse.\n",
      "\n",
      "For Nexus, that balance is rooted in its origins. The Delaware-headquartered firm, with offices in Menlo Park, Mumbai and Bengaluru, has operated as a single fund and an integrated U.S.–India team since its founding in 2006.\n",
      "\n",
      "It backs early-stage software and India-focused startups from the same pool of capital. Over time, its cross-border software bets have encompassed a range from infrastructure and developer tools to AI agent startups. U.S. portfolio includes companies such as Postman, Apollo, MinIO, Giga, and Firecrawl, which have become widely adopted in developer tooling and AI infrastructure.\n",
      "\n",
      "Meanwhile its India portfolio has broadened across consumer, fintech, logistics, and digital infrastructure. Some of its bets there include Zepto, Delhivery, Rapido, Turtlemint, and Infra.Market\n",
      "\n",
      "“AI is a huge inflection point, and we are anchoring on that,” Jishnu Bhattacharjee, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch in an interview. “But we are also seeing that many of these AI innovations are actually getting used to serve the masses better.”\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "Nexus manages $3.2 billion in capital across its funds and has invested in more than 130 companies over the years. The firm has recorded more than 30 exits to date, including several IPOs, underscoring the depth of its early-stage, long-horizon approach.\n",
      "\n",
      "Abhishek Sharma, a managing partner at Nexus Venture Partners in the U.S., told TechCrunch the firm’s sweet spot remains inception to seed and Series A, often beginning with checks as small as a few hundred thousand dollars or around $1 million.\n",
      "\n",
      "Nexus, which operates with an eight-member investment team, began with a $100 million fund and has kept its fund size at $700 million since launching Fund VII in 2023. It typically raises every 2.5 to 3 years. Bhattacharjee said the reason for keeping the eighth fund the same size was the firm believes $700 million is the right amount for its early-stage strategy.\n",
      "\n",
      "“We don’t want to raise money for the sake of raising,” he noted.\n",
      "\n",
      "Even though India’s AI journey is not as advanced as the U.S.’s in many areas, Nexus believes India could leapfrog in several parts of the AI ecosystem.\n",
      "\n",
      "Bhattacharjee underlined the country’s large talent pool, rising digital infrastructure, and demand for localized models that support India’s many languages and service needs. These dynamics, he said, are pushing Indian startups to build AI applications and agents faster, often atop open-source tools and emerging domestic AI infrastructure companies.\n",
      "\n",
      "The partners pointed to companies backed by Nexus, such as Zepto and Neysa, to illustrate how AI is taking shape in India. They said Zepto, the quick-commerce platform, uses AI extensively across its operations — from customer support to routing and fulfillment — demonstrating how consumer businesses are becoming deeply AI-native. Besides, infrastructure players like Neysa are emerging to address India-specific needs, including sovereign AI workloads, localized data handling and support for the country’s many languages.\n",
      "\n",
      "Nexus did not share fund metrics. The partners said its funds have been realizing significant enough returns over the years to largely fill this fund from returning limited partners. The firm’s LP base spans the U.S., Europe, the Middle East, Southeast Asia and Japan.\n",
      "\n",
      "Title: All the biggest news from AWS’ big tech show re:Invent 2025\n",
      "Author: ['Kirsten Korosec', 'Julie Bort', 'Rebecca Szkutak', 'Transportation Editor', 'Venture Editor', 'Senior Reporter', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Connie Loizos']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: Amazon Web Services’ annual tech conference AWS re:Invent has wrapped up another day with a deluge of product news and keynotes — plus the obligatory customer success stories.\n",
      "\n",
      "The unsurprising theme is AI for the enterprise. This year it’s all about upgrades that give customers greater control to customize AI agents, including one that AWS claims can learn from you and then work independently for days.\n",
      "\n",
      "AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.\n",
      "\n",
      "“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”\n",
      "\n",
      "On December 3, the conference pressed on with its AI agents messaging, as well as deeper dives into customer stories. Swami Sivasubramanian, vice president of Agentic AI at AWS, gave one of the keynote talks. To say he was bullish is perhaps understating the vibe.\n",
      "\n",
      "\n",
      "\n",
      "“We are living in times of great change,” Sivasubramanian said during the talk. “For the first time in history, we can describe what we want to accomplish in natural language, and agents generate the plan. They write the code, call the necessary tools, and execute the complete solution. Agents give you the freedom to build without limits, accelerating how quickly you can go from idea to impact in a big way.”\n",
      "\n",
      "While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the ones that got our attention. TechCrunch will update this article, with the newest insights at the top, through the end of AWS re:Invent. Be sure to check back.\n",
      "\n",
      "Doubling down on LLMs\n",
      "\n",
      "AWS announced more tools for enterprise customers to create their own models. Specifically, AWS said it is adding new capabilities for both Amazon Bedrock and Amazon SageMaker AI to make building custom LLMs easier.\n",
      "\n",
      "\n",
      "\n",
      "For instance, AWS is bringing serverless model customization to SageMaker, which allows developers to start building a model without needing to think about compute resources or infrastructure. The serverless model customization can be accessed through either a self-guided path or by prompting an AI agent.\n",
      "\n",
      "\n",
      "\n",
      "AWS also announced Reinforcement Fine Tuning in Bedrock, which allows developers to choose a preset workflow or reward system and have Bedrock run their customization process automatically from start to finish.\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "Andy Jassy shares some numbers\n",
      "\n",
      "Amazon CEO Andy Jassy took to social media platform X to expound on AWS chief Matt Garman’s keynote speech. The message: The current generation of its Nvidia-competitor AI chip Trainium2 is already bringing in loads of cash.\n",
      "\n",
      "His comments were tied to the reveal of its next-generation chip, Trainium3, and meant to forecast a promising revenue future for the product.\n",
      "\n",
      "Database savings arrives\n",
      "\n",
      "Tucked among the dozens of announcements is one item that is already getting cheers: Discounts.\n",
      "\n",
      "Specifically, AWS said it was launching Database Savings Plans, which help customers reduce database costs by up to 35% when they commit to a consistent amount of usage ($/hour) over a one-year term. The company said the savings will automatically apply each hour to eligible usage across supported database services, and any additional usage beyond the commitment is billed at on-demand rates.\n",
      "\n",
      "Corey Quinn, chief cloud economist at Duckbill, summed it up well in his blog post, “Six years of complaining finally pays off.”\n",
      "\n",
      "Can’t get a better deal than free, Amazon hopes\n",
      "\n",
      "Is there any way for another AI coding tool to win the hearts of startup founders? Amazon hopes a year’s worth of credits, for free, will do the trick for its offering, Kiro. The company will be giving away credits to Kiro Pro+ to qualified startups that apply for the deal before the end of the month. However, only early-stage startups in certain countries are eligible.\n",
      "\n",
      "An AI training chip and Nvidia compatibility\n",
      "\n",
      "AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.\n",
      "\n",
      "AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.\n",
      "\n",
      "Expanded AgentCore capabilities\n",
      "\n",
      "AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.\n",
      "\n",
      "AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.\n",
      "\n",
      "A nonstop AI agent worker bee\n",
      "\n",
      "AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.\n",
      "\n",
      "Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such as preventing incidents when pushing new code live. Preview versions of the agents are available now.\n",
      "\n",
      "New Nova models and services\n",
      "\n",
      "AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.\n",
      "\n",
      "The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.\n",
      "\n",
      "Lyft’s argument for AI agents\n",
      "\n",
      "The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues.\n",
      "\n",
      "The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year.\n",
      "\n",
      "An AI Factory for the private data center\n",
      "\n",
      "Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.\n",
      "\n",
      "The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.\n",
      "\n",
      "Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.\n",
      "\n",
      "Title: Meta poaches Apple design exec Alan Dye to lead new creative studio in Reality Labs\n",
      "Author: ['Amanda Silberling', 'Senior Writer', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: Alan Dye, the design executive who led Apple’s user interface team for the last decade, is leaving the company to join Meta, according to a report from Bloomberg’s Mark Gurman.\n",
      "\n",
      "This is a significant hire for Meta, as the company makes a push toward consumer devices like smart glasses and virtual reality headsets. Dye will focus on improving AI features in these devices and report directly to Chief Technology Officer Andrew Bosworth.\n",
      "\n",
      "At Apple, Dye will be replaced by Steve Lemay, who has had “a key role in the design of every major Apple interface since 1999,” according to a statement Apple CEO Tim Cook gave Bloomberg.\n",
      "\n",
      "It seems that Meta is recruiting from its competitors to help the company compete in the AI race, as Meta also poached researchers from OpenAI this summer. (Allegedly, Meta CEO Mark Zuckerberg hand-delivered homemade soup to an OpenAI employee in a recruitment push; OpenAI chief research officer Mark Chen said that he has since delivered his own soup to promising Meta recruits.)\n",
      "\n",
      "Shortly after the news broke of Dye’s departure, Zuckerberg announced a new creative studio within Reality Labs that would be led by Dye. There, he’ll be joined by Billy Sorrentino, another former Apple designer who led interface design across Reality Labs; Joshua To, who led interface design across Reality Labs; Meta’s industrial design team, led by Pete Bristol; and its metaverse design and art teams led by Jason Rubin.\n",
      "\n",
      "Zuckerberg said the studio would “bring together design, fashion, and technology to define the next generation of our products and experiences.”\n",
      "\n",
      "“Our idea is to treat intelligence as a new design material and imagine what becomes possible when it is abundant, capable, and human-centered,” the Meta CEO wrote on Threads. “We plan to elevate design within Meta, and pull together a talented group with a combination of craft, creative vision, systems thinking, and deep experience building iconic products that bridge hardware and software.”\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "This article was updated after publication with additional information about Meta’s plans.\n",
      "\n",
      "Title: Andy Jassy says Amazon’s Nvidia competitor chip is already a multibillion-dollar business\n",
      "Author: ['Julie Bort', 'Venture Editor', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media', 'Min-Width']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: Can any company, big or small, really topple Nvidia’s AI chip dominance? Maybe not. But there are hundreds of billions of dollars of revenue for those who can even peel off a chunk of it for themselves, Amazon CEO Andy Jassy said this week.\n",
      "\n",
      "As expected, the company revealed at the AWS re:Invent conference the next generation of its Nvidia-competitor AI chip, Trainium3, which is 4x faster yet uses less power than the current Trainium2. Jassy revealed a few tidbits about the current Trainium in a post on X that shows why the company is so bullish on the chip.\n",
      "\n",
      "He said the Trainium2 business “has substantial traction, is a multi-billion-dollar revenue run-rate business, has 1M+ chips in production, and 100K+ companies using it as the majority of Bedrock usage today.”\n",
      "\n",
      "Bedrock is Amazon’s AI app development tool that allows companies to pick and choose among many AI models.\n",
      "\n",
      "Jassy said Amazon’s AI chip is winning among the company’s enormous roster of cloud customers because it “has price-performance advantages over other GPU options that are compelling.” In other words, he believes it works better and costs less than those “other GPUs” out there on the market.\n",
      "\n",
      "That is, of course, Amazon’s classic MO, offering its own homegrown tech at lower prices.\n",
      "\n",
      "Additionally, AWS CEO Matt Garman offered even more insight in an interview with CRN, about one customer responsible for a big chunk of those billions in revenue: No shock here, it’s Anthropic.\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "“We’ve seen some enormous traction from Trainium2, particularly from our partners at Anthropic who we’ve announced Project Rainier, where there’s over 500,000 Trainium2 chips helping them build the next generations of models for Claude,” Garman said.\n",
      "\n",
      "Project Rainier is Amazon’s most ambitious AI cluster of servers, spread across multiple data centers in the U.S. and built to serve Anthropic’s skyrocketing needs. It came online in October. Amazon is, of course, a major investor in Anthropic. In exchange, Anthropic made AWS its primary model training partner, even though Anthropic is now also offered on Microsoft’s cloud via Nvidia’s chips.\n",
      "\n",
      "OpenAI is now also using AWS in addition to Microsoft’s cloud. But the OpenAI partnership couldn’t have contributed much to Trainium’s revenue because AWS is running it on Nvidia chips and systems, the cloud giant said.\n",
      "\n",
      "Indeed, only a few U.S. companies like Google, Microsoft, Amazon, and Meta have all the engineering pieces — silicon chip design expertise, homegrown high-speed interconnect. and networking technology — to even attempt true competition with Nvidia. (Remember, Nvidia cornered the market on one major high-performance networking tech in 2019 when CEO Jensen Huang outbid Intel and Microsoft to buy InfiniBand hardware maker Mellanox.)\n",
      "\n",
      "On top of that, AI models and software built to be served up by Nvidia’s chips also rely on Nvidia’s proprietary Compute Unified Device Architecture (CUDA) software. CUDA allows the apps to use the GPUs for parallel processing compute, among other tasks. Just like the Intel versus SPARC chip war of yesterday, it’s no small thing to rewrite an AI app for a non-CUDA chip.\n",
      "\n",
      "Still, Amazon may have a plan for that. As we previously reported, the next generation of its AI chip, Trainium4, will be built to interoperate with Nvidia’s GPUs in the same system. Whether that helps peel more business away from Nvidia or simply reinforces its dominance, but on AWS’s cloud, remains to be seen.\n",
      "\n",
      "It may not matter to Amazon. If it is already on track to make multibillion dollars from the Trainium2 chip, and the next generation will be that much better, it may be winner enough.\n",
      "\n",
      "Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.\n",
      "\n",
      "Title: WordPress’s vibe-coding experiment, Telex, has already been put to real-world use\n",
      "Author: ['Sarah Perez', 'Consumer News Editor', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: WordPress’s experimental AI development tool, Telex, has already been put to real-world use, only months after its September debut. At the company’s annual “State of the Word” event on Tuesday in San Francisco, WordPress project co-founder and Automattic CEO Matt Mullenweg shared several examples where Telex had been used within a working WordPress shop to do things like create price comparisons, price calculators, and pull in real-time business hours plus a map link to a retail store, among other examples.\n",
      "\n",
      "Telex, which Mullenweg previously described as a “v0 or Lovable, but specifically for WordPress,” is essentially the publishing platform’s attempt to build its own vibe-coding tool for the AI era. The software allows developers to generate Gutenberg blocks — the modular bits of text, images, columns, and more — that make up a WordPress website.\n",
      "\n",
      "While the software is still labeled as an experiment, Mullenweg was able to demonstrate several real-world examples that had been built by community creator Nick Hamze.\n",
      "\n",
      "In the first example, Mullenweg showed off a pricing comparison tool built with Telex, noting that these sorts of rich, interactive web elements were something that a developer used to have to custom-build but could now be created in a few seconds.\n",
      "\n",
      "In another demo, a developer used Telex to add real-time store hours, a phone number, and a link to get directions to the header block of their WordPress site.\n",
      "\n",
      "Telex was also shown being used to create a carousel of partner logos on a business’s site, a custom pricing tool, a Google Calendar integration, and a grid for posts on a WordPress homepage, where each post’s card on the site had the same height.\n",
      "\n",
      "“Again, things that you used to have to, like, hire developers, do custom software like this would have cost thousands, tens of thousands of dollars to build, even just years ago. We’re now able to do in a browser for pennies,” said Mullenweg. “It’s kind of insane.”\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "Image Credits:WordPress State of the Word\n",
      "\n",
      "Another developer, Tammie Lister, used Telex to create a new Gutenberg block every day in the month of October, creating things like a playable, ASCII version of Tetris and a trick-or-treat block for Halloween.\n",
      "\n",
      "In an email to TechCrunch, Hamze touted Telex’s capabilities, saying, “the thing that blows my mind and should blow yours is I’m not a developer. I can’t write a single line of code, but I can describe what I want to Telex, and it can make it for me. That freedom is intoxicating, and I’m all in on AI,” he said.\n",
      "\n",
      "“I think as long as people think of these tools as ‘developer’ tools, they are missing the point on what they can really accomplish, which is letting regular folks do things they never could have done before,” Hamze added.\n",
      "\n",
      "The Telex demos were discussed alongside other AI-focused initiatives at WordPress, including architectural developments, like the Abilities API and MCP adapter. The former defines what WordPress can do in a way that AI systems can interpret, the company explained, while the latter exposes those abilities so any MCP-compatible tool can understand and use them.\n",
      "\n",
      "“This adapter pattern means WordPress can participate in AI workflows without duplicating logic or creating separate integrations for every AI platform,” Mullenweg told event attendees. “So you can now connect a WordPress installation to popular tools like Claude, Copilot, and many other platforms that support MCP.”\n",
      "\n",
      "In addition, he noted that developers were already using AI in their everyday workflows through tools like Cursor, Claude Code, and other next-generation CLIs. This, Mullenweg said, “means you can refactor projects, search code bases, automate tasks, [and] run scripts with WP CLI alongside the AI agent.”\n",
      "\n",
      "Mullenweg said that, in 2026, WordPress would introduce some benchmarks and evaluations that AI models can use to test on WordPress tasks, like changing plugins, editing text, or even manipulating the WordPress interface using browser agents.\n",
      "\n",
      "This article was updated after initial publication with a comment from Hamze.\n",
      "\n",
      "Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.\n",
      "\n",
      "Title: VCs deploy ‘kingmaking’ strategy to crown AI winners in their infancy\n",
      "Author: ['Marina Temkin', 'Reporter', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: In early October, DualEntry, an AI enterprise resource planning (ERP) startup, announced a $90 million Series A round led by Lightspeed and Khosla Ventures, valuing the one-year-old business at $415 million.\n",
      "\n",
      "The company seeks to replace legacy software like Oracle NetSuite with its offering that can automate routine tasks and provide predictive insights. The massive funding round from top-tier VCs signaled that the startup is likely experiencing phenomenal revenue growth.\n",
      "\n",
      "However, one VC who declined to invest told TechCrunch that DualEntry’s annual recurring revenue (ARR) was just around $400,000 when he reviewed the deal in August. DualEntry’s co-founder, Santiago Nestares, denies that number. When asked about revenue when the deal closed, Nestares said it was “considerably higher than that.”\n",
      "\n",
      "Even so, an extremely handsome valuation relative to revenue is becoming an increasingly common investment strategy among top-tier VC firms. The tactic is known as “kingmaking.”\n",
      "\n",
      "This approach involves deploying massive funding into one startup in a competitive category, aiming to overwhelm rivals by granting the chosen company a bank-account advantage so significant that it creates the appearance of market dominance.\n",
      "\n",
      "Kingmaking isn’t new, but its timing has shifted dramatically.\n",
      "\n",
      "“Venture capitalists have always evaluated a set of competitors and then made a bet on who they think the winner is going to be in a category. What’s different is that it’s happening much earlier,” said Jeremy Kaufmann, a partner at Scale Venture Partners.\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "This early aggressive funding contrasts with the last investment cycle.\n",
      "\n",
      "“The 2010s version of this was just called ‘capital as a weapon,’” said David Peterson, partner at Angular Ventures. He pointed out that massive funding into Uber and Lyft was a canonical example of this, but the capital weaponization for the ride-sharing companies didn’t begin until they reached their Series C or D rounds.\n",
      "\n",
      "As with Uber vs. Lyft, investors in DualEntry’s competitors Rillet and Campfire are evidently just as eager to see their bets succeed with the help of substantive capital. In early August, Rillet raised a $70 million Series B led by a16z and Iconiq, just two months after the company closed a $25 million Series A led by Sequoia.\n",
      "\n",
      "Similarly, Campfire AI had two back-to-back funding rounds. In October, it grabbed a $65 million Series B, just a couple of months after announcing a $35 million Series A round led by Accel.\n",
      "\n",
      "AI ERP is just one of the several AI application categories where startups are raising funding in rapid succession. “There’s no new data between rounds. Series Bs happen 27-60 days after Series As regularly,” Jaya Gupta a partner at Foundation Capital, posted on X last month. Besides AI ERP, she wrote that she sees this pattern in categories such as IT service management and SOC compliance.\n",
      "\n",
      "While some startups like Cursor or Lovable have reportedly grown at a breakneck pace between their back-to-back rounds, several VCs told TechCrunch that’s not the case for all. AI ERPs and several other categories of startups that raised multiple rounds in 2025 still have ARRs in the single-digit millions, these investors said.\n",
      "\n",
      "Although not all VCs agree that kingmaking is a sound investment strategy, there are reasons why offering large amounts of capital could be beneficial even when the startup maintains a modest burn rate. For instance, well-funded startups are perceived as more likely to survive by large enterprise buyers, making them the preferred vendor for significant software purchases. That’s a strategy that helped legal AI startup Harvey attract large law firm customers, investors say.\n",
      "\n",
      "Still, history shows that massive capitalization offers no guarantee of success, with notable failures, including the logistics company Convoy and the bankruptcy reorg of scooter company Bird.\n",
      "\n",
      "But those precedents don’t faze major VC firms. They prefer to bet on a category that seems like a good case for AI, and they would rather invest early because, as Peterson put it: “Everybody has fully internalized the lesson of the power law. In the 2010s, companies could grow faster and be bigger than almost anybody had realized. You couldn’t have overpaid if you were an early Uber investor.”\n",
      "\n",
      "Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.\n",
      "\n",
      "Title: Anthropic hires lawyers as it preps for IPO\n",
      "Author: ['Dominic-Madori Davis', 'Rebecca Szkutak', 'Julie Bort', 'Jagmeet Singh', 'Kirsten Korosec', 'Amanda Silberling', '.Post-Authors-List__Authors --Font-Size Var', 'Align-Items Center Display Flex Gap Var', '.Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Display Flex Flex-Shrink Margin Padding .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Li List-Style None Margin-Left Margin-Top Important .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Li First-Child Margin-Left .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs .Post-Authors-List__Author-Thumb Background-Color Var', 'Border Solid Var --Wp--Custom--Color--White']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: In Brief\n",
      "\n",
      "Anthropic is prepping for an IPO that could come as early as 2026, the FT reports.\n",
      "\n",
      "It has brought on law firm Wilson Sonsini to help kick off the process, and the company is tackling an internal checklist to prepare it for what could be one of the largest IPOs ever.\n",
      "\n",
      "The company is also reportedly looking to raise a funding round that could value it at over $300 billion and has also been in talks with investment banks, though it has not chosen an underwriter, the FT reported. Anthropic last announced a $13 billion raise in September, giving it a $183 billion valuation. Wilson Sonsini has been an adviser to Anthropic since 2022.\n",
      "\n",
      "This news comes as OpenAI, valued at $500 billion, is also reportedly testing the waters for an IPO and has started to prep itself for the process, though no listing date has been suggested, Reuters reported.\n",
      "\n",
      "Title: AWS doubles down on custom LLMs with features meant to simplify model creation\n",
      "Author: ['Rebecca Szkutak', 'Senior Reporter', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: Right on the heels of announcing Nova Forge, a service to train custom Nova AI models, Amazon Web Services (AWS) announced more tools for enterprise customers to create their own frontier models.\n",
      "\n",
      "AWS announced new capabilities in Amazon Bedrock and Amazon SageMaker AI at its AWS re:Invent conference on Wednesday. These new capabilities are designed to make building and fine-tuning custom large language models (LLMs) easier for developers.\n",
      "\n",
      "The cloud provider is introducing serverless model customization in SageMaker, which allows developers to start building a model without needing to think about compute resources or infrastructure, according to Ankur Mehrotra, general manager of AI platforms at AWS, in an interview with TechCrunch.\n",
      "\n",
      "To access these serverless model-building capabilities, developers can either follow a self-guided point-and-click path or an agent-led experience where they can prompt SageMaker using natural language. The agent-led feature is launching in preview.\n",
      "\n",
      "“If you’re a healthcare customer and you wanted a model to be able to understand certain medical terminology better, you can simply point SageMaker AI, if you have labeled data, then select the technique and then off SageMaker goes, and [it] fine tunes the model,” Mehrotra said.\n",
      "\n",
      "This capability is available for customizing Amazon’s own Nova models and certain open source models (those with publicly available model weights), including DeepSeek and Meta’s Llama.\n",
      "\n",
      "AWS is also launching Reinforcement Fine-Tuning in Bedrock that allows developers to choose either a reward function or a pre-set workflow, and Bedrock will run a model customization process automatically from start to finish.\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "Frontier LLMs — meaning the most advanced AI models — and model customization appear to be an area of focus for AWS at this year’s conference.\n",
      "\n",
      "AWS announced Nova Forge, a service where AWS will build custom Nova models for its enterprise customers for $100,000 a year, during AWS CEO Matt Garman’s keynote on Tuesday.\n",
      "\n",
      "“A lot of our customers are asking, ‘If my competitor has access to the same model, how do I differentiate myself?’” Mehrotra said. “‘How do I build unique solutions that are optimized, that optimize my brand, for my data, for my use case, and how do I differentiate myself?’ What we’ve found is that, the key to solving that problem is being able to create customized models.”\n",
      "\n",
      "AWS has yet to gain a substantial user base for its AI models. A July survey from Menlo Ventures found that enterprises greatly prefer Anthropic, OpenAI, and Gemini to other models. However, the ability to customize and fine-tune these LLMs could start to give AWS a competitive advantage.\n",
      "\n",
      "Follow along with all of TechCrunch’s coverage of the annual enterprise tech event here, and see all the announcements you may have missed thus far here.\n",
      "\n",
      "Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.\n",
      "\n",
      "Title: Another bid to block state AI regulation has failed… for now\n",
      "Author: ['Rebecca Bellan', 'Amanda Silberling', 'Connie Loizos', 'Tim De Chant', '.Post-Authors-List__Authors --Font-Size Var', 'Align-Items Center Display Flex Gap Var', '.Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Display Flex Flex-Shrink Margin Padding .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Li List-Style None Margin-Left Margin-Top Important .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs Li First-Child Margin-Left .Post-Authors-List__Authors .Post-Authors-List__Author-Thumbs .Post-Authors-List__Author-Thumb Background-Color Var', 'Border Solid Var --Wp--Custom--Color--White', 'Border-Radius', 'Height -O-Object-Fit Cover Object-Fit Cover Width .Post-Authors-List__Authors .Post-Authors-List__Author-List Display Flex Flex-Wrap Wrap Gap Var']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: In Brief\n",
      "\n",
      "The latest bid to squeeze a ban on states regulating AI into an annual defense bill has reportedly been rejected after facing bipartisan pushback.\n",
      "\n",
      "House Majority Leader Steve Scalise (R-LA) said Tuesday that Republican leaders would look for “other places” to include the measure — an effort that President Trump has supported — according to The Hill.\n",
      "\n",
      "The proposal to preempt states from enacting their own AI regulation came months after GOP lawmakers sought to include a 10-year moratorium on state AI laws in Trump’s tax and spending bill earlier this year. The provision failed then due to strong resistance from both parties.\n",
      "\n",
      "Silicon Valley has supported such measures, arguing that state regulations create an unworkable patchwork of rules that could stymy innovation.\n",
      "\n",
      "Critics argue that most state AI legislation is focused on safety, transparency, and consumer protections, and in the absence of federal AI laws that perform those tasks, blocking states from regulating would be effectively handing over control to Big Tech with no oversight.\n",
      "\n",
      "Scalise reportedly acknowledged that the defense bill was not the place to include such a provision, and echoed Trump’s previous calls to introduce the ban as a separate bill. A leaked draft executive order signals Trump is considering taking matters into his own hands, though those efforts have reportedly paused for now.\n",
      "\n",
      "Title: Google Photos’ 2025 Recap turns to Gemini to find your highlights\n",
      "Author: ['Sarah Perez', 'Consumer News Editor', 'Lorenzo Franceschi-Bicchierai', 'Dominic-Madori Davis', 'Aisha Malik', 'Julie Bort', 'Connie Loizos', 'Anthony Ha', '--C-Author-Card-Image-Size Align-Items Center Display Flex Gap Var', 'Media']\n",
      "Publish Date: 2025-12-03 00:00:00\n",
      "Content: Google Photos users can now access their year-end Recap, the photo-hosting site’s own version of something akin to Spotify Wrapped. Like other annual reviews, the Google Photos Recap lets you look back on your past year in photos, offering a combination of memorable highlights enhanced with graphics and other effects, plus photo stats and more.\n",
      "\n",
      "U.S. users will also gain access to a new feature powered by Google’s AI, Gemini, which will showcase your hobbies and other top highlights, the company says.\n",
      "\n",
      "First introduced in 2024, Google Photos Recap aims to capitalize on the data-powered review trend, popularized by services like Spotify Wrapped, and, in past decades, by time-traveling apps like Timehop.\n",
      "\n",
      "However, this year, the Google Photos Recap also serves as a testing ground for Gemini, as the company unleashes the AI on your photo archive to help surface more of the moments you might like to review. Google says that Gemini models can understand the context of your photos to pull out more details. Specifically, the models were used to identify things like your “one true passion” and the other top four highlights that “made your year” in the Recap.\n",
      "\n",
      "Image Credits:Google\n",
      "\n",
      "In addition, the recap will offer photo stats from the year, like total photo count, top people, and, new for this year, a total selfie count. The feature also now lets you hide specific people or photos. After doing so, you can then regenerate your Recap for an updated version.\n",
      "\n",
      "The photos and memories from the Recap can be easily shared on social media and elsewhere. A new integration with CapCut will add a button at the end of the Recap to export it to the photo and video editing app, where you can use other Google Photos templates to customize the Recap further.\n",
      "\n",
      "Image Credits:Google\n",
      "\n",
      "There’s also a new carousel at the end of the Recap containing short videos, photos, and collages designed for sharing to group chats or social media. One option even allows you to share your Recap directly to your WhatsApp Status.\n",
      "\n",
      "Techcrunch event Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. Join the Disrupt 2026 Waitlist Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector. San Francisco | WAITLIST NOW\n",
      "\n",
      "If you don’t immediately see your Recap, you can request Google Photos to generate it for you using an option at the top of the app. After viewing the Recap, it will remain in your app throughout the month of December. To access it again during this time, you can find it either in the back of your Memories carousel or pinned in your Collections tab.\n",
      "\n",
      "Image Credits:Google\n",
      "\n",
      "In addition to the annual review, Google Photos will release a series of 2025 highlights throughout the month of December, the company noted.\n",
      "\n",
      "Title: The future of country music is here, and it’s AI\n",
      "Author: ['Charlie Harding']\n",
      "Publish Date: 2025-12-04 12:00:00+00:00\n",
      "Content: When songwriter Patrick Irwin moved to Nashville last year, he was entering a lottery. Each day hundreds of sessions take place where writers create a song demo to pitch to a publisher. Publishers then share those songs with labels and managers, who may share those songs with the artists. Even if a major country star records (“cuts”) the song, it still takes a stroke of luck for that song to become a No. 1 hit.\n",
      "\n",
      "The odds of winning are extremely low. Recently, Irwin was in a room where his cowriters Sam Fink and Duane Deerweater tried something new. Instead of booking studio time or calling a “track guy” to produce a demo, one cowriter opened Suno, an AI music platform, uploaded a voice memo with just guitar and vocals, and typed in a prompt: “traditional country, male vocal, folk country, story telling, 90s country, rhythmic.” Thirty seconds later he had two fully produced demos complete with drums, electric guitars, bass, and backing harmonies. There were no studio musicians, no invoices.\n",
      "\n",
      "“You tell it the genre and it totally does the whole thing, it’s insane,” says Irwin. He was as astonished as he was disturbed. This was not the Nashville, a city with a storied 200-year history of producing much of America’s greatest music, that he had imagined.\n",
      "\n",
      "Irwin isn’t alone in this feeling. In the background, AI is taking over the city. At the start of 2024, few professionals had even tried these tools, but in the past six months, songwriters and producers have embraced them to work faster and cheaper — and for some, more resourcefully. No label, no major publisher, nor Suno would give comment for this story. But after speaking with musicians, writers, and a dozen anonymous insiders, it’s clear that Nashville has become an AI town.\n",
      "\n",
      "Original recording of \"Hold On To You\" by Patrick Irwin, Sam Fink, and Duane Deerweater:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Hold On To You\" remixed by Suno with the prompt: \"Traditional country / male vocal / Folk country / Story telling / 90s country / rhythmic\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Songwriter Trannie Anderson — who’s written for Lainey Wilson, Dan + Shay, and Reba McEntire — says the tech is ubiquitous. Though she doesn’t use it herself, she sees wide use “from entry-level songwriters to the top dogs.” She isn’t exaggerating; multiple sources have told me that even stars like Dustin Lynch and Jelly Roll are being sent pitches with their voices artificially generated into demos, something AI voice transfer makes possible. Lynch’s manager Brad Belanger confirmed this, adding, “What a world we’re moving into.” Jelly Roll’s representatives declined to comment on the record.\n",
      "\n",
      "The headlines might be focused on “Walk My Walk” by Breaking Rust, an AI song that recently topped the Billboard Country Digital Song Sales chart, but that story is largely smoke and mirrors. That chart is a holdover from the pre-streaming era and easily gamed. The real revolution isn’t on the charts yet — it’s happening in the writing rooms.\n",
      "\n",
      "Nashville is known as a “10-year town,” reflecting the amount of time you have to grind it out before getting a hit. Even for established songwriters, it takes a long while between the original songwriting session and making it to country radio. “Two years is the fastest I’ve seen,” says Jon Sherwood, a UMPG writer who penned the Bailey Zimmerman and Luke Combs hit “Backup Plan.” Sherwood still writes the traditional way, sans AI, but he notes the speed is changing the game. In the past, writers would pen a song, then pay a “track guy” $500 to $1,000 to record a professional demo to pitch to stars.\n",
      "\n",
      "Suno is creating efficiencies in the Nashville song assembly line. Songwriters still write the lyrics and melody because Suno’s generated lyrics are cloyingly cliche (though many of Nashville-based sources agreed that country radio hits can be too). They instead use the AI to handle the demo production. They record a voice memo, upload it, and use the “remix” feature to turn it into bro-country, alt-country, or “hick-hop” in seconds. Maggie Reaves, a rising songwriter signed to publishers Dream 3 and Kobalt, recently had an assignment for a major artist with a one-day turnaround, so she wrote the song and “threw it in Suno.” Her publisher told her, “This is perfect. This is going straight to her.”\n",
      "\n",
      "Reaves writes an average of 200 songs a year. Demoing her songs can be prohibitively expensive. She used to save up money to demo songs for $500 each — demoing all her songs could hypothetically cost tens of thousands of dollars each year. Now, she pays $96 a year for near-infinite attempts: “I immediately saw this could replace that.”\n",
      "\n",
      "A sample draft of a voice memo \"Dirt Road\" by Charlie Harding:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Dirt Road\" remixed by Suno with the prompt: \"Style: Outlaw country with a driving half-time shuffle. Deep baritone male vocal with a laid-back but commanding delivery, similar to Waylon Jennings. Warm analog tone, slightly gritty. Instrumentation: Twangy Telecaster electric guitar with light overdrive, steady acoustic rhythm guitar, Fender bass with a tight, forward groove, brushed or lightly swung drums, subtle pedal steel swells, occasional baritone guitar riffs. Vibe & Production: 70s Nashville outlaw sound — dry drums, wide but minimal reverb, punchy low end, guitars panned wide, vocals centered and intimate. Feel should be confident, cool, and unhurried.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"Dirt Road\" with additional lyrics written with specifically-prompted Claude, remixed by Suno with the prompt: \"Style: Warm, modern country with a blend of Southern soul and rootsy swagger. Strong, rich female vocal with plenty of character — earthy, expressive, and slightly smoky. Laid-back confidence with melodic phrasing that feels both classic and current. Instrumentation: Round, melodic bass; tight, steady drums with a hint of swampy groove; acoustic guitar strums; smooth electric guitar licks with light twang; occasional slide guitar or pedal steel for emotional lift; light organ pads underneath. Production: Polished but organic. Warm low end, clean guitars, vocals placed upfront with subtle grit, tasteful reverb for space but not gloss. Overall vibe should feel grounded, heartfelt, and radio-ready with a bit of a back-porch feel. Mood: Honest, soulful, confident storytelling — grounded in real experience, heart, and country charm.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "She’s not alone. Publishers are even running back catalogs through Suno to find new angles on forgotten tracks.\n",
      "\n",
      "The sound isn’t perfect and can resemble an over-compressed, “dated” MP3: slightly lo-fi, not very dynamic, low sample rate and bit depth. But the uncanny valley of Suno songs is undeniably the voice, presenting with a slight grainy quality to the vocal that is overly pitch-corrected; the inflections are borderline Cylon. Yet Reaves says 70 percent of the output is solid enough to play in the car (traffic noise is enough to mask low-quality sound) and gives a clear picture of the finished song in order to pitch to an artist. These are demos, after all.\n",
      "\n",
      "“You tell it the genre and it totally does the whole thing, it’s insane.”\n",
      "\n",
      "Suno isn’t just useful as a demoing tool; it also helps producers rapidly brainstorm different creative approaches to a song or musical passage. Independent songwriter Kalen Nash usually produces songs the old way, in his studio, track by track, but recently has adopted Suno for creative inspiration; he calls it a “band in your pocket.” He’s used it to turn diary entries into full songs. Jacob Durrett, a Big Loud producer, uses it to find alternative versions and “vibes” for songs. He can put in a “half-cooked” idea for inspiration: Just a guitar idea scratch track is enough for Suno to output multiple melody and full song ideas in any genre imaginable. “I’m in awe of it sometimes, how good it can be, you know?” He says that Suno is giving him “a productivity boost more than a creative boost.” As a skilled musician, he’s equally capable of trying a song out in any style — it just takes longer. His hope is that AI will take over the tedious parts of his job, like renaming files and preparing them to mix, so he can focus on the creative part.\n",
      "\n",
      "Music publisher Eric Olson, who encourages writers to use the tool, calls it an “unlimited co-writer in the room.” He finds it useful for coming up with samples without the headache of clearances, or concerns that someone else has sampled the same part. For him, it’s about buying back time. “If I can give Suno the last 20 percent and spend more time with my kids, that’s huge,” he says.\n",
      "\n",
      "It’s not all biscuits and gravy, though. Most people in Nashville wear multiple hats: songwriter, “track guy,” studio musician, touring guitarist. “There used to be a whole world where musicians were making six figures only playing demo scale,” says Ian Fitchuk, the Grammy-winning producer of Kacey Musgraves’ Golden Hour. Fitchuk has steered clear of Suno and worries about the musicians losing that income. Trannie Anderson calls it “the final nail in the coffin” for the demo studio system. If the “farm team” of demo players disappears, the industry might face a talent crisis down the road.\n",
      "\n",
      "There are also legal and ethical headaches: “If Suno spits out a lead line an artist uses, what’s the protocol?” asks Reaves. The copyright office doesn’t protect AI work, which makes the ownership of these hybrid songs messy; plus, the AI was trained on existing music. “AI learns from my songs, my friends’ songs … We aren’t being compensated,” says Anderson. And there’s the “ick” factor. Durrett hates when the AI outputs a voice that sounds exactly like a friend of his. (He claims it’s happened many times.)\n",
      "\n",
      "Despite the concerns, Suno just secured $250 million in funding, is making $200 million in annual revenue, and adoption is accelerating. Wait around two years and you’re going to hear songs made with the help of Suno all over country radio.\n",
      "\n",
      "The question is how they will sound. Anderson hears that “there’s an element missing,” she says. “Humanity and a soul … The Holy Spirit doesn’t live in AI.”\n",
      "\n",
      "Title: Anthropic’s AI bubble ‘YOLO’ warning\n",
      "Author: ['Alex Heath']\n",
      "Publish Date: 2025-12-03 21:45:00+00:00\n",
      "Content: Posts from this author will be added to your daily email digest and your homepage feed.\n",
      "\n",
      "This is an excerpt of Sources by Alex Heath, a newsletter about AI and the tech industry, syndicated just for The Verge subscribers once a week.\n",
      "\n",
      "Dario Amodei took the stage at the DealBook Summit on Wednesday to throw punches without naming names.\n",
      "\n",
      "The Anthropic CEO spent a good chunk of the interview with Andrew Ross Sorkin drawing a careful line between his company’s approach and that of a certain competitor. When asked about whether the AI industry is in a bubble, Amodei separated the “technological side” from the “economic side” and then twisted the knife.\n",
      "\n",
      "“On the technological side, I feel really solid,” he said. “On the economic side, I have my concerns where, even if the technology fulfills all its promises, I think there are players in the ecosystem who, if they just make a timing error, they just get it off by a little bit, bad things could happen.”\n",
      "\n",
      "Who might those players be? Despite Sorkin’s prodding, Amodei wouldn’t name OpenAI or Sam Altman. But he didn’t have to.\n",
      "\n",
      "“There are some players who are YOLOing,” he said. “Let’s say you’re a person who just kind of constitutionally wants to YOLO things or just likes big numbers, then you may turn the dial too far.”\n",
      "\n",
      "He also touched on “circular deals,” where chip suppliers like Nvidia invest in AI companies that then spend those funds on their chips. Amodei acknowledged that Anthropic has done some of these deals, though “not at the same scale as some other players,” and walked through the math of how they can work responsibly: A new gigawatt data center costs roughly $10 billion to build over five years. A vendor invests upfront, and an AI startup pays back its share of the deal as revenue grows.\n",
      "\n",
      "While he again didn’t name names, he referenced the eye-popping numbers OpenAI has been trumpeting for its compute buildout. “I don’t think there’s anything wrong with that in principle,” he said. “Now, if you start stacking these where they get to huge amounts of money, and you’re saying, ‘By 2027 or 2028 I need to make $200 billion a year,’ then yeah, you can overextend yourself.”\n",
      "\n",
      "The cone of uncertainty\n",
      "\n",
      "The heart of Amodei’s argument was a concept he’s been using internally: the “cone of uncertainty.”\n",
      "\n",
      "He said that Anthropic’s revenue has grown tenfold annually for three years, from zero to $100 million in 2023, $100 million to $1 billion in 2024, and now somewhere between $8 billion and $10 billion by this year’s end. (Sam Altman, by comparison, has said that OpenAI expects to end 2025 with an annualized revenue run rate exceeding $20 billion.) But even Amodei doesn’t know if Anthropic will hit $20 billion or $50 billion next year. “It’s very uncertain.”\n",
      "\n",
      "That uncertainty is concerning, he explained, because data centers take one to two years to build. Decisions on 2027 compute needs have to be made now. Buy too little, and you lose customers to competitors. Buy too much, and you risk bankruptcy. Amodei added, “How much buffer there is in that cone is basically determined by my margins.”\n",
      "\n",
      "“We want to buy enough that we’re confident even in the 10th percentile scenario,” he said. “There’s always a tail risk. But we’re trying to manage that risk well.” He positioned Anthropic’s enterprise focus, with higher margins and more predictable revenue, as structurally safer than that of consumer-first businesses. “We don’t have to do any code reds.”\n",
      "\n",
      "Title: Anyone can try to edit Grokipedia 0.2 but Grok is running the show\n",
      "Author: ['Robert Hart']\n",
      "Publish Date: 2025-12-03 17:47:45+00:00\n",
      "Content: Elon Musk envisions Grokipedia — xAI’s AI-generated, anti-woke spin on Wikipedia — as a definitive monument to human knowledge, something complete and truthful enough to etch in stone and preserve in space. In reality, it’s a hot mess, and it’s only getting worse now that anyone can suggest edits.\n",
      "\n",
      "Proposing edits on Grokipedia is simple, so simple that the site apparently doesn’t feel a need to give instructions on how to do it. You highlight some text, click the “Suggest Edit” button, and fill in a form with a summary of the proposed change, with an option to suggest content and provide supporting sources. Reviewing edit suggestions is Grok, xAI’s problematic, Musk worshipping AI chatbot. Grok, yes, the chatbot, will also be the one making actual changes to the site. Most edits on Wikipedia don’t require approval, but there is an active community of human editors who watch the “recent changes” page closely.\n",
      "\n",
      "It’s not very clear what changes Grok is making, though. The system is confusing and isn’t very transparent. Grokipedia tells me there have been “22,319” approved edits so far, though I’ve no way of seeing what these edits were, on what pages they happened, or who suggested them. It contrasts with the well-documented editing logs on Wikipedia, which can be sorted by pages, users, or, in the case of anonymous users, IP addresses. My hunch is that many of Grokipedia’s edits are adding internal links to other Grokipedia pages within articles, though I’ve no firm evidence beyond scrolling through a few pages.\n",
      "\n",
      "The closest I got to seeing where edits were actually happening was on the homepage. There’s a small panel below the search bar displaying five or so recent updates on a rotation, though these only give the name of the article and say that an unspecified edit has been approved. Not exactly comprehensive. These are entirely at the mercy of whatever users feel like suggesting, leading to a confusing mix of stories. Elon Musk and religious pages were the only things that seemed to come up frequently when I looked, interspersed with things like the TV shows Friends and The Traitors UK and requests to note the potential medical benefits of camel urine.\n",
      "\n",
      "On Wikipedia, there is a clear timeline of edits outlining what happened, who did what, and the reasons for doing so, with viewable chat logs for contentious issues. There are also copious guidelines on editing style, sourcing requirements, and processes, and you can directly compare edited versions of the site to see exactly what changed and where. Grokipedia had no such guidelines — and it showed, many requests were a jumbled mess — but it did have an editing log. It was a nightmare that only hinted at transparency. The log — which only shows a timestamp, the suggestion, and Grok’s decision and often-convoluted AI-generated reasoning — must be scrolled through manually on a tiny pop-up at the side of the page with no ability to skip ahead or sort by time or type of edit. It’s frustrating, and that’s with only a few edits, and it doesn’t show where changes were actually implemented. With more edits, it would be completely unusable.\n",
      "\n",
      "Unsurprisingly, Grok doesn’t seem to be the most consistent editor. It makes for confounding reading at times and edit logs betray the lack of clear guidelines for wannabe editors. For example, the editing log for Musk’s biographical page shows many suggestions about his daughter, Vivian, who is transgender. Editors suggest using both her name and pronouns in line with her gender identity and those assigned at birth. While it’s almost impossible to follow what happened precisely, Grok’s decision to edit incrementally meant there was a confusing mix of both throughout the page.\n",
      "\n",
      "As a chatbot, Grok is amenable to persuasion. For a suggested edit to Musk’s biographical page, a user suggested “the veracity of this statement should be verified,” referring to a quote about the fall of Rome being linked to low birth rates. In a reply far wordier than it needed to be, Grok rejected the suggestion as unnecessary. For a similar request with different phrasing, Grok reached the opposite conclusion, accepting the suggestion and adding the kind of information it previously said was unnecessary. It isn’t too taxing to imagine how one might game requests to ensure edits are accepted.\n",
      "\n",
      "While this is all technically possible on Wikipedia, the site has a small army of volunteer administrators — selected after a review process or election — to keep things in check. They enforce standards by blocking accounts or IP addresses from editing and locking down pages in cases of page vandalism or edit wars. It’s not clear Grokipedia has anything in place to do the same, leaving it completely at the mercy of random people and a chatbot that once called itself MechaHitler. The issue showed itself on several pages related to World War II and Hitler, for example. I found repeated (rejected) requests to note the dictator was also a painter and that far fewer people had died in the Holocaust than actually did. The corresponding pages on Wikipedia were “protected,” meaning they could only be edited by certain accounts. There were also detailed logs explaining the decision to protect them. If the editing system — or site in general — were easier to navigate, I’m sure I’d find more examples.\n",
      "\n",
      "Pages like these are obvious targets for abuse, and it’s no surprise they’re among the first hit by malicious editors. They won’t be the last, and with Grokipedia’s chaotic editing system and Grok’s limited guardrails, it may soon be hard to tell what’s vandalism and what isn’t. At this rate, Grokipedia doesn’t feel poised for the stars, it feels poised to collapse into a swamp of barely readable disinformation.\n",
      "\n",
      "Title: One day, AI might be better than you at surfing the web. That day isn’t today.\n",
      "Author: ['Victoria Song']\n",
      "Publish Date: 2025-12-03 17:00:00+00:00\n",
      "Content: is a senior reporter and author of the Optimizer newsletter. She has more than 13 years of experience reporting on wearables, health tech, and more. Before coming to The Verge, she worked for Gizmodo and PC Magazine.\n",
      "\n",
      "All I wanted was a pair of New Balances. I was done trusting stylish influencers who swore Vans, Converse, and Allbirds were up to the challenge of walking 20,000 steps day in and day out. They are not. Fall is the season of holiday sales, so there’s no better time to shop… if you’re immune to being overwhelmed by modern day e-commerce.\n",
      "\n",
      "Wouldn’t it be grand if I could skip all the fake deals and barely disguised ads, and have the internet find the best stuff for me? What if I could tell the internet my wish and have it granted?\n",
      "\n",
      "Tech CEOs have been evangelizing that this is the future. Tell the bot what you want, kick up your feet, and let AI do the rest. Microsoft CEO Satya Nadella recently said on a podcast that one day, AI will be able to “use a computer as well as a human.” He’s far from the only executive touting that bots and agents might soon be better than we are at just about everything.\n",
      "\n",
      "In the past few weeks, it’s become clear that browsers are the latest entrant in the AI arms race. We’re talking about things like Perplexity’s Comet, ChatGPT Atlas, and even Chrome — browsers that natively embed chatbots into the internet experience. The pitch is to reorient how we browse, to move us away from the search engines that have reigned for the past three decades. The central idea is the same as we’ve heard from all the other agents-all-the-way-down companies: AI will be just as good as you are at surfing the web. Possibly better.\n",
      "\n",
      "Big, if true.\n",
      "\n",
      "Right now, AI browsers come in two main flavors. There are your regular browsers that have an AI assistant stapled on in a collapsible window, such as Chrome with its Gemini features, or Edge with Copilot Mode. Then there are more specialized AI browsers, most notably ChatGPT Atlas, Perplexity’s Comet, and The Browser Company’s Dia. This second category often supplants your search bar with AI and sometimes includes an “agentic mode,” in which the AI can complete more complex, browser-related tasks for you. Theoretically, that includes helping you book reservations or add items to a shopping cart.\n",
      "\n",
      "Evaluating AI browsers While AI browsers share a similar approach, they each have varying takes on the ideal web surfing experience. Some make you pay for certain features, and of course, there are differences in the underlying models. But this isn’t meant to be a ranking. For this piece, I’m evaluating whether AI browsers can currently deliver a better internet. So I decided to focus on three main criteria: When are AI browsers most useful? I’m looking to see which, if any, browsing tasks become easier or faster by adding AI.\n",
      "\n",
      "How much prompt babying is needed? Theoretically, I shouldn’t have to craft an overly specific prompt or answer a zillion follow-ups to get the result I’m looking for. Google is good at figuring out what you meant to type — I expect the same from ChatGPT.\n",
      "\n",
      "If there is an agent, do I trust it to complete tasks for me? The whole point of AI agents is to let them do things for me. You need high confidence that the results are trustworthy.\n",
      "\n",
      "For testing, I decided on a few ground rules. I kept it to five browsers: Chrome, Edge, Atlas, Comet, and Dia. There are more available, but this felt like a representative mix of both AI browser categories from a variety of players in the field. I focused on desktop apps, and tried to make settings as uniform as possible: I generally instructed the AI browsers to keep answers snappy, shared my location information where possible, enabled memory settings, and described myself as a “tech journalist specializing in health and wearable tech.” I also approached testing from a variety of AI skill levels. What would results look like if I was a complete AI newbie versus someone more adept at prompting? Lastly, if I tried one task in a browser, I gave it a go in all the browsers, down to the same exact prompt.\n",
      "\n",
      "Ultimately, my question was not which AI browser you should use, but whether any of them are worth your time and energy. This was a journey to see whether any of them live up to the hype.\n",
      "\n",
      "The short answer: they don’t.\n",
      "\n",
      "Stapling an AI assistant to a browser doesn’t magically redefine how you interact with a chatbot. It’s more like hanging out in person, rather than texting. You’re having the same conversation, just in different formats, each with their own pros and cons. But no matter the browser, I kept running into the same fundamental problem: you have to think extra hard about how to craft the right prompt.\n",
      "\n",
      "That’s the opposite of how search, particularly Google, has evolved. At Google’s peak, you could type in a string of misspelled words into the search bar, and somehow you’d still get the right answer. AI models require a lot more prep and guidance.\n",
      "\n",
      "At Google’s peak, you could type in a string of misspelled words into the search bar, and somehow you’d still get the right answer.\n",
      "\n",
      "Take the universal torture of sorting your emails. On any given day, I want to know what my most important emails are, and which ones I need to respond to ASAP. The first few times I took a crack at this task, I asked the various browsers to summarize my emails. (I know “summarize my emails” isn’t a stellar prompt, but it’s often a default suggestion. And defaults exist because they should be generally helpful.) All I got were literal descriptions of the emails in my inbox. In my personal inbox, it said I had one thread in my primary folder, listed the subject, summarized the preview, and then stated it was “dated Nov 20th, and was marked not starred or important.”\n",
      "\n",
      "I tried refining my ask. Instead of “summarize,” I prompted AI to “identify important emails based on urgency.” In my work inbox, that generated a list of not-important, not-urgent email threads because the models have no idea what I actually find important. I wanted reader feedback, pitches from trusted contacts, or threads I’d forgotten to reply to. What I got instead were irrelevant pitches, mostly for health scams.\n",
      "\n",
      "I made zero headway until Comet suggested the prompt “find important unanswered emails.” The top four emails that surfaced were littered with important keywords for a tech journalist — Urgent! Embargo! Exclusive for The Verge! All had multiple follow-up requests. You can see why Comet would think they mattered, but after reviewing them, all were emails I didn’t need to read at all, much less reply to. AI had fallen for the oldest trick in the book: conflating keywords with truth.\n",
      "\n",
      "I was ready to write off the experiment when I noticed Comet’s AI had buried the lede. Nestled three-quarters of the way into its long-winded summary was a bullet point labeled “Personalized requests/follow-ups.” It highlighted two emails: one from a CEO addressing feedback I’d made in a recent product write-up, and another from a reader with a tip relevant to my beat. Neither was “urgent” but both merited a closer look.\n",
      "\n",
      "I tried Comet’s “find important unanswered emails” prompt in the other AI browsers. They all highlighted other previously skipped, keyword-stuffed pitches. None flagged the two emails I was interested in. So I tried even harder:\n",
      "\n",
      "Find unanswered emails in which I had previously responded with interest or feature personalized requests/feedback. Then, evaluate which ones I should respond to based on timeliness and keywords such as “embargo” featuring dates in the next two weeks. Ignore emails with multiple follow-ups to which I have not responded.\n",
      "\n",
      "This went slightly better with Comet and Dia. Both surfaced multiple relevant email threads, but only one ultimately required a response. Copilot in Edge highlighted one relevant thread and five junk pitches. Gemini in Chrome was a dud: It surfaced only a Black Friday marketing email.\n",
      "\n",
      "In Atlas, ChatGPT merely replied, “It looks like Gmail successfully returned the unread message IDs, but the actual content for those messages didn’t come back — the batch read returned empty, which means the Gmail API didn’t provide the email bodies this round.” It proceeded to ask two long-winded follow-ups. At this point, my options were to refine my prompt further or give up.\n",
      "\n",
      "I gave up.\n",
      "\n",
      "Emails were mostly a failure, but there were some daily tasks where AI browsers were alright. I had to search through a 48-page legal document for a family matter, and while CMD-F is tried and true, the legalese made my brain melt. So I loaded the document in a tab and prompted the AI browsers to list all the relevant pages and sections, with an accompanying summary. All the browsers returned the same pages, with slightly different summaries. I still had to do my own reading, but it got me to a useful jumping-off point faster.\n",
      "\n",
      "These browsers can also work well for internet search — provided you’re patient enough to reprogram 20 years of Google muscle memory. Where AI search works best is answering questions about the exact site you’re on. While pondering a phone upgrade, I asked the bots to compile various iPhone specs and size dimensions into a table from Apple’s website and the wider web. That was much more convenient and helpful than flipping through multiple tabs. At the end of the process, I was much more confident about which iPhone I was upgrading to.\n",
      "\n",
      "I was much more successful whenever I shifted my mindset to “how can AI help me interact with this page?”\n",
      "\n",
      "Whenever I went into a task asking AI to do things for me, I’d end up frustrated. I was much more successful whenever I shifted my mindset to “how can AI help me interact with this page?” For example, I was trying to parse a clinical study, and hit a particularly technical paragraph written in dense medical jargon. Asking the models to summarize and explain some concepts I was iffy on in plain English was helpful.\n",
      "\n",
      "Summarizing or compiling data like this was the most convenient part of using AI browsers. All the browsers do this fairly well, and it’s a useful thing to have at your fingertips — it’s not without the occasional back-and-forth, but overall, I needed less time and fewer tabs to get to a point where I’d take over the heavy lifting of getting something done online. I’m always in favor of fewer tabs while browsing.\n",
      "\n",
      "We already know that AI is good at summarizing and compiling, though. Complex queries are where these browsers are meant to shine. Here, too, one must grease both elbows and wrestle AI into submission.\n",
      "\n",
      "Ahead of the Stranger Things season 5 premiere, I was chatting with a colleague about watching an 18-minute YouTube recap video. They were separately working on another AI project and asked if AI browser assistants could turn videos into downloadable, .txt transcript files. So I tried prompting: can you rip a transcript of this YouTube video?\n",
      "\n",
      "Previous Next\n",
      "\n",
      "1 / 2 Note how Dia says it will export the full transcript. Screenshot: Dia\n",
      "\n",
      "Copilot said no, on account of the video’s copyright. (Never mind that most YouTube videos already have transcripts, right there on the page. This should not be a hard problem.) What I could get was a summary or an outline of the video’s content. Comet ripped an accurate transcript for the first 25 seconds before stating that the “transcript continues for Seasons 1-4 with detailed plot and character recaps.” Dia gave a time-stamped transcript, but only for the first 15 minutes. Atlas and Chrome were the only two to give full transcripts. As in, an extremely long, line-by-line transcript right into the chat window.\n",
      "\n",
      "Next, I asked each browser’s AI if they could turn that transcript into a downloadable .txt file with timestamps. Only Atlas completed the task. The rest said generating a downloadable file wasn’t in the cards, but I could copy-paste the plain text into a file myself.\n",
      "\n",
      "So much for “just telling the AI what I want.”\n",
      "\n",
      "After several detours, I returned to my original task: figuring out which pair of New Balances to buy, and finding the best deal possible.\n",
      "\n",
      "When I say I wanted a pair of New Balances, it’s because I’ve spent about three months researching. I look at social media, I ask friends, I read up on the history of various brands before ultimately choosing one. Then I’ll spend a few hours on that brand’s website whittling down my options until I have about three. Afterwards, I’ll try to find a deal online. It’s a long, arduous process prone to human error. Hence, why it’s been two years since I set out to find a pair of durable, stylish, and comfy walking shoes and I’ve yet to find one.\n",
      "\n",
      "With AI browsers, the research part was “easy.” In a nutshell, I had to give it highly specific research prompts. That meant telling it that I’m: flat-footed, more comfortable in wide shoes, looking for a lifestyle sneaker and therefore no running shoes, looking for something that can easily handle 15,000 to 20,000 steps per day, interested in a versatile color but prefer a neutral white, wanting something that works with athleisure and elevated street wear, and not looking to spend more than $120 (but would prefer to stay under $100).\n",
      "\n",
      "Because I didn’t specify “closest store near me” in my original prompt, I’m being asked to pick which store. Screenshot: ChatGPT Atlas\n",
      "\n",
      "What ensued were multiple back-and-forths where the browsers both did and didn’t listen to my needs. The longer the response, the more likely I’d get contradictory advice — here’s a $200 ultra-performance running shoe with a carbon plate as your top rec, but at the very bottom, here’s an $85 model that has more of a lifestyle feel in the completely wrong color. Rinse and repeat. After roughly 20 rounds across five browsers, I arrived at the New Balance 530.\n",
      "\n",
      "The 530 had also made it to my short list when I’d done the process manually. But while I was faster at narrowing down New Balance models on my own, AI had provided reasoning behind each choice. Things like, this model has extra cushioning for durability or the silhouette in that model would work with multiple outfits. My picks were mostly based on vibes.\n",
      "\n",
      "Enter phase two: finding a deal. I asked all five browsers to find me the lowest price on a pair of New Balance 530s in all white, white-and-silver, or white-and-pink, in a women’s 8.5 (25cm), that’s in-stock in my zip code. If there was an agentic mode, I asked the AI to put it in my cart.\n",
      "\n",
      "This was the third attempt to close the pop-up. Screenshot: ChatGPT Atlas\n",
      "\n",
      "Cue several more back-and-forths, in which I got differing results. Dia, Comet, Chrome, and Edge found the same local Foot Locker, but selected different colors. Atlas was able to finally put the right pair in my cart, but not without checking in several times to make sure I really wanted to. It also tried to override my preference for pickup and switch to delivery. Once, I watched Atlas spend a minute trying to close a pop-up just to get back to shopping.\n",
      "\n",
      "I ran the full experiment several times, and each time, I was sure the browsers were finding the best price on a given day. However, I became less and less confident that these were the shoes I really wanted. Especially when Atlas threw in the New Balance 574 Core as an alternative, because they were “one of NB’s most iconic everyday silhouettes” and were a versatile, androgynous shoe. (ChatGPT knows I love unisex styles.)\n",
      "\n",
      "If I’m judging these browsers on the premise that AI could be better than you at surfing the web, that simply isn’t the case. At no point would I have considered the experience “hands-off.” But more broadly, my whole AI browser experience reinforced that I spend a lot of time doing things for AI so that it can sometimes do things for me. I’m changing the way I think, the way I word questions, and the way I search and digest information. It’s less about how AI fits into my life and more about how I can adapt what I do naturally to accommodate its growing presence.\n",
      "\n",
      "A good experience with these browsers assumes a lot of things. So does googling, but after 20 years, it requires much less mental effort than the best of what AI browsers currently offer. With AI browsers, you’ve got to be fairly adept at prompting. You’ve got to understand the strengths of chatbots — and be patient enough to work around their weaknesses. Or, at the very least, you have to be willing to learn. This is true for many people. But I’m not confident that anyone who downloads an AI browser will find the learning curve worth it.\n",
      "\n",
      "AI can sometimes be useful, but it’s always a lot of work. And I still need new shoes. I’ve decided to just visit a New Balance store in person.\n",
      "\n",
      "Title: Amazon says Alexa Plus can find that movie scene you’re thinking about\n",
      "Author: ['Jess Weatherbed']\n",
      "Publish Date: 2025-12-03 15:00:00+00:00\n",
      "Content: Posts from this author will be added to your daily email digest and your homepage feed.\n",
      "\n",
      "Amazon has launched a new AI-powered feature for Fire TV that lets you jump to specific moments of a movie by describing the scene to Alexa Plus. The feature, which was previously announced during Amazon’s hardware event in September, works with Prime Video and builds on the X-Ray feature that provides information about the content you’re watching.\n",
      "\n",
      "“Our number one mission at Fire TV is getting you to what you want to watch — fast,” Amazon says in its announcement. “Just describe a movie scene like you would to a friend, and Alexa Plus will jump directly to that specific moment — no more searching required.”\n",
      "\n",
      "The Alexa Plus feature “works with thousands of Prime Video movies by understanding scene descriptions, character names, and famous quotes,” according to Amazon. Users can skip to a scene by mentioning details about characters, actors, locations, and more, such as asking to find “the card scene in Love Actually,” or “where Joshua asks, ‘shall we play a game?’” in WarGames.\n",
      "\n",
      "The feature is designed to make it easier and faster to locate and watch scenes compared to manually fast-forwarding through movies, giving Fire TV users fewer reasons to search for the same content on other platforms like YouTube. Amazon says the feature utilizes a variety of AI models, including Amazon Nova and Anthropic Claude, and can identify movies without the title being included in the descriptions.\n",
      "\n",
      "The Fire TV feature is currently limited to indexed scenes in select movies that have been purchased or rented from Prime Video, or are available to stream via a Prime membership subscription. Amazon says that the feature will soon be expanded to include more scenes and TV shows.\n",
      "\n",
      "\n",
      "===== DOMAIN: CYBER_SECURITY =====\n",
      "\n",
      "Title: ThreatsDay Bulletin: Wi-Fi Hack, npm Worm, DeFi Theft, Phishing Blasts— and 15 More Stories\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: Think your Wi-Fi is safe? Your coding tools? Or even your favorite financial apps? This week proves again how hackers, companies, and governments are all locked in a nonstop race to outsmart each other.\n",
      "\n",
      "Here's a quick rundown of the latest cyber stories that show how fast the game keeps changing.\n",
      "\n",
      "If there's one thing these stories show, it's that cybersecurity never sleeps. The threats might sound technical, but the impact always lands close to home — our money, our data, our trust. Staying alert and informed isn't paranoia anymore; it's just good sense.\n",
      "\n",
      "Title: 5 Threats That Reshaped Web Security This Year [2025]\n",
      "Author: ['The Hacker News', 'Dec']\n",
      "Publish Date: 2025-12-05 00:00:00\n",
      "Content: As 2025 draws to a close, security professionals face a sobering realization: the traditional playbook for web security has become dangerously obsolete. AI-powered attacks, evolving injection techniques, and supply chain compromises affecting hundreds of thousands of websites forced a fundamental rethink of defensive strategies.\n",
      "\n",
      "Here are the five threats that reshaped web security this year, and why the lessons learned will define digital protection for years to come.\n",
      "\n",
      "1. Vibe Coding\n",
      "\n",
      "Natural language coding, \"vibe coding\", transformed from novelty to production reality in 2025, with nearly 25% of Y Combinator startups using AI to build core codebases. One developer launched a multiplayer flight simulator in under three hours, eventually scaling it to 89,000 players and generating thousands in monthly revenue.\n",
      "\n",
      "The Result\n",
      "\n",
      "Code that functions perfectly yet contains exploitable flaws, bypassing traditional security tools. AI generates what you ask for, not what you forget to ask.\n",
      "\n",
      "The Damage\n",
      "\n",
      "Production Database Deleted – Replit's AI assistant wiped Jason Lemkin's database (1,200 executives, 1,190 companies) despite code freeze orders\n",
      "\n",
      "– Replit's AI assistant wiped Jason Lemkin's database (1,200 executives, 1,190 companies) despite code freeze orders AI Dev Tools Compromised – Three CVEs exposed critical flaws in popular AI coding assistants: CurXecute (CVE-2025-54135) enabled arbitrary command execution in Cursor, EscapeRoute (CVE-2025-53109) allowed file system access in Anthropic's MCP server, and (CVE-2025-55284) permitted data exfiltration from Claude Code via DNS-based prompt injection\n",
      "\n",
      "– Three CVEs exposed critical flaws in popular AI coding assistants: CurXecute (CVE-2025-54135) enabled arbitrary command execution in Cursor, EscapeRoute (CVE-2025-53109) allowed file system access in Anthropic's MCP server, and (CVE-2025-55284) permitted data exfiltration from Claude Code via DNS-based prompt injection Authentication Bypassed – AI-generated login code skipped input validation, enabling payload injection at a U.S. fintech startup\n",
      "\n",
      "– AI-generated login code skipped input validation, enabling payload injection at a U.S. fintech startup Unsecure code statistics in Vibe coding – 45% of all AI-generated code contains exploitable flaws; 70% Vulnerability Rate in the Java language.\n",
      "\n",
      "Base44 Platform Compromised (July 2025)\n",
      "\n",
      "In July 2025, security researchers discovered a critical authentication bypass vulnerability in Base44, a popular vibe coding platform owned by Wix. The flaw allowed unauthenticated attackers to access any private application on the shared infrastructure, affecting enterprise applications handling PII, HR operations, and internal chatbots.\n",
      "\n",
      "Wix patched the flaw within 24 hours, but the incident exposed a critical risk: when platform security fails, every application built on top becomes vulnerable simultaneously.\n",
      "\n",
      "The Defense Response\n",
      "\n",
      "Organizations now implement security-first prompting, multi-step validation, and behavioral monitoring that detects unexpected API calls, deviant serialization patterns, or timing vulnerabilities. With the EU AI Act classifying some vibe coding as \"high-risk AI systems,\" functional correctness no longer guarantees security integrity.\n",
      "\n",
      "2. JavaScript Injection\n",
      "\n",
      "In March 2025, 150,000 websites were compromised by a coordinated JavaScript injection campaign promoting Chinese gambling platforms. Attackers injected scripts and iframe elements impersonating legitimate betting sites like Bet365, using full-screen CSS overlays to replace actual web content with malicious landing pages.\n",
      "\n",
      "The campaign's scale and sophistication demonstrated how lessons from 2024's Polyfill.io compromise, where a Chinese company weaponized a trusted library affecting 100,000+ sites, including Hulu, Mercedes-Benz, and Warner Bros., had been weaponized into repeatable attack patterns. With 98% of websites using client-side JavaScript, the attack surface has never been larger.\n",
      "\n",
      "The Impact\n",
      "\n",
      "Even React's XSS protection failed as attackers exploited prototype pollution, DOM-based XSS, and AI-driven prompt injections.\n",
      "\n",
      "The Damage\n",
      "\n",
      "150,000+ Sites Compromised – Gambling campaign demonstrated industrial-scale JavaScript injection in 2025\n",
      "\n",
      "– Gambling campaign demonstrated industrial-scale JavaScript injection in 2025 22,254 CVEs Reported – A 30% jump from 2023, exposing massive vulnerability growth\n",
      "\n",
      "– A 30% jump from 2023, exposing massive vulnerability growth 50,000+ Banking Sessions Hijacked – Malware targeted 40+ banks across three continents using real-time page structure detection\n",
      "\n",
      "The Solution\n",
      "\n",
      "Organizations now store raw data and encode by output context: HTML encoding for divs, JavaScript escaping for script tags, URL encoding for links. Behavioral monitoring flags when static libraries suddenly make unauthorized POST requests.\n",
      "\n",
      "Download the 47-page JavaScript injection playbook with framework-specific defenses\n",
      "\n",
      "3. Magecart/E-skimming 2.0\n",
      "\n",
      "Magecart attacks surged 103% in just six months as attackers weaponized supply chain dependencies, according to Recorded Future's Insikt Group. Unlike traditional breaches that trigger alarms, web skimmers masquerade as legitimate scripts while harvesting payment data in real-time.\n",
      "\n",
      "The Reality\n",
      "\n",
      "Attacks demonstrated alarming sophistication: DOM shadow manipulation, WebSocket connections, and geofencing. One variant went dormant when Chrome DevTools opened.\n",
      "\n",
      "The Damage\n",
      "\n",
      "Major Brands Compromised – British Airways, Ticketmaster, and Newegg lost millions in fines and reputation damage\n",
      "\n",
      "– British Airways, Ticketmaster, and Newegg lost millions in fines and reputation damage Modernizr Library Weaponized – Code activated only on payment pages across thousands of websites, invisible to WAFs\n",
      "\n",
      "– Code activated only on payment pages across thousands of websites, invisible to WAFs AI-Powered Selectivity – Attackers profiled browsers for luxury purchases, exfiltrating only high-value transactions\n",
      "\n",
      "cc-analytics Domain Campaign (Sep 2025)\n",
      "\n",
      "Security researchers uncovered a sophisticated Magecart campaign leveraging heavily obfuscated JavaScript to steal payment card data from compromised e-commerce websites, with the malicious infrastructure centered around the domain cc-analytics[.]com has actively been harvesting sensitive customer information for at least one year\n",
      "\n",
      "The Defense Response\n",
      "\n",
      "Organizations discovered CSP provided false confidence; attackers simply compromised whitelisted domains. The solution: validate code by behavior, not source. PCI DSS 4.0.1 Section 6.4.3 now requires continuous monitoring of all scripts accessing payment data, with compliance mandatory from March 2025.\n",
      "\n",
      "4. AI Supply Chain Attacks\n",
      "\n",
      "Malicious package uploads to open-source repositories jumped 156% in 2025 as attackers weaponized AI. Traditional attacks meant stolen credentials. New threats introduced polymorphic malware that rewrites itself with each instance and context-aware code that detects sandboxes.\n",
      "\n",
      "The Consequence\n",
      "\n",
      "AI-generated variants mutate daily, rendering signature-based detection useless. IBM's 2025 report showed breaches take 276 days to identify and 73 days to contain.\n",
      "\n",
      "The Damage\n",
      "\n",
      "Solana Web3.js Backdoor – Hackers drained $160,000–$190,000 in cryptocurrency during a five-hour window\n",
      "\n",
      "– Hackers drained $160,000–$190,000 in cryptocurrency during a five-hour window 156% Surge in Malicious Packages – Semantically camouflaged with documentation and unit tests to appear legitimate\n",
      "\n",
      "– Semantically camouflaged with documentation and unit tests to appear legitimate 276-Day Detection Window – AI-generated polymorphic malware evades traditional security scanning\n",
      "\n",
      "The Shai-Hulud Worm (Sep-Dec 2025)\n",
      "\n",
      "Self-replicating malware used AI-generated bash scripts (identified by comments and emojis) to compromise 500+ npm packages and 25,000+ GitHub repositories in 72 hours. The attack weaponized AI command-line tools for reconnaissance and was designed to evade AI-based security analysis – both ChatGPT and Gemini incorrectly classified the malicious payloads as safe. The worm harvested credentials from developer environments and automatically published trojanized versions using stolen tokens, turning CI/CD pipelines into distribution mechanisms.\n",
      "\n",
      "The Counter-Measures\n",
      "\n",
      "Organizations deployed AI-specific detection, behavioral provenance analysis, zero-trust runtime defense, and \"proof of humanity\" verification for contributors. The EU AI Act added penalties up to €35 million or 7% of global revenue.\n",
      "\n",
      "5. Web Privacy Validation\n",
      "\n",
      "Research revealed that 70% of top US websites drop advertising cookies even when users opt out, exposing organizations to compliance failures and reputational damage. Periodic audits and static cookie banners couldn't keep pace with \"privacy drift.\"\n",
      "\n",
      "The Problem\n",
      "\n",
      "Marketing pixels collect unauthorized IDs, third-party code tracks outside stated policies, and consent mechanisms break after updates, all silently.\n",
      "\n",
      "The Damage\n",
      "\n",
      "€4.5 Million Fine for Retailer – Loyalty program script sent customer emails to external domains for four months undetected\n",
      "\n",
      "– Loyalty program script sent customer emails to external domains for four months undetected HIPAA Violations at Hospital Network – Third-party analytics scripts silently collected patient data without consent\n",
      "\n",
      "– Third-party analytics scripts silently collected patient data without consent 70% Cookie Non-Compliance – Top US websites ignore user opt-out preferences, contradicting privacy claims\n",
      "\n",
      "Capital One Tracking Pixels (March 2025)\n",
      "\n",
      "The federal court ruled that Meta Pixel, Google Analytics, and Tealium's sharing of credit card application status, employment details, and bank account information constituted \"data exfiltration\" under CCPA. The March 2025 decision expanded liability beyond traditional breaches, exposing companies to $100-$750 per incident (CCPA) plus $5,000 per incident (CIPA wiretap violations), turning routine tracking into litigation risk equivalent to security breaches.\n",
      "\n",
      "The Defense Response: Continuous web privacy validation became the solution: agentless monitoring ensuring real-world activity aligns with declared policies through data mapping, instant alerts, and fix verification. Only 20% of companies felt confident in compliance at the year's start; those implementing continuous monitoring simplified audits and integrated privacy into security workflows.\n",
      "\n",
      "Download the CISO's Expert Guide to Web Privacy Validation with vendor-specific recommendations here.\n",
      "\n",
      "The Path Forward: Proactive Security in an AI-Driven World\n",
      "\n",
      "These five threats share a common thread: reactive security has become a liability. The lesson of 2025 is clear: by the time you detect a problem with traditional methods, you've already been compromised.\n",
      "\n",
      "Organizations thriving in this landscape share three characteristics:\n",
      "\n",
      "They assume breach as the default state. Rather than preventing all intrusions, they focus on rapid detection and containment, understanding that perfect prevention is impossible.\n",
      "\n",
      "Rather than preventing all intrusions, they focus on rapid detection and containment, understanding that perfect prevention is impossible. They embrace continuous validation. Successful security programs operate in constant vigilance mode rather than periodic audit cycles.\n",
      "\n",
      "Successful security programs operate in constant vigilance mode rather than periodic audit cycles. They treat AI as both a tool and threat. The same technology that generates vulnerabilities can power defensive systems. Deploying AI-aware security to detect AI-generated threats has moved from experimental to essential.\n",
      "\n",
      "Your 2026 Security Readiness Checklist\n",
      "\n",
      "Security teams should prioritize these five validations:\n",
      "\n",
      "Inventory third-party dependencies – Map every external script, library, and API endpoint in production. Unknown code is an unmonitored risk. Implement behavioral monitoring – Deploy runtime detection that flags anomalous data flows, unauthorized API calls, and unexpected code execution. Audit AI-generated code – Treat all LLM-generated code as untrusted input. Require security review, secrets scanning, and penetration testing before deployment. Validate privacy controls in production – Test cookie consent, data collection boundaries, and third-party tracking in live environments, not just staging. Establish continuous validation – Move from quarterly audits to real-time monitoring with automated alerting.\n",
      "\n",
      "The question isn't whether to adopt these security paradigms but how quickly organizations can implement them. The threats that reshaped web security in 2025 aren't temporary disruptions – they're the foundation for years to come.\n",
      "\n",
      "The organizations that act now will define the security standards; those that hesitate will scramble to catch up.\n",
      "\n",
      "Title: GoldFactory Hits Southeast Asia with Modified Banking Apps Driving 11,000+ Infections\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: Cybercriminals associated with a financially motivated group known as GoldFactory have been observed staging a fresh round of attacks targeting mobile users in Indonesia, Thailand, and Vietnam by impersonating government services.\n",
      "\n",
      "The activity, observed since October 2024, involves distributing modified banking applications that act as a conduit for Android malware, Group-IB said in a technical report published Wednesday.\n",
      "\n",
      "Assessed to be active as far back as June 2023, GoldFactory first gained attention early last year, when the Singapore-headquartered cybersecurity company detailed the threat actor's use of custom malware families like GoldPickaxe, GoldDigger, and GoldDiggerPlus targeting both Android and iOS devices.\n",
      "\n",
      "Evidence points to GoldFactory being a well-organized Chinese-speaking cybercrime group with close connections to Gigabud, another Android malware that was spotted in mid-2023. Despite major disparities in their codebases, both GoldDigger and Gigabud have been found to share similarities in their impersonation targets and landing pages.\n",
      "\n",
      "The first cases in the latest attack wave were detected in Thailand, with the threat subsequently appearing in Vietnam by late 2024 and early 2025 and in Indonesia from mid-2025 onwards.\n",
      "\n",
      "Group-IB said it has identified more than 300 unique samples of modified banking applications that have led to almost 2,200 infections in Indonesia. Further investigation has uncovered over 3,000 artifacts that it said led to no less than 11,000 infections. About 63% of the altered banking apps cater to the Indonesian market.\n",
      "\n",
      "The infection chains, in a nutshell, involve the impersonation of government entities and trusted local brands and approaching prospective targets over the phone to trick them into installing malware by instructing them to click on a link sent on messaging apps like Zalo.\n",
      "\n",
      "In at least one case documented by Group-IB, fraudsters posed as Vietnam's public power company EVN and urged victims to pay overdue electricity bills or risk facing immediate suspension of the service. During the call, the threat actors are said to have asked the victims to add them on Zalo so as to receive a link to download an app and link their accounts.\n",
      "\n",
      "The links redirect the victims to fake landing pages that masquerade as Google Play Store app listings, resulting in the deployment of a remote access trojan like Gigabud, MMRat, or Remo, which surfaced earlier this year using the same tactics as GoldFactory. These droppers then pave the way for the main payload that abuses Android's accessibility services to facilitate remote control.\n",
      "\n",
      "\"The malware [...] is based on the original mobile banking applications,\" researchers Andrey Polovinkin, Sharmine Low, Ha Thi Thu Nguyen, and Pavel Naumov said. \"It operates by injecting malicious code into only a portion of the application, allowing the original application to retain its normal functionality. The functionality of injected malicious modules can differ from one target to another, but mainly it bypasses the original application's security features.\"\n",
      "\n",
      "Specifically, it works by hooking into the application's logic to execute the malware. Three different malware families have been discovered based on the frameworks used in the modified applications to perform runtime hooking: FriHook, SkyHook, and PineHook. Regardless of these differences, the functionality of the modules overlaps, making it possible to -\n",
      "\n",
      "Hide the list of applications that have accessibility services enabled\n",
      "\n",
      "Prevent screencast detection\n",
      "\n",
      "Spoof the signature of an Android application\n",
      "\n",
      "Hide the installation source\n",
      "\n",
      "Implement custom integrity token providers, and\n",
      "\n",
      "Obtain the victims' balance account\n",
      "\n",
      "While SkyHook makes use of the publicly available Dobby framework to execute the hooks, FriHook employs a Frida gadget that's injected into the legitimate banking application. PineHook, as the name implies, utilizes a Java-based hooking framework called Pine.\n",
      "\n",
      "Group-IB said its analysis of the malicious infrastructure erected by GoldFactory also uncovered a pre-release testing build of a new Android malware variant dubbed Gigaflower that's likely a successor to the Gigabud malware.\n",
      "\n",
      "It supports around 48 commands to enable real-time screen and device activity streaming using WebRTC; weaponize accessibility services for keylogging, reading user interface content, and performing gestures; serve fake screens to mimic system updates, PIN prompts, and account registration to harvest personal information, and extract data from images associated with identification cards using a built-in text recognition algorithm.\n",
      "\n",
      "Also currently in the works is a QR code scanner feature that attempts to read the QR code on Vietnamese identity cards, likely with the goal of simplifying the process of capturing the details.\n",
      "\n",
      "Interestingly, GoldFactory appears to have ditched its bespoke iOS trojan in favor of an unusual approach that now instructs victims to borrow an Android device from a family member or relative to continue the process. It's currently not clear what prompted the shift, but it's believed that it's due to stricter security measures and app store moderation on iOS.\n",
      "\n",
      "\"While earlier campaigns focused on exploiting KYC processes, recent activity shows direct patching of legitimate banking applications to commit fraud,\" the researchers said. \"The use of legitimate frameworks such as Frida, Dobby, and Pine to modify trusted banking applications demonstrates a sophisticated yet low-cost approach that allows cybercriminals to bypass traditional detection and rapidly scale their operation.\"\n",
      "\n",
      "Title: Record 29.7 Tbps DDoS Attack Linked to AISURU Botnet with up to 4 Million Infected Hosts\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: Cloudflare on Wednesday said it detected and mitigated the largest ever distributed denial-of-service (DDoS) attack that measured at 29.7 terabits per second (Tbps).\n",
      "\n",
      "The activity, the web infrastructure and security company said, originated from a DDoS botnet-for-hire known as AISURU, which has been linked to a number of hyper-volumetric DDoS attacks over the past year. The attack lasted for 69 seconds. It did not disclose the target of the attack.\n",
      "\n",
      "The botnet has prominently targeted telecommunication providers, gaming companies, hosting providers, and financial services. Also tackled by Cloudflare was a 14.1 Bpps DDoS attack from the same botnet. AISURU is believed to be powered by a massive network comprising an estimated 1-4 million infected hosts worldwide.\n",
      "\n",
      "\"The 29.7 Tbps was a UDP carpet-bombing attack bombarding an average of 15,000 destination ports per second,\" Omer Yoachimik and Jorge Pacheco said. \"The distributed attack randomized various packet attributes in an attempt to evade defenses.\"\n",
      "\n",
      "In all, Cloudflare has mitigated 2,867 Aisuru attacks since the start of the year, out of which 1,304 hyper-volumetric attacks were launched from the botnet in the third quarter of 2025 alone. A total of 8.3 million DDoS attacks were blocked during the entire time period, a figure that represents a 15% increase from the previous quarter and a 40% jump from last year.\n",
      "\n",
      "As many as 36.2 million DDoS attacks were thwarted in 2025, of which 1,304 were network-layer attacks exceeding 1 Tbps, up from 717 in Q1 2025 and 846 in Q2 2025. Some of the other notable trends observed in Q3 2025 are listed below -\n",
      "\n",
      "The number of DDoS attacks that exceeded 100 million packets per second (Mpps) increased by 189% QoQ.\n",
      "\n",
      "Most attacks, 71% of HTTP DDoS and 89% of network layer, lasted less than 10 minutes.\n",
      "\n",
      "Seven out of the 10 top sources of DDoS were locations within Asia, including Indonesia, Thailand, Bangladesh, Vietnam, India, Hong Kong, and Singapore. The other three sources are Ecuador, Russia, and Ukraine.\n",
      "\n",
      "DDoS attacks against the mining, minerals, and metals industry surged, making it the 49th most attacked sector globally.\n",
      "\n",
      "The automotive industry saw the largest increase in DDoS attacks, placing it as the sixth most attacked sector globally.\n",
      "\n",
      "DDoS attack traffic against artificial intelligence (AI) companies spiked by 347% in September 2025\n",
      "\n",
      "Information technology, telecommunications, gambling, gaming, and internet services topped the list of most attacked sectors.\n",
      "\n",
      "China, Turkey, Germany, Brazil, the U.S., Russia, Vietnam, Canada, South Korea, and the Philippines were the most attacked countries.\n",
      "\n",
      "Nearly 70% of HTTP DDoS attacks originated from known botnets.\n",
      "\n",
      "\"We've entered an era where DDoS attacks have rapidly grown in sophistication and size — beyond anything we could've imagined a few years ago,\" Cloudflare said. \"Many organizations have faced challenges in keeping pace with this evolving threat landscape.\"\n",
      "\n",
      "Title: Critical RSC Bugs in React and Next.js Allow Unauthenticated Remote Code Execution\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: A maximum-severity security flaw has been disclosed in React Server Components (RSC) that, if successfully exploited, could result in remote code execution.\n",
      "\n",
      "The vulnerability, tracked as CVE-2025-55182, carries a CVSS score of 10.0. The vulnerability has been codenamed React2shell.\n",
      "\n",
      "It allows \"unauthenticated remote code execution by exploiting a flaw in how React decodes payloads sent to React Server Function endpoints,\" the React Team said in an alert issued today.\n",
      "\n",
      "\"Even if your app does not implement any React Server Function endpoints, it may still be vulnerable if your app supports React Server Components.\"\n",
      "\n",
      "According to cloud security firm Wiz, the issue is a case of logical deserialization that stems from processing RSC payloads in an unsafe manner. As a result, an unauthenticated attacker could craft a malicious HTTP request to any Server Function endpoint that, when deserialized by React, achieves execution of arbitrary JavaScript code on the server.\n",
      "\n",
      "\"The issue stems from unsafe handling of serialized payloads in the React Flight protocol,\" software supply chain security company Aikido said. \"Malformed or adversarial payloads can influence server-side execution in unintended ways. Patched React versions include stricter validation and hardened deserialization behavior.\"\n",
      "\n",
      "The vulnerability impacts versions 19.0, 19.1.0, 19.1.1, and 19.2.0 of the following npm packages -\n",
      "\n",
      "react-server-dom-webpack\n",
      "\n",
      "react-server-dom-parcel\n",
      "\n",
      "react-server-dom-turbopack\n",
      "\n",
      "It has been addressed in versions 19.0.1, 19.1.2, and 19.2.1. New Zealand-based security researcher Lachlan Davidson has been credited with discovering and reporting the flaw to Meta on November 29, 2025. The social media giant originally created and maintained the JavaScript library before moving it to the React Foundation in October 2025.\n",
      "\n",
      "It's worth noting that the vulnerability also affects Next.js using App Router. The issue has been assigned the CVE identifier CVE-2025-66478 (CVSS score: 10.0). It impacts versions >=14.3.0-canary.77, >=15, and >=16. Patched versions are 16.0.7, 15.5.7, 15.4.8, 15.3.6, 15.2.6, 15.1.9, and 15.0.5.\n",
      "\n",
      "That said, any library that bundles RSC is likely to be affected by the flaw. This includes, but is not limited to, Vite RSC plugin, Parcel RSC plugin, React Router RSC preview, RedwoodJS, and Waku.\n",
      "\n",
      "Endor Labs, Miggo Security, and VulnCheck have all emphasized that no special setup is required to weaponize the flaw, adding that it's exploitable both without requiring a login and over HTTP.\n",
      "\n",
      "\"An attacker needs only network access to send a crafted HTTP request to any Server Function endpoint,\" Endor Labs said. \"The vulnerability affects default framework configurations, meaning standard deployments are immediately exploitable without special conditions.\"\n",
      "\n",
      "Until patches can be applied, it's recommended to deploy Web Application Firewall (WAF) rules if available, monitor HTTP traffic to Server Function endpoints for any suspicious or malformed request, and consider temporarily restricting network access to affected applications.\n",
      "\n",
      "Web infrastructure provider Cloudflare said it has deployed a new safeguard in its cloud-based WAF solution to address CVE-2025-55182. It noted that all customers on free and paid plans are protected \"as long as their React application traffic is proxied\" through the service.\n",
      "\n",
      "Wiz said 39% of cloud environments have instances vulnerable to CVE-2025-55182 and/or CVE-2025-66478. In light of the severity of the vulnerability, it's advised that users apply the fixes as soon as possible for optimal protection.\n",
      "\n",
      "Justin Moore, senior manager of threat intel research at Palo Alto Networks Unit 42, said more than 968,000 servers running modern frameworks like React and Next.js have been identified, exposing a lucrative attack surface that's ripe for exploitation.\n",
      "\n",
      "\"This newly discovered flaw is a critical threat because it is a master key exploit, succeeding not by crashing the system, but by abusing its trust in incoming data structures,\" Moore said. \"The system executes the malicious payload with the same reliability as legitimate code because it operates exactly as intended, but on malicious input.\"\n",
      "\n",
      "(The story was updated after publication to include additional insights.)\n",
      "\n",
      "Title: Discover the AI Tools Fueling the Next Cybercrime Wave — Watch the Webinar\n",
      "Author: ['The Hacker News', 'Dec']\n",
      "Publish Date: None\n",
      "Content: Remember when phishing emails were easy to spot? Bad grammar, weird formatting, and requests from a \"Prince\" in a distant country?\n",
      "\n",
      "Those days are over.\n",
      "\n",
      "Today, a 16-year-old with zero coding skills and a $200 allowance can launch a campaign that rivals state-sponsored hackers. They don't need to be smart; they just need to subscribe to the right AI tool.\n",
      "\n",
      "We are witnessing the industrialization of cybercrime. The barrier to entry has collapsed, and your current email filters are looking for threats that no longer exist.\n",
      "\n",
      "Watch the Live Breakdown of AI Phishing Tools ➜\n",
      "\n",
      "The New \"Big Three\" of Cybercrime\n",
      "\n",
      "Security leaders don't need another lecture on what phishing is. You need to see exactly what you are up against. This isn't science fiction—these tools are being sold on the dark web right now.\n",
      "\n",
      "In this webinar, we are going inside the \"AI Phishing Factory\" to deconstruct the three tools rewriting the threat landscape:\n",
      "\n",
      "WormGPT: Think of ChatGPT, but without the \"ethical guardrails.\" It doesn't have a conscience. It writes flawless, highly personalized Business Email Compromise (BEC) messages that sound exactly like your CEO—no typos, perfect tone.\n",
      "\n",
      "Think of ChatGPT, but without the \"ethical guardrails.\" It doesn't have a conscience. It writes flawless, highly personalized Business Email Compromise (BEC) messages that sound exactly like your CEO—no typos, perfect tone. FraudGPT: The \"Netflix\" of hacking. For a low monthly subscription, attackers get a full suite of tools to write malicious code, create scam landing pages, and draft emails. It is hacking-as-a-service.\n",
      "\n",
      "The \"Netflix\" of hacking. For a low monthly subscription, attackers get a full suite of tools to write malicious code, create scam landing pages, and draft emails. It is hacking-as-a-service. SpamGPT: This acts like a high-end marketing automation tool, but for criminals. It allows attackers to A/B test their scams and deliver them at a volume that overwhelms standard detection limits.\n",
      "\n",
      "Here is the hard truth: You cannot train your employees fast enough to outsmart a machine that learns instantly. If an email is written by AI to be indistinguishable from a legitimate sender, someone will click. It is a statistical certainty.\n",
      "\n",
      "Most defensive strategies focus on detection—trying to spot the bad email. But when the AI changes the emails' signature every second, detection fails.\n",
      "\n",
      "Register for the Webinar ➜\n",
      "\n",
      "Stop the Damage, Not Just the Email\n",
      "\n",
      "This session isn't about scaring you with the problem; it's about fixing it.\n",
      "\n",
      "Since we know users will eventually click, we have to change the strategy. We need to make the click irrelevant. We need to ensure that even if they land on the phishing page, the attacker gets nothing.\n",
      "\n",
      "Join us to learn how to:\n",
      "\n",
      "Identify the specific signatures of WormGPT and FraudGPT attacks. Shift your defense strategy from \"blocking emails\" to \"protecting identity.\" Neutralize the attack at the point of access by removing the one thing hackers want: the credentials.\n",
      "\n",
      "The bad guys are using AI to scale their attacks. You need to use intelligence to scale your defense.\n",
      "\n",
      "Secure Your Seat Now ➜\n",
      "\n",
      "Don't wait for the quarterly report to find out you were vulnerable. Get the strategy you need to shut this down now.\n",
      "\n",
      "Title: Microsoft Silently Patches Windows LNK Flaw After Years of Active Exploitation\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: Microsoft has silently plugged a security flaw that has been exploited by several threat actors since 2017 as part of the company's November 2025 Patch Tuesday updates, according to ACROS Security's 0patch.\n",
      "\n",
      "The vulnerability in question is CVE-2025-9491 (CVSS score: 7.8/7.0), which has been described as a Windows Shortcut (LNK) file UI misinterpretation vulnerability that could lead to remote code execution.\n",
      "\n",
      "\"The specific flaw exists within the handling of .LNK files,\" according to a description in the NIST National Vulnerability Database (NVD). \"Crafted data in an .LNK file can cause hazardous content in the file to be invisible to a user who inspects the file via the Windows-provided user interface. An attacker can leverage this vulnerability to execute code in the context of the current user.\"\n",
      "\n",
      "In other words, these shortcut files are crafted such that viewing their properties in Windows conceals the malicious commands executed by them out of the user's sight by using various \"whitespace\" characters. To trigger their execution, attackers could disguise the files as harmless documents.\n",
      "\n",
      "Details of the shortcoming first emerged in March 2025, when Trend Micro's Zero Day Initiative (ZDI) disclosed that the issue had been exploited by 11 state-sponsored groups from China, Iran, North Korea, and Russia as part of data theft, espionage, and financially motivated campaigns, some of which date back to 2017. The issue is also tracked as ZDI-CAN-25373.\n",
      "\n",
      "At that time, Microsoft told The Hacker News that the flaw does not meet the bar for immediate servicing and that it will consider fixing it in a future release. It also pointed out that the LNK file format is blocked across Outlook, Word, Excel, PowerPoint, and OneNote, as a result of which any attempt to open such files will trigger a warning to users not to open files from unknown sources.\n",
      "\n",
      "Subsequently, a report from HarfangLab found that the shortcoming was abused by a cyber espionage cluster known as XDSpy to distribute a Go-based malware called XDigo as part of attacks targeting Eastern European governmental entities, the same month the flaw was publicly disclosed.\n",
      "\n",
      "Then, in late October 2025, the issue reared up a third time after Arctic Wolf flagged an offensive campaign in which China-affiliated threat actors weaponized the flaw in attacks aimed at European diplomatic and government entities and delivered the PlugX malware.\n",
      "\n",
      "This development prompted Microsoft to issue a formal guidance on CVE-2025-9491, reiterating its decision not to patch it and emphasizing that it does not consider it a vulnerability \"due to the user interaction involved and the fact that the system already warns users that this format is untrusted.\"\n",
      "\n",
      "0patch said the vulnerability is not just about hiding the malicious part of the command out of the Target field, but the fact that a LNK file \"allows the Target arguments to be a very long string (tens of thousands of characters), but the Properties dialog only shows the first 260 characters, silently cutting off the rest.\"\n",
      "\n",
      "This also means that a bad actor can create an LNK file that can run a long command, which would cause only the first 260 characters of it to be displayed to the user who viewed its properties. The rest of the command string is simply truncated. According to Microsoft, the file's structure theoretically allows for strings of up to 32k characters.\n",
      "\n",
      "The silent patch released by Microsoft addresses the problem by showing in the Properties dialog the entire Target command with arguments, no matter its length. That said, this behavior hinges on the possibility that there can exist shortcut files with more than 260 characters in their Target field.\n",
      "\n",
      "0patch's micropatch for the same flaw takes a different route by displaying a warning when users attempt to open an LNK file with command-line arguments over 260 characters by padding the Target field.\n",
      "\n",
      "\"Even though malicious shortcuts could be constructed with fewer than 260 characters, we believe disrupting actual attacks detected in the wild can make a big difference for those targeted,\" it said.\n",
      "\n",
      "When reached for comment, a Microsoft spokesperson did not directly confirm the release of a patch, but passed along the tech giant's security guidance that states the company is \"continuously rolling out product and UI enhancements to help keep customers protected and improve the experience.\"\n",
      "\n",
      "\"As a security best practice, Microsoft encourages customers to exercise caution when downloading files from unknown sources as indicated in security warnings, which have been designed to recognize and warn users about potentially harmful files,\" the spokesperson added.\n",
      "\n",
      "(The story was updated after publication to include a response from Microsoft.)\n",
      "\n",
      "Title: WordPress King Addons Flaw Under Active Attack Lets Hackers Make Admin Accounts\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: A critical security flaw impacting a WordPress plugin known as King Addons for Elementor has come under active exploitation in the wild.\n",
      "\n",
      "The vulnerability, CVE-2025-8489 (CVSS score: 9.8), is a case of privilege escalation that allows unauthenticated attackers to grant themselves administrative privileges by simply specifying the administrator user role during registration.\n",
      "\n",
      "It affects versions from 24.12.92 through 51.1.14. It was patched by the maintainers in version 51.1.35 released on September 25, 2025. Security researcher Peter Thaleikis has been credited with discovering and reporting the flaw. The plugin has over 10,000 active installs.\n",
      "\n",
      "\"This is due to the plugin not properly restricting the roles that users can register with,\" Wordfence said in an alert. \"This makes it possible for unauthenticated attackers to register with administrator-level user accounts.\"\n",
      "\n",
      "Specifically, the issue is rooted in the \"handle_register_ajax()\" function that's invoked during user registration. But an insecure implementation of the function meant that unauthenticated attackers can specify their role as \"administrator\" in a crafted HTTP request to the \"/wp-admin/admin-ajax.php\" endpoint, allowing them to obtain elevated privileges.\n",
      "\n",
      "Successful exploitation of the vulnerability could enable a bad actor to seize control of a susceptible site that has installed the plugin, and weaponize the access to upload malicious code that can deliver malware, redirect site visitors to sketchy sites, or inject spam.\n",
      "\n",
      "Wordfence said it has blocked over 48,400 exploit attempts since the flaw was publicly disclosed in late October 2025, with 75 attempts thwarted in the last 24 hours alone. The attacks have originated from the following IP addresses -\n",
      "\n",
      "45.61.157.120\n",
      "\n",
      "182.8.226.228\n",
      "\n",
      "138.199.21.230\n",
      "\n",
      "206.238.221.25\n",
      "\n",
      "2602:fa59:3:424::1\n",
      "\n",
      "\"Attackers may have started actively targeting this vulnerability as early as October 31, 2025, with mass exploitation starting on November 9, 2025,\" the WordPress security company said.\n",
      "\n",
      "Site administrators are advised to ensure that they are running the latest version of the plugin, audit their environments for any suspicious admin users, and monitor for any signs of abnormal activity.\n",
      "\n",
      "Title: Brazil Hit by Banking Trojan Spread via WhatsApp Worm and RelayNFC NFC Relay Fraud\n",
      "Author: ['The Hacker News', 'Dec', 'Ravie Lakshmanan']\n",
      "Publish Date: None\n",
      "Content: The threat actor known as Water Saci is actively evolving its tactics, switching to a sophisticated, highly layered infection chain that uses HTML Application (HTA) files and PDFs to propagate via WhatsApp a worm that deploys a banking trojan in attacks targeting users in Brazil.\n",
      "\n",
      "The latest wave is characterized by the attackers shifting from PowerShell to a Python-based variant that spreads the malware in a worm-like manner over WhatsApp Web.\n",
      "\n",
      "\"Their new multi-format attack chain and possible use of artificial intelligence (AI) to convert propagation scripts from PowerShell to Python exemplifies a layered approach that has enabled Water Saci to bypass conventional security controls, exploit user trust across multiple channels, and ramp up their infection rates,\" Trend Micro researchers Jeffrey Francis Bonaobra, Sarah Pearl Camiling, Joe Soares, Byron Gelera, Ian Kenefick, and Emmanuel Panopio said.\n",
      "\n",
      "In these attacks, users receive messages from trusted contacts on WhatsApp, urging them to interact with malicious PDF or HTA attachments and activate the infection chain and ultimately drop a banking trojan that can harvest sensitive data. The PDF lure instructs victims to update Adobe Reader by clicking on an embedded link.\n",
      "\n",
      "Users who receive HTA files are deceived into executing a Visual Basic Script immediately upon opening, which then runs PowerShell commands to fetch next-stage payloads from a remote server, an MSI installer for the trojan and a Python script that's responsible for spreading the malware via WhatsApp Web.\n",
      "\n",
      "\"This newly observed variant allows for broader browser compatibility, object-oriented code structure, enhanced error handling, and faster automation of malware delivery through WhatsApp Web,\" Trend Micro said. \"Together, these changes make propagation faster, more resilient to failure, and easier to maintain or extend.\"\n",
      "\n",
      "The MSI installer, for its part, serves as a conduit for delivering the banking trojan using an AutoIt script. The script also runs checks to ensure that only one instance of the trojan is running at any given point of time. It accomplishes this by verifying the presence of a marker file named \"executed.dat.\" If it does not exist, the script creates the file and notifies an attacker-controlled server (\"manoelimoveiscaioba[.]com\").\n",
      "\n",
      "Other AutoIt artifacts uncovered by Trend Micro have also been found to verify whether the Windows system language is set to Portuguese (Brazil), proceeding further to scan the infected system for banking-related activity only if this criteria is met. This includes checking for folders related to major Brazilian banking applications, security, and anti-fraud modules, such as Bradesco, Warsaw, Topaz OFD, Sicoob, and Itaú.\n",
      "\n",
      "It's worth noting Latin America (LATAM)-focused banking trojans like Casbaneiro (aka Metamorfo and Ponteiro) have incorporated similar features as far back as 2019. Furthermore, the script analyzes the user's Google Chrome browsing history to search visits to banking websites, specifically a hard-coded list comprising Santander, Banco do Brasil, Caixa Econômica Federal, Sicredi, and Bradesco.\n",
      "\n",
      "The script then proceeds to another critical reconnaissance step that involves checking for installed antivirus and security software, as well as harvesting detailed system metadata. The main functionality of the malware is to monitor open windows and extract their window titles to compare them against a list of banks, payment platforms, exchanges, and cryptocurrency wallets.\n",
      "\n",
      "If any of these windows contain keywords related to targeted entities, the script looks for a TDA file dropped by the installer and decrypts and injects it into a hollowed \"svchost.exe\" process, following which the loader searches for an additional DMP file containing the banking trojan.\n",
      "\n",
      "\"If a TDA file is present, the AutoIt script decrypts and loads it as an intermediate PE loader (Stage 2) into memory,\" Trend Micro explained. \"However, if only a DMP file is found (no TDA present), the AutoIt script bypasses the intermediate loader entirely and loads the banking trojan directly into the AutoIt process memory, skipping the process hollowing step and running as a simpler two-stage infection.\"\n",
      "\n",
      "Persistence is achieved by constantly keeping tabs on the newly spawned \"svchost.exe\" process. Should the process be terminated, the malware starts afresh and waits to re-inject the payload the next time the victim opens a browser window for a financial service that's targeted by Water Saci.\n",
      "\n",
      "The attacks stand out for a major tactical shift. The banking trojan deployed is not Maverick, but rather a malware that exhibits structural and behavioral continuity with Casbaneiro. This assessment is based on the AutoIt-based delivery and loader mechanism employed, as well as the window title monitoring, Registry-based persistence, and IMAP-based fallback command-and-control (C2 or C&C) mechanism.\n",
      "\n",
      "Once launched, the trojan carries out \"aggressive\" anti-virtualization checks to sidestep analysis and detection, and gathers host information through Windows Management Instrumentation (WMI) queries. It makes Registry modifications to set up persistence and establishes contact with a C2 server (\"serverseistemasatu[.]com\") to send the collected details and receive backdoor commands that grant remote control over the infected system.\n",
      "\n",
      "Besides scanning the titles of active windows to identify whether the user is interacting with banking or cryptocurrency platforms, the trojan forcibly terminates several browsers to force victims to reopen banking sites under \"attacker-controlled conditions.\" Some of the supported features of the trojan are listed below -\n",
      "\n",
      "Send system information\n",
      "\n",
      "Enable keyboard capture\n",
      "\n",
      "Start/stop screen capture\n",
      "\n",
      "Modify screen resolution\n",
      "\n",
      "Simulate mouse movements and clicks\n",
      "\n",
      "Perform file operations\n",
      "\n",
      "Upload/download files\n",
      "\n",
      "Enumerate windows, and\n",
      "\n",
      "Create fake banking overlays to capture credentials and transaction data\n",
      "\n",
      "The second aspect of the campaign is the use of a Python script, an enhanced version of its PowerShell predecessor, to enable malware delivery to every contact via WhatsApp Web sessions using the Selenium browser automation tool.\n",
      "\n",
      "There is \"compelling\" evidence to suggest that Water Saci may have used a large language model (LLMs) or code-translation tool to port their propagation script from PowerShell to Python, given the functional similarities between the two versions and the inclusion of emojis in console outputs.\n",
      "\n",
      "\"The Water Saci campaign exemplifies a new era of cyber threats in Brazil, where attackers exploit the trust and reach of popular messaging platforms like WhatsApp to orchestrate large-scale, self-propagating malware campaigns,\" Trend Micro said.\n",
      "\n",
      "\"By weaponizing familiar communication channels and employing advanced social engineering, threat actors are able to swiftly compromise victims, bypass traditional defenses, and sustain persistent banking trojan infections. This campaign demonstrates how legitimate platforms can be transformed into powerful vectors for malware delivery and underscores the growing sophistication of cybercriminal operations in the region.\"\n",
      "\n",
      "Brazil Targeted by New RelayNFC Android Malware\n",
      "\n",
      "The development comes as Brazilian banking users are also being targeted by a previously undocumented Android malware dubbed RelayNFC that's designed to carry out Near-Field Communication (NFC) relay attacks and siphon contactless payment data. The campaign has been running since early November 2025.\n",
      "\n",
      "\"RelayNFC implements a full real-time APDU relay channel, allowing attackers to complete transactions as though the victim's card were physically present,\" Cyble said in an analysis. \"The malware is built using React Native and Hermes bytecode, which complicates static analysis and helps evade detection.\"\n",
      "\n",
      "Primarily spread via phishing, the attack makes use of decoy Portuguese-language sites (e.g., \"maisseguraca[.]site\") to trick users into installing the malware under the pretext of securing their payment cards. The end goal of the campaign is to capture the victim's card details and relay them to attackers, who can then perform fraudulent transactions using the stolen data.\n",
      "\n",
      "Like other NFC relay malware families such as SuperCard X and PhantomCard, RelayNFC operates as a reader that's designed to gather the card data by instructing the victim to tap their payment card on the device. Once the card data is read, the malware displays a message that prompts them to enter their 4- or 6-digit PIN. The captured information is then sent to the attacker's server through a WebSocket connection.\n",
      "\n",
      "\"When the attacker initiates a transaction from their POS-emulator device, the C&C server sends a specially crafted message of type 'apdu' to the infected phone,\" Cyble said. \"This message contains a unique request ID, a session identifier, and the APDU command encoded as a hexadecimal string.\"\n",
      "\n",
      "\"Upon receiving this instruction, RelayNFC parses the packet, extracts the APDU data, and forwards it directly to the victim device's NFC subsystem, effectively acting as a remote interface to the physical payment card.\"\n",
      "\n",
      "The cybersecurity company said its investigation also uncovered a separate phishing site (\"test.ikotech[.]online\") that distributes an APK file with a partial implementation of Host Card Emulation (HCE), indicating that the threat actors are experimenting with different NFC relay techniques.\n",
      "\n",
      "Because HCE allows an Android device to emulate a payment card, the mechanism permits a victim's card interactions to be transmitted between a legitimate payment-of-sale (PoS) terminal and an attacker-controlled device, thereby facilitating a real-time NFC relay attack. The feature is assessed to be under development, as the APK file does not register the HCE service in the package manifest file.\n",
      "\n",
      "\"The RelayNFC campaign highlights the rapid evolution of NFC relay malware targeting payment systems, particularly in Brazil,\" the company said. \"By combining phishing-driven distribution, React Native-based obfuscation, and real-time APDU relaying over WebSockets, the threat actors have created a highly effective mechanism for remote EMV transaction fraud.\"\n",
      "\n",
      "Title: Microsoft 365 license check bug blocks desktop app downloads\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: ​Microsoft is investigating and working to resolve a known issue that prevents customers from downloading Microsoft 365 desktop apps from the Microsoft 365 homepage.\n",
      "\n",
      "As detailed in a Wednesday incident report (OP1192004) seen by BleepingComputer, this bug has been impacting users since November 2nd, causing Office Client issues for affected customers.\n",
      "\n",
      "Microsoft has already developed and is now testing a fix to address the issue and added that it will provide an update on progress by 6:30 PM UTC later today.\n",
      "\n",
      "While it noted that the bug may affect any user who attempts to download Microsoft 365 desktop apps, it has not yet provided more details on the extent of the problem.\n",
      "\n",
      "However, when it acknowledged the issue this morning, Microsoft tagged it as an incident, a designation used for critical service issues that usually involve noticeable user impact.\n",
      "\n",
      "\"Our analysis of the components of Microsoft 365 infrastructure, as well as recently deployed changes, identified that a recent service update containing a code issue is impacting the license check process, leading to users being unable to download Microsoft 365 desktop apps from the homepage,\" Microsoft said.\n",
      "\n",
      "\"We're continuing to validate and test the aforementioned fix in our internal environment to ensure its efficacy prior to deploying it to the affected infrastructure and we expect to provide an estimated deployment time line by our next scheduled update.\"\n",
      "\n",
      "Microsoft is also working to resolve a known issue that blocks some users from opening Excel email attachments in the new Outlook client due to an encoding error in Excel file names.\n",
      "\n",
      "One year ago, Microsoft addressed another known issue triggered by licensing changes that caused random \"Product Deactivated\" errors for customers using Microsoft 365 Office apps, while last month, it resolved a bug caused by misconfigured authentication components that prevented customers from installing the Microsoft 365 desktop apps on Windows devices.\n",
      "\n",
      "Title: Marquis data breach impacts over 74 US banks, credit unions\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Financial software provider Marquis Software Solutions is warning that it suffered a data breach that impacted dozens of banks and credit unions across the US.\n",
      "\n",
      "Marquis Software Solutions provides data analytics, CRM tools, compliance reporting, and digital marketing services to over 700 banks, credit unions, and mortgage lenders.\n",
      "\n",
      "In data breach notifications filed with US Attorney General offices, Marquis says it suffered a ransomware attack on August 14, 2025, after its network was breached through its SonicWall firewall.\n",
      "\n",
      "This allowed the hackers to steal \"certain files from its systems\" during the attack.\n",
      "\n",
      "\"The review determined that the files contained personal information received from certain business customers,\" reads a notification filed with Maine's AG office.\n",
      "\n",
      "\"The personal information potentially involved for Maine residents includes names, addresses, phone numbers, Social Security numbers, Taxpayer Identification Numbers, financial account information without security or access codes, and dates of birth.\"\n",
      "\n",
      "Marquis is now filing notifications on behalf of its customers, in some cases breaking down the number of people impacted per bank in a state. These notifications state that similar data was exposed in the attack for customers in other U.S. states.\n",
      "\n",
      "According to notifications filed in Maine, Iowa, and Texas, over 400,000 customers have been impacted from the following 74 banks and credit unions.\n",
      "\n",
      "1st Northern California Credit Union Abbott Laboratories Employees Credit Union Advantage Federal Credit Union Agriculture Federal Credit Union Alltrust Credit Union BayFirst National Bank Bellwether Community Credit Union C&N Bank Cape Cod Five Capital City Bank Group Central Virginia Federal Credit Union Clark County Credit Union Community 1st Credit Union Community Bancshares of Mississippi, Inc. Cornerstone Community Financial Credit Union CPM Federal Credit Union CSE Federal Credit Union CU Hawaii Federal Credit Union d/b/a Community Bank Discovery Federal Credit Union Earthmover Credit Union Educators Credit Union Energy Capital Credit Union Fidelity Cooperative Bank First Community Credit Union First Northern Bank of Dixon Florida Credit Union Fort Community Credit Union Founders Federal Credit Union Freedom of Maryland Federal Credit Union Gateway First Bank Generations Federal Credit Union Gesa Credit Union Glendale Federal Credit Union Hope Federal Credit Union IBERIABANK n/k/a First Horizon Bank Industrial Federal Credit Union Interior Federal Interior Federal Credit Union Interra Credit Union Jonestown Bank & Trust Co. Kemba Financial Credit Union Liberty First Credit Union Maine State Credit Union Market USA FCU MemberSource Credit Union Michigan First Credit Union MIT Federal Credit Union New Orleans Firemen's Federal Credit Union New Peoples Bank Newburyport Five Cents Savings Bank NIH Federal Credit Union Pasadena Federal Credit Union Pathways Financial Credit Union Peake Federal Credit Union Pelican Credit Union Pentucket Bank PFCU Credit Union QNB Bank Security Credit Union Seneca Savings ServU Credit Union StonehamBank Cooperative Suncoast Credit Union Texoma Community Credit Union Thomaston Savings Bank Time Bank TowneBank Ulster Savings Bank University Credit Union Valley Strong Credit Union Westerra Credit Union Whitefish Credit Union Zing Credit Union\n",
      "\n",
      "At this time, Marquis says that there is no evidence that data has been misused or published anywhere.\n",
      "\n",
      "However, as previously reported by Comparitech, a now-deleted filing by Community 1st credit union claimed that Marquis paid a ransomm, which is done to prevent the leaking and abuse of stolen data.\n",
      "\n",
      "\"Marquis paid a ransomware shortly after 08/14/25. On 10/27/25 C1st was notified that nonpublic personal information related to C1st members was included in the Marquis breach,\" reads the deleted notification seen by Comparitech.\n",
      "\n",
      "While the company's data breach notifications state only that it has \"taken steps to reduce the risk of this type of incident,\" a filing by CoVantage Credit Union with the New Hampshire AG shares further details about how the company is increasing security.\n",
      "\n",
      "This notification states that Marquis has now enhanced its security controls by doing the following:\n",
      "\n",
      "Ensuring that all firewall devices are fully patched and up to date,\n",
      "\n",
      "Rotating passwords for local accounts,\n",
      "\n",
      "Deleting old or unused accounts,\n",
      "\n",
      "Ensuring that multi-factor authentication is enabled for all firewall and virtual private network (\"VPN\") accounts,\n",
      "\n",
      "Increasing logging retention for firewall devices, (\n",
      "\n",
      "Applying account lock-out policies at the VPN for too many failed logins,\n",
      "\n",
      "Applying geo-IP filtering to only allow connections from specific countries needed for business operations, and\n",
      "\n",
      "Applying policies to automatically block connections to/from known Botnet Command and Control servers at the firewall.\n",
      "\n",
      "These steps indicate that the threat actors likely gained access to the company network through a SonicWall VPN account, a known tactic used by some ransomware gangs, especially Akira ransomware.\n",
      "\n",
      "Targeting SonicWall firewalls\n",
      "\n",
      "While Marquis has not shared any further details about the ransomware attack, the Akira ransomware gang has been targeting SonicWall firewalls to gain initial access to corporate networks since at least early September 2024.\n",
      "\n",
      "Akira started breaching SonicWall SSL VPN devices in 2024 by exploiting the CVE-2024-40766 vulnerability, which allowed attackers to steal VPN usernames, passwords, and seeds to generate one-time passcodes.\n",
      "\n",
      "Even after SonicWall patched the bug, many organizations didn't properly reset their VPN credentials, allowing Akira to continue breaching patched devices with previously stolen credentials.\n",
      "\n",
      "A recent report shows the group is still signing in to SonicWall VPN accounts even when MFA is enabled, suggesting the attackers stole OTP seeds during the earlier exploitation.\n",
      "\n",
      "Once Akira gets in through the VPN, they move quickly to scan the network, perform reconnaissance, gain elevated privileges in the Windows Active Directory, and steal data before deploying ransomware.\n",
      "\n",
      "Title: Critical flaw in WordPress add-on for Elementor exploited in attacks\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Attackers are exploiting a critical-severity privilege escalation vulnerability (CVE-2025–8489) in the King Addons for Elementor plugin for WordPress, which lets them obtain administrative permissions during the registration process.\n",
      "\n",
      "The threat activity started on October 31, just a day after the issue was publicly disclosed. So far, the Wordfence security scanner from Defiant, a company that provides security services for WordPress websites, has blocked more than 48,400 exploit attempts.\n",
      "\n",
      "King Addons is a third-party add-on for Elementor, a popular visual page builder plugin for WordPress sites. It is used on roughly 10,000 websites, providing additional widgets, templates, and features.\n",
      "\n",
      "CVE-2025–8489, discovered by researcher Peter Thaleikis, is a flaw in the plugin’s registration handler that allows anyone signing up to specify their user role on the website, including the administrator role, without enforcing any restrictions.\n",
      "\n",
      "According to observations from Wordfence, attackers send a crafted ‘admin-ajax.php’ request specifying ‘user_role=administrator,’ to create rogue admin accounts on targeted sites.\n",
      "\n",
      "Malicious request\n",
      "\n",
      "Source: Wordfence\n",
      "\n",
      "The researchers noticed a peak in the exploitation activity between November 9 and 10, with two IP addresses being the most active: 45.61.157.120 (28,900 attempts) and 2602:fa59:3:424::1 (16,900 attempts).\n",
      "\n",
      "Wordfence provides a more extensive list of offensive IP addresses and recommends that website administrators look for them in the log files. The presence of new administrator accounts is also a clear sign of compromise.\n",
      "\n",
      "Website owners are advised to upgrade to version 51.1.35 of King Addons, which addresses CVE-2025–8489, released on September 25.\n",
      "\n",
      "Wordfence researchers are also warning of another critical vulnerability in the Advanced Custom Fields: Extended plugin, active on more than 100,000 WordPress websites, which can be exploited by an unauthenticated attacker to execute code remotely.\n",
      "\n",
      "The flaw affects versions 0.9.0.5 through 0.9.1.1 of the plugin and is currently tracked as CVE-2025-13486. It was discovered and reported responsibly by Marcin Dudek, the head of the national computer emergency response team (CERT) in Poland.\n",
      "\n",
      "The vulnerability is \"due to the function accepting user input and then passing that through call_user_func_array(),” Wordfence explains.\n",
      "\n",
      "“This makes it possible for unauthenticated attackers to execute arbitrary code on the server, which can be leveraged to inject backdoors or create new administrative user accounts.”\n",
      "\n",
      "The security issue was reported on November 18, and the plugin vendor addressed it in version 0.9.2 of Advanced Custom Fields: Extended, released a day after receiving the vulnerability report.\n",
      "\n",
      "Given that the flaw can be leveraged without authentication only through a crafted request, the public disclosure of technical details is likely to generate malicious activity.\n",
      "\n",
      "Website owners are advised to move to the latest version as soon as possible or disable the plugin on their sites.\n",
      "\n",
      "Title: French DIY retail giant Leroy Merlin discloses a data breach\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: French home improvement and gardening retailer Leroy Merlin is notifying customers that their personal info has been compromised in a data breach.\n",
      "\n",
      "Leroy Merlin operates in multiple European countries as well as in South Africa and Brazil, employs 165,000 people, and has an annual revenue of $9.9 billion.\n",
      "\n",
      "The incident affects only customers in France, according to the notification published on social media by SaxX_, and exposed the following data types:\n",
      "\n",
      "Full name\n",
      "\n",
      "Phone number\n",
      "\n",
      "Email address\n",
      "\n",
      "Postal address\n",
      "\n",
      "Date of birth\n",
      "\n",
      "Loyalty program-related information\n",
      "\n",
      "“A cyberattack recently targeted our information system, and some of your personal data may have leaked outside the company” (machine translated), reads the notification the company sent to affected customers.\n",
      "\n",
      "“As soon as the incident was detected, we took all necessary measures to block unauthorized access and contain the incident.”\n",
      "\n",
      "Leroy Merlin's notice\n",
      "\n",
      "Source: @_SaxX_\n",
      "\n",
      "The company clarified that the exposed information does not include banking data or online account passwords.\n",
      "\n",
      "Also, the notice mentions that the stolen information has not been used in a malicious way, suggesting that it has not been leaked online or leveraged for extortion, but cautioned customers to remain vigilant of unsolicited communications.\n",
      "\n",
      "Customers receiving the notification are also provided with information on how to identify phishing messages attempting to impersonate the brand.\n",
      "\n",
      "If any anomaly is detected in customer account activity or trouble with redeeming loyalty discounts, customers are asked to report the activity directly to the company.\n",
      "\n",
      "BleepingComputer could confirm that the notification is genuine and has reached out to Leroy Merlin to request more details about the breach and how many customers are affected. We have not received a reply by publication time.\n",
      "\n",
      "At the time of writing, we did not see any ransomware group claiming the attack.\n",
      "\n",
      "Title: Freedom Mobile discloses data breach exposing customer data\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Freedom Mobile, the fourth-largest wireless carrier in Canada, has disclosed a data breach after attackers hacked into its customer account management platform and stole the personal information of an undisclosed number of customers.\n",
      "\n",
      "Founded in 2008 as Wind Mobile by telecommunications provider Globalive, Freedom has over 2,2 million subscribers and now says it provides coverage to 99% of Canadians.\n",
      "\n",
      "Vidéotron, a subsidiary of Canadian telecommunications company Québecor, acquired Freedom in 2023, creating the country's fourth major wireless carrier with more than 3.5 million mobile customers and nearly 7,500 employees.\n",
      "\n",
      "In a data breach notification published today, Freedom said it detected a breach of its customer account management platform on October 23.\n",
      "\n",
      "\"Our investigation revealed that a third party used the account of a subcontractor to gain access to the personal information of a limited number of our customers,\" Freedom stated.\n",
      "\n",
      "\"We quickly identified the incident and implemented corrective measures and security enhancements, including blocking the suspicious accounts and corresponding IP addresses.\"\n",
      "\n",
      "The personal and contact information exposed in the incident includes first and last names, home addresses, dates of birth, home and/or cell phone numbers, and Freedom Mobile account numbers.\n",
      "\n",
      "Although it found no evidence that the compromised data has been misused since the breach, the wireless carrier advised affected customers to be suspicious of unexpected messages requesting their personal information or directing them to a website to provide it.\n",
      "\n",
      "Freedom also recommends not clicking links or downloading attachments from emails or texts that seem suspicious and regularly checking their accounts for unusual activity.\n",
      "\n",
      "A Freedom Mobile spokesperson told BleepingComputer that the network and operations were not affected and that \"this is not a ransomware type of incident.\"\n",
      "\n",
      "While the spokesperson also said that the threat actors used the stolen account of a subcontractor to gain access to \"the personal information of a limited number of our customers,\" they didn't share how many customers were affected by the resulting data breach.\n",
      "\n",
      "In May 2019, Freedom Mobile confirmed another data breach after a third-party vendor exposed an unsecured customer support database online, which contained the data of approximately 15,000 customers.\n",
      "\n",
      "Update December 03, 16:27 EST: Added Freedom Mobile statement.\n",
      "\n",
      "Title: Russia blocks Roblox over distribution of LGBT \"propaganda\"\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Roskomnadzor, Russia's telecommunications watchdog, has blocked access to the Roblox online gaming platform for failing to stop the distribution of what it described as LGBT propaganda and extremist materials.\n",
      "\n",
      "\"Roskomnadzor has restricted access to the American Internet service Roblox in connection with the revealed facts of mass and repeated dissemination of materials with propaganda and justification of extremist and terrorist activities, calls for illegal actions of a violent nature and propaganda of LGBT topics,\" Russia's internet regulator said in a Wednesday press statement.\n",
      "\n",
      "\"Such actions are committed, including in the so-called rooms and platforms of the game, where users can become performers of terrorist actions, attack the school, and participate in gambling. The cases of such illegal behavior against children have long been of a non-alignance both in the United States and in Russia.\"\n",
      "\n",
      "The move follows a similar statement in November, when Roskomnadzor said that Roblox's moderation team had repeatedly confirmed it couldn't block the distribution of unsafe materials on the platform.\n",
      "\n",
      "Roblox can be played on both desktop and mobile platforms. As of this month, Roblox for Android has been downloaded over 1 billion times on the Google Play Store, while its iOS counterpart has over 16 million ratings on Apple's App Store.\n",
      "\n",
      "\"We respect the local laws and regulations in the countries where we operate and believe Roblox provides a positive space for learning, creation and meaningful connection for everyone,\" a Roblox spokesperson told BleepingComputer. \"We have a deep commitment to safety and we have a robust set of proactive and preventative safety measures designed to catch and prevent harmful content on our platform.”\n",
      "\n",
      "Russian news agency Interfax also reported on Friday that Roskomnadzor is planning to ban WhatsApp, a messaging platform used by over 3 billion people across more than 180 countries.\n",
      "\n",
      "One year ago, Russia blocked the Viber encrypted messaging app, used by hundreds of millions worldwide, for violating the country's anti-terrorism and anti-extremism legislation, months after banning the Signal encrypted messaging service for the same reason. ​\n",
      "\n",
      "In March 2023, Roskomnadzor also banned government and state agencies from using multiple foreign private messaging platforms, including Discord, Microsoft Teams, Telegram, Threema, Viber, WhatsApp, and WeChat.\n",
      "\n",
      "Multiple virtual private network (VPN) applications were also banned in Russia in three waves: January 2020, June 2021, and December 2021.\n",
      "\n",
      "A Roblox spokesperson was not immediately available for comment when contacted by BleepingComputer earlier today.\n",
      "\n",
      "Title: Google expands Android scam protection feature to Chase, Cash App in U.S.\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Google is expanding support for its Android's in-call scam protection to multiple banks and financial applications in the United States.\n",
      "\n",
      "The announcement specifically mentions the addition of fintech app Cash App, which has 57 million users, and the JPMorganChase mobile banking app, which has more than 50 million downloads on Google Play.\n",
      "\n",
      "In-call scam protection is a new feature that was announced in May and introduced in Android 16, Its purpose is to warn users of a potential danger when they launch a financial app and are sharing their screen while in a call with an unknown number.\n",
      "\n",
      "Google says that this security feature defends against a popular scam where cybercriminals are \"impersonating banks or other trusted institutions on the phone to try to manipulate victims into sharing their screen in order to reveal banking information or make a financial transfer.\"\n",
      "\n",
      "In this scenario, an alert is shown, informing the user that the caller may be an impersonator and that the instructions they convey should be ignored. The user is also advised not to share any information or make any payments.\n",
      "\n",
      "The warning pop-up persists for 30 seconds and the only option is to end the call. Google notes that the 30-second pause should break the attacker's social-engineering \"spell,\" and disrupt the false sense of urgency and panic that are required for the scam to be successful.\n",
      "\n",
      "The warning message\n",
      "\n",
      "Source: Google\n",
      "\n",
      "The in-call scam protection system only works on Android 11 and later and started as a trial in the U.K., where apps from most major banks are enrolled.\n",
      "\n",
      "After helping \"thousands of users end calls that could have cost them a significant amount of money,\" the company expanded the pilot with financial apps in Brazil and India.\n",
      "\n",
      "Today, the system expands to U.S., where users of several popular fintech and bank apps, among them CashApp and JPMorgan Chase, are supported. The protection system continues to run in testing phase.\n",
      "\n",
      "Users should be aware of risky actions required of them from unknown callers, such as installing APKs from unofficial sources, granting accessibility permissions to malware apps, and disabling Play Protect on the device.\n",
      "\n",
      "As part of good security practices, users should avoid sharing personal information with unknown callers and never jump into action before confirming the status of their accounts by contacting their bank directly.\n",
      "\n",
      "Title: Microsoft \"mitigates\" Windows LNK flaw exploited as zero-day\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Microsoft has silently \"mitigated\" a high-severity Windows LNK vulnerability exploited by multiple state-backed and cybercrime hacking groups in zero-day attacks.\n",
      "\n",
      "Tracked as CVE-2025-9491, this security flaw allows attackers to hide malicious commands within Windows LNK files, which can be used to deploy malware and gain persistence on compromised devices. However, the attacks require user interaction to succeed, as they involve tricking potential victims into opening malicious Windows Shell Link (.lnk) files.\n",
      "\n",
      "Threat actors distribute these files in ZIP or other archives because email platforms commonly block .lnk attachments due to their risky nature.\n",
      "\n",
      "The vulnerability lies in how Windows handles .LNK files, allowing threat actors to exploit the way the operating system displays them to evade detection and execute code on vulnerable devices without the user's knowledge by padding the Target field in Windows .LNK files with whitespaces to hide malicious command-line arguments.\n",
      "\n",
      "This ensures that the file's Target field properties display only the first 260 characters due to the added whitespaces, so users can't see the actual command executed when the LNK file is double-clicked.\n",
      "\n",
      "As Trend Micro threat analysts discovered in March 2025, the CVE-2025-9491 was already being widely exploited by 11 state-sponsored groups and cybercrime gangs, including Evil Corp, Bitter, APT37, APT43 (also known as Kimsuky), Mustang Panda, SideWinder, RedHotel, Konni, and others.\n",
      "\n",
      "​​\"Diverse malware payloads and loaders like Ursnif, Gh0st RAT, and Trickbot have been tracked in these campaigns, with malware-as-a-service (MaaS) platforms complicating the threat landscape,\" Trend Micro said.\n",
      "\n",
      "Arctic Wolf Labs also reported in October that the Chinese state-backed Mustang Panda hacking group was exploiting this Windows vulnerability in zero-day attacks targeting European diplomats in Hungary, Belgium, and other European nations to deploy the PlugX remote access trojan (RAT) malware.\n",
      "\n",
      "Malicious arguments not showing in the Target field (Trend Micro)\n",
      "\n",
      "Microsoft pushes silent \"patch\"\n",
      "\n",
      "​Microsoft told BleepingComputer in March that it would \"consider addressing\" this zero-day flaw, even though it didn't \"meet the bar for immediate servicing.\"\n",
      "\n",
      "It also added in a November advisory that it doesn't consider this a vulnerability \"due to the user interaction involved and the fact that the system already warns users that this format is untrusted,\" even though threat actors could still exploit a Mark of the Web bypass vulnerability to circumvent these warnings and ensure their attacks' success.\n",
      "\n",
      "Despite this, as ACROS Security CEO and 0patch co-founder Mitja Kolsek found, Microsoft has silently changed LNK files in the November updates in an apparent effort to mitigate the CVE-2025-9491 flaw. After installing last month's updates, users can now see all characters in the Target field when opening the Properties of LNK files, not just the first 260.\n",
      "\n",
      "However, this isn't necessarily a fix since malicious arguments added to LNK files will not be deleted, and the user receives no warning when opening LNK files with a Target string exceeding 260 characters\n",
      "\n",
      "A Microsoft spokesperson was not immediately available for comment when contacted by BleepingComputer earlier today to confirm if this change is an attempt to mitigate the vulnerability.\n",
      "\n",
      "Unofficial patches available\n",
      "\n",
      "Until Microsoft adequately addresses this security flaw, ACROS Security has released an unofficial patch via its 0Patch micropatch platform, which limits all shortcut target strings to 260 characters and warns users about the potential danger of opening shortcuts with unusually long target strings.\n",
      "\n",
      "\"Our patch would break the 1000+ malicious shortcuts identified by Trend Micro for all targeted users, while Microsoft's patch would only allow the most cautious among these users - who would probably not launch such shortcuts anyway - to see the entire malicious command string,\" Kolsek said.\n",
      "\n",
      "\"Even though malicious shortcuts could be constructed with fewer than 260 characters, we believe disrupting actual attacks detected in the wild can make a big difference for those targeted.\"\n",
      "\n",
      "ACROS Security's unofficial CVE-2025-9491 patch is available for 0patch users with PRO or Enterprise accounts who use Windows versions that have reached end of support (Windows 7 through Windows 11 22H2, and Windows Server 2008 R2 through Windows Server 2022).\n",
      "\n",
      "Title: Deep dive into DragonForce ransomware and its Scattered Spider connection\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: Security researchers have conducted an in-depth analysis of DragonForce ransomware that initially emerged in 2023 and has since evolved into what it calls a \"ransomware cartel.\"\n",
      "\n",
      "The most recent variant exploits susceptible drivers such as truesight.sys and rentdrv2.sys to deactivate security programs, shut down protected processes and fix encryption vulnerabilities that were earlier linked to Akira ransomware.\n",
      "\n",
      "The updated encryption scheme addresses vulnerabilities that were openly documented in a Habr publication referenced on DragonForce's leak website.\n",
      "\n",
      "DragonForce has intensified its operations against organizations worldwide, publishing details of more compromised entities than in the previous year.\n",
      "\n",
      "The group's most prominent breach, involving retail company Marks & Spencer, was carried out in partnership with the cybercriminal collective Scattered Spider hacking group.\n",
      "\n",
      "The emergence of DragonForce\n",
      "\n",
      "DragonForce operates as a ransomware-as-a-service (RaaS) operation. The group reignited ransomware activities, and has been actively recruiting nefarious collaborators through underground cybercrime platforms.\n",
      "\n",
      "At the start, the gang used the compromised LockBit 3.0 builder to create its encryption tools and later transitioned to a modified version of Conti v3 source code.\n",
      "\n",
      "Transforming from ransomware group to “cartel”\n",
      "\n",
      "Returning in 2025, DragonForce rebranded itself as a “ransomware cartel,” marking a sudden shift in operational strategy.\n",
      "\n",
      "By offering affiliates 80% of profits, customizable encryptors and infrastructure, DragonForce lowers the barrier to entry for new and inexperienced cybercriminals.\n",
      "\n",
      "The move encourages more affiliates to join the cartel and broaden its presence.\n",
      "\n",
      "All-in-one integrated backup and cybersecurity platform for MSPs Acronis Cyber Protect Cloud integrates data protection, cybersecurity, and endpoint management. Easily scale cyber protection services from a single platform – while efficiently running your MSP business. Free 30-day Trial\n",
      "\n",
      "DragonForce and its Scattered Spider connection\n",
      "\n",
      "DragonForce's partnership with Scattered Spider, a financially motivated threat actor known for sophisticated social engineering and initial access operations, has proven effective in enabling ransomware deployments across high-value targets.\n",
      "\n",
      "Scattered Spider typically begins its intrusion by conducting reconnaissance on an organization’s staff to identify potential targets and develop convincing personas and pretexts.\n",
      "\n",
      "The group collects details such as names, job titles, and other publicly available information using social media platforms and open-source intelligence tools. They then use advanced social engineering tactics to obtain or reset credentials and circumvent multifactor authentication through deceptive tactics such as MFA fatigue or SIM swapping.\n",
      "\n",
      "Once access is gained, Scattered Spider signs in as the compromised user and registers its own device to maintain entry.\n",
      "\n",
      "Following the initial breach, Scattered Spider establishes persistence by deploying remote monitoring and management (RMM) tools or tunneling services.\n",
      "\n",
      "For example, these tools can include ScreenConnect, AnyDesk, TeamViewer and Splashtop. Once inside the network, Scattered Spider conducts thorough reconnaissance, targeting assets in SharePoint, credential repositories, backup servers and VPN configuration documentation.\n",
      "\n",
      "In recent activity, Scattered Spider has leveraged AWS Systems Manager Inventory to identify additional systems for lateral movement. They utilize extract, transform and load (ETL) tools to compile gathered data into a central database, which is then exfiltrated to attacker-controlled MEGA or Amazon S3 storage services.\n",
      "\n",
      "The operation concludes with the deployment of DragonForce ransomware, encrypting data across Windows, Linux and ESXi environments.\n",
      "\n",
      "Better together ransomware\n",
      "\n",
      "DragonForce represents a new, more organized and persistent threat, built on established ransomware frameworks but incrementally improved and distributed at scale.\n",
      "\n",
      "Unlike groups that heavily customize their code, DragonForce focuses on cartel-style recruitment, affiliate operational flexibility and broad partnerships, making it a formidable and highly adaptable actor.\n",
      "\n",
      "Coupled with Scattered Spider, cybercrime groups under cooperative models, rather than purely competitive ones, marks a shift that complicates defensive efforts for organizations worldwide.\n",
      "\n",
      "Key takeaways\n",
      "\n",
      "The DragonForce and Scattered Spider duo is a wakeup-call for \"cartelization\" cybercrime, where highly specialized threat actors combine their skills, in this case, Scattered Spider's elite social engineering and initial access skills and DragonForce's robust ransomware-as-a-service model, to execute devastating, high-profile attacks.\n",
      "\n",
      "Their strategic alliance significantly elevates the threat landscape by creating a more efficient and adaptive criminal operation focused on breaching defenses by exploiting human error before leveraging sophisticated malware.\n",
      "\n",
      "Looking ahead, IT security professionals must consider that defense requires addressing ransomware collaborative models head on.\n",
      "\n",
      "Implement and strictly enforce phishing-resistant multifactor authentication (MFA) methods to neutralize Scattered Spider's primary initial access vectors, and focus on robust endpoint detection and response (EDR) solutions that alert the deployment of remote monitoring tools and the use of vulnerable drivers, which are the technical tell-tales of a handoff from an initial access broker to a ransomware affiliate.\n",
      "\n",
      "Security teams need to anticipate that attacks are no longer single-entity threats, but coordinated, multistage intrusions using the best tools and techniques from an ecosystem of specialized cyber adversaries.\n",
      "\n",
      "About TRU\n",
      "\n",
      "The Acronis Threat Research Unit (TRU) is a team of cybersecurity experts specializing in threat intelligence, AI and risk management. The TRU team researches emerging threats, provides security insights and supports IT teams with guidelines, incident response and educational workshops.\n",
      "\n",
      "See the latest TRU research\n",
      "\n",
      "Sponsored and written by Acronis.\n",
      "\n",
      "Title: Aisuru botnet behind new record-breaking 29.7 Tbps DDoS attack\n",
      "Author: []\n",
      "Publish Date: None\n",
      "Content: In just three months, the massive Aisuru botnet launched more than 1,300 distributed denial-of-service attacks, one of them setting a new record with a peak at 29.7 terabits per second.\n",
      "\n",
      "Aisuru is a huge botnet-for-hire service that provides an army of routers and IoT devices compromised via known vulnerabilities or through brute-forcing weak credentials.\n",
      "\n",
      "Internet management and infrastructure company Cloudflare estimates that the botnet uses between one and four million infected hosts across the world.\n",
      "\n",
      "Cybercriminals can rent from distributors parts of the Aisuru botnet to launch distributed denial-of-service (DDoS) attacks.\n",
      "\n",
      "The largest hyper-volumetric attack from Aisuru-controlled devices occurred in the third quarter of 2025 and was successfully mitigated by Cloudflare.\n",
      "\n",
      "The previous record DDoS attack, which peaked at 22.2 Tbps, was also mitigated by Cloudflare and was attributed to Aisuru with medium confidence. More recently, Microsoft disclosed that the same botnet hit its Azure network with a massive 15 Tbps DDoS attack launched from 500,000 IP addresses.\n",
      "\n",
      "Cloudflare reports that it mitigated 2,867 Aisuru attacks since the beginning of the year, almost 45% of them being hyper-volumetric - attacks that exceed 1 Tbps or 1 billion packets per second (Bpps).\n",
      "\n",
      "The internet company did not name the target of the record-breaking incident, but notes that the attack lasted 69 seconds and peaked at 29.7 Tbps. It used UDP carpet-bombing to direct “garbage” traffic to an average of 15,000 destination ports per second.\n",
      "\n",
      "Graph from the record-breaking Aisuru attack\n",
      "\n",
      "Source: Cloudflare\n",
      "\n",
      "Another massive DDoS attack that the company mitigated reached 14.1 Bpps.\n",
      "\n",
      "Cloudflare says that Aisuru attacks can be so devastating that the amount of traffic can disrupt internet service providers (ISPs), even if they are not directly targeted.\n",
      "\n",
      "\"If Aisuru’s attack traffic can disrupt parts of the US’ Internet infrastructure when said ISPs were not even the target of the attack, imagine what it can do when it’s directly aimed at unprotected or insufficiently protected ISPs, critical infrastructure, healthcare services, emergency services, and military systems,\" Cloudflare says.\n",
      "\n",
      "Rise in hyper-volumetric attacks\n",
      "\n",
      "Statistical data from Cloudflare shows that hyper-volumetric DDoS attacks from the Aisuru botnet are rising steadily this year, reaching 1,304 incidents in Q3 alone.\n",
      "\n",
      "According to the researchers, Aisuru is targeting companies in various sectors, including gaming, hosting providers, telecommunications, and financial services.\n",
      "\n",
      "Hypervolumetric DDoS attacks per quarter\n",
      "\n",
      "Source: Cloudflare\n",
      "\n",
      "DDoS attacks exceeding 100 Mpps increased by 189% QoQ, and those exceeding 1 Tbps more than doubled (227%) QoQ.\n",
      "\n",
      "Most attacks end in less than 10 minutes, according to Cloudflare, leaving defenders and on-demand services little time to respond.\n",
      "\n",
      "“A short attack may only last a few seconds, but the disruption it causes can be severe, and recovery takes far longer,” explained Cloudflare.\n",
      "\n",
      "“Engineering and operational teams are then stuck with a complex, multi-step process to get critical systems back online, check data for consistency across distributed systems, and restore secure, reliable service to customers.”\n",
      "\n",
      "In terms of the number of DDoS attacks, this past quarter wasn’t at the level of Q1, but 2025 continues to be far more severe than the past years, and even without November and December having been accounted for yet.\n",
      "\n",
      "Number of DDoS attacks as of October 2025\n",
      "\n",
      "Source: Cloudflare\n",
      "\n",
      "Cloudflare says that in Q3 it mitigated an average of 3,780 DDoS attacks every hour, most coming from Indonesia, Thailand, Bangladesh, and Ecuador, and targeting China, Turkey, Germany, Brazil, and the United States.\n",
      "\n",
      "Title: Smashing Security podcast #446: A hacker doxxes himself, and social engineering-as-a-service\n",
      "Author: ['Graham Cluley Is An Award-Winning']\n",
      "Publish Date: 2025-12-04 00:30:56+00:00\n",
      "Content: Stories from the world of hacking, ransomware, cybersecurity, and rogue AI.\n",
      "\n",
      "A teenage cybercriminal posts a smug screenshot to mock a sextortion scammer… and accidentally hands over the keys to his real-world identity. Meanwhile, we look into the crystal ball for 2026 and consider how stolen data is now the jet fuel of cybercrime – and how next year could be even nastier than 2025.\n",
      "\n",
      "Plus, Graham rants about recipe sites that won’t shut up, and there’s even more love for Lily Allen’s album “West End Girl” album.\n",
      "\n",
      "All this and more is discussed in episode 446 of the “Smashing Security” podcast with cybersecurity veteran Graham Cluley, and special guest Rik Ferguson.\n",
      "\n",
      "Host:\n",
      "\n",
      "Graham Cluley:\n",
      "\n",
      "@grahamcluley.com @[email protected] / grahamcluley\n",
      "\n",
      "Guest:\n",
      "\n",
      "Rik Ferguson:\n",
      "\n",
      "/ rikferguson\n",
      "\n",
      "Episode links:\n",
      "\n",
      "Sponsored by:\n",
      "\n",
      "1Password – Take the first step to better security by securing your team’s credentials.\n",
      "\n",
      "Vanta – Expand the scope of your security program with market-leading compliance automation… while saving time and money. Smashing Security listeners get $1000 off!\n",
      "\n",
      "Drata – The world’s most advanced Trust Management platform – making risk and compliance management accessible, continuous, and 10x more automated than ever before.\n",
      "\n",
      "Support the show:\n",
      "\n",
      "You can help the podcast by telling your friends and colleagues about “Smashing Security”, and leaving us a review on Apple Podcasts or Podchaser.\n",
      "\n",
      "Join Smashing Security PLUS for ad-free episodes and our early-release feed!\n",
      "\n",
      "Follow us:\n",
      "\n",
      "Follow the show on Bluesky, or join us on the Smashing Security subreddit, or visit our website for more episodes.\n",
      "\n",
      "Thanks:\n",
      "\n",
      "Theme tune: “Vinyl Memories” by Mikael Manvelyan.\n",
      "\n",
      "Assorted sound effects: AudioBlocks.\n",
      "\n",
      "Found this article interesting? Follow Graham Cluley on LinkedIn, Bluesky, or Mastodon to read more of the exclusive content we post.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "from newspaper.article import ArticleException\n",
    "\n",
    "def scrape_news_from_feed(feed_url):\n",
    "    articles = []\n",
    "    now = datetime.utcnow()\n",
    "    last_24_hours = now - timedelta(hours=24)\n",
    "\n",
    "    feed = feedparser.parse(feed_url)\n",
    "\n",
    "    for entry in feed.entries:\n",
    "\n",
    "        # Check if the feed entry has a published date\n",
    "        if \"published_parsed\" in entry and entry.published_parsed is not None:\n",
    "            entry_datetime = datetime(*entry.published_parsed[:6])\n",
    "\n",
    "            # Skip if older than 24 hours\n",
    "            if entry_datetime < last_24_hours:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # create a newspaper article object\n",
    "            article = newspaper.Article(entry.link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            articles.append({\n",
    "                'title': article.title,\n",
    "                'author': article.authors,\n",
    "                'publish_date': article.publish_date,\n",
    "                'content': article.text\n",
    "            })\n",
    "        except ArticleException as e:\n",
    "            print(f\"Failed to fetch article: {entry.link} | Exception: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for article: {entry.link} | Exception: {e}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MULTIPLE DOMAIN RSS SOURCES\n",
    "# ---------------------------\n",
    "\n",
    "rss_sources = {\n",
    "    \"data_science\": [\n",
    "        \"https://towardsdatascience.com/feed\",\n",
    "        \"https://techcrunch.com/category/artificial-intelligence/feed/\",\n",
    "        \"https://www.theverge.com/rss/ai-artificial-intelligence/index.xml\"\n",
    "    ],\n",
    "    \"cyber_security\": [\n",
    "        \"https://feeds.feedburner.com/TheHackersNews\",\n",
    "        \"https://www.bleepingcomputer.com/feed/\",\n",
    "        \"https://threatpost.com/feed/\",\n",
    "        \"https://krebsonsecurity.com/feed/\",\n",
    "        \"https://www.welivesecurity.com/en/rss/feed/\",\n",
    "        \"https://www.securityweek.com/feed/\",\n",
    "        \"https://grahamcluley.com/feed/\",\n",
    "        \"https://www.cyberdefensemagazine.com/feed/\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Fetch Data per domain\n",
    "# ---------------------------\n",
    "\n",
    "all_articles = {}\n",
    "\n",
    "for domain, feeds in rss_sources.items():\n",
    "    domain_articles = []\n",
    "    for feed in feeds:\n",
    "        domain_articles.extend(scrape_news_from_feed(feed))\n",
    "    all_articles[domain] = domain_articles\n",
    "\n",
    "# ---------------------------\n",
    "# Output Articles by domain\n",
    "# ---------------------------\n",
    "\n",
    "for domain, articles in all_articles.items():\n",
    "    print(f\"\\n===== DOMAIN: {domain.upper()} =====\\n\")\n",
    "    for article in articles:\n",
    "        print(\"Title:\", article['title'])\n",
    "        print(\"Author:\", article['author'])\n",
    "        print(\"Publish Date:\", article['publish_date'])\n",
    "        print(\"Content:\", article['content'])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b4f3b",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89666ae8-16e5-407b-a158-ec5e4084cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\evzen\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.11.12)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.9 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.9 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/2.9 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/2.9 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 2.5 MB/s  0:00:01\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461a9791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our series of letters from African journalists, novelist and writer Geoffrey Ggregates looks at some of the challenges faced by systems designers when trying to improve the performance of their products or services. I have seen systems where a conversion metric, used initially to measure desktop-based click flows, was left unchanged despite mobile-firsts and shifts in user intent. The outcome was a measure that continued to update and plot, but it was no longer in line with user behaviour. The outcome was a measure that continued to update and plot, but it was no longer in line with user behaviour. When Metrics Improve While Alignment Fails (Image by Author) Metrics That Guide Versus Metrics That Mislead. The outcome was a measure that continued to update and plot, but it was no longer in line with user behaviour. The measurement created during a proof-of-concept can become a permanent element in production. The measurement created during a proof-of-concept can become a permanent element in production. The outcome was a measure that continued to update and plot, but it was no longer in line with user behaviour. The measurement created during a proof-of-concept can become a permanent element in production.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=500,\n",
    "        min_length=200,\n",
    "        num_beams=5\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "text ='''ggregates Obscure Systemic Blind Spots A major weakness of most KPI systems is the reliance on aggregate performance. The averaging of large user bases or data sets frequently obscures localised failure modes. I had earlier tested a credit scoring model that usually had high AUC scores. On paper, it was a success. But on the regional and user cohort-by-region disaggregations, one group, younger applicants in low-income regions, fared significantly worse. The model generalised well, but it possessed a structural blind spot.\n",
    "\n",
    "This bias is not reflected in the dashboards unless it is measured. And even when found, it is often treated as an edge case instead of a pointer to a more fundamental representational failure. The KPI here was not only misleading but also right: a performance average that masked performance inequity. It is not only a technical liability but also an ethical and regulatory one in systems operating at the national or global scale.\n",
    "\n",
    "From Metrics Debt to Metric Collapse\n",
    "\n",
    "KPIs become more solid as organisations grow larger. The measurement created during a proof-of-concept can become a permanent element in production. With time, the premises on which it is based become stale. I have seen systems where a conversion metric, used initially to measure desktop-based click flows, was left unchanged despite mobile-first redesigns and shifts in user intent. The outcome was a measure that continued to update and plot, but was no longer in line with user behaviour. It was now metrics debt; code that was not broken but no longer performed its intended task.\n",
    "\n",
    "Worse still, when such metrics are included in the model optimisation process, a downward spiral may occur. The model overfits to pursue the KPI. The misalignment is reaffirmed by retraining. Misinterpretation is spurred by optimisation. And unless one interrupts the loop by hand, the system degenerates as it reports the progress.\n",
    "\n",
    "When Metrics Improve While Alignment Fails (Image by Author)\n",
    "\n",
    "Metrics That Guide Versus Metrics That Mislead'''\n",
    "\n",
    "\n",
    "\n",
    "print(summarize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633657b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
